Notes-apps-iprscan.txt

</entry>



<entry [Mon Oct  8 21:31:58 EDT 2007] MORE MISCALCULATIONS - IT APPEARS IPRSCAN USES TOTAL LENGTH FOR CUTOFF AND KNOWN LENGTH FOR ORF LENGTH>



SOLUTION:

ADAPT collectionsequencesorf.pl TO USE TOTAL LENGTH FOR CUTOFF

DIAGNOSIS:

 ./collectionsequencesiprscan.pl -d funnybase11
<interpro_matches>

   <protein id="2708_1_ORF2" length="66" crc64="CA70A48CADA16F8D" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="3" end="15" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="2708_1_ORF3" length="59" crc64="CF417B6C0EE82AED" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="41" end="59" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="2708_2_ORF1" length="78" crc64="94BE5411A3702435" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="57" end="74" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>

</interpro_matches>

ORF 3 NOT DEFINED OR MULTIPLE RESULTS
Collection id: 2708
Collection accession: 1
Orf:
$VAR1 = undef;
SELECT * FROM collectionsequencesorf
                WHERE collectionid = 2708
                AND frame = 1
                AND length > 49
Orf name: 2708_1_ORF3
Frame: 1
Number: 3
Length: 59


SEQUENCE 3130:


<interpro_matches>

   <protein id="3130_1_ORF1" length="106" crc64="45D578E794224B75" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="1" end="105" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="3130_1_ORF2" length="60" crc64="87D4C59DD9A7B705" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="13" end="32" score="NA" status="?" evidence="Seg" />
            <location start="33" end="60" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="3130_2_ORF1" length="58" crc64="FEDE94EE9383D2A8" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="18" end="39" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="3130_2_ORF2" length="56" crc64="A60A6F2B9DE16F1E" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="33" end="42" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="3130_2_ORF3" length="180" crc64="DE7FA0171CF13171" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="76" end="120" score="NA" status="?" evidence="Seg" />
            <location start="162" end="180" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="3130_3_ORF1" length="113" crc64="466A0912A6087EC9" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="20" end="39" score="NA" status="?" evidence="Seg" />
            <location start="94" end="105" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="3130_3_ORF2" length="120" crc64="DD663FBBD8C77B70" >
        <interpro id="IPR001020" name="Phosphotransferase system, HPr histidine phosphorylation site" type="PTM">
          <found_in>
            <rel_ref ipr_ref="IPR000032"/>
            <rel_ref ipr_ref="IPR005698"/>
         </found_in>
          <classification id="GO:0005351" class_type="GO">
            <category>Molecular Function</category>
            <description>sugar porter activity</description>
          </classification>
          <classification id="GO:0006810" class_type="GO">
            <category>Biological Process</category>
            <description>transport</description>
          </classification>
          <classification id="GO:0009401" class_type="GO">
            <category>Biological Process</category>
            <description>phosphoenolpyruvate-dependent sugar phosphotransferase system</description>
          </classification>
          <match id="PS00369" name="PTS_HPR_HIS" dbname="PROSITE">
            <location start="3" end="10" score="NA" status="?" evidence="ScanRegExp" />
          </match>
        </interpro>
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="43" end="118" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>

</interpro_matches>

ORF 2 NOT DEFINED OR MULTIPLE RESULTS
Collection id: 3130
Collection accession: 1
Orf:
$VAR1 = undef;
SELECT * FROM collectionsequencesorf
                WHERE collectionid = 3130
                AND frame = 1
                AND length > 49
Orf name: 3130_1_ORF2
Frame: 1
Number: 2
Length: 60



ONLY ONE FRAME 1 ORF AT 50BP LENGTH CUTOFF:

select frame, orfnumber, orfstart, orfstop, length, peptide from collectionsequencesorf where length> 49 and collectionid =3130 order by frame, orfnumber, orfstart\G
*************************** 1. row ***************************
    frame: 1
orfnumber: 8
 orfstart: 403
  orfstop: 720
   length: 103
  peptide: GPRXRCAARGRGERATRPXERPXPRSDAHRRDRRRAARGAGRRRRGHPGRRRPAGGRGPRGGGGCGAECLRGLGARAAGTRARAAAYTSSARRRLRGDTRGERGGE
*************************** 2. row ***************************
    frame: 2
orfnumber: 5
 orfstart: 356
  orfstop: 1003
   length: 154
  peptide: KKYNQTAQTLTVRTTHEGRDEXXXSCLNXARTVARPSLCPGTRDPVHDARRGAAESEPHDPXSGRXRDRTPTAETGDAPRAAPADAVAGIRAGDDPPAGAVRAAGAGAGPSACAGSGRARQGRERAPRPTLAPRAGXCAETRVGSGAGNEVXKRIEMEKKEVXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*************************** 3. row ***************************
    frame: 3
orfnumber: 1
 orfstart: 3
  orfstop: 341
   length: 89
  peptide: QTAQXXPXYKKGTXXXXXHGGARGAGGGGAVGGGXGGXXIVKQNTKYKYTNSSVLQCDSEXIENELSYPPSINGDXPHCTLCMHPATDPHXKLXXXXXXXXXXPPRNSNNQIN
*************************** 4. row ***************************
    frame: 3
orfnumber: 3
 orfstart: 348
  orfstop: 707
   length: 113
  peptide: RXGLHTRAATKXXXPVSIXLAQSHDLHSVQGLGTPXTMRGAGPRRASHTTRRAAXTAIGRPPPRPATRRARRRQTPSRASGPATTRRRARSARRGRVRGRVLARARGARGRDASARRGLH
4 rows in set (0.00 sec)





mysql> select frame, orfnumber, orfstart, orfstop, length, peptide from collectionsequencesorf where collectionid =3130 order by frame, orfnumber, orfstart\G
*************************** 1. row ***************************
    frame: 1
orfnumber: 1
 orfstart: 1
  orfstop: 120
   length: 31
  peptide: PKRLKXXXFTRRGQXXXPXTGGRAGRAGGGRWXAXGGGGL
*************************** 2. row ***************************
    frame: 1
orfnumber: 4
 orfstart: 130
  orfstop: 276
   length: 34
  peptide: KTNSPTPLRSTGTXRTAHFACIPRPTRTXSXXXXXXXXXXXXXATATIR
*************************** 3. row ***************************
    frame: 1
orfnumber: 7
 orfstart: 286
  orfstop: 399
   length: 33
  peptide: SNSTNFNGXDYTRGPRRXXXVLSQLXSHSRTTFTLSRD
*************************** 4. row ***************************
    frame: 1
orfnumber: 8
 orfstart: 403
  orfstop: 720
   length: 103
  peptide: GPRXRCAARGRGERATRPXERPXPRSDAHRRDRRRAARGAGRRRRGHPGRRRPAGGRGPRGGGGCGAECLRGLGARAAGTRARAAAYTSSARRRLRGDTRGERGGE
*************************** 5. row ***************************
    frame: 2
orfnumber: 1
 orfstart: 2
  orfstop: 175
   length: 45
  peptide: PNGSXXXXLQEGDXTXXXXRGGARGGRGGGGGXRXGGXXYSKTKYKIQIHKLISTAVR
*************************** 6. row ***************************
    frame: 2
orfnumber: 2
 orfstart: 179
  orfstop: 346
   length: 45
  peptide: RXHRKRTLLPPFDQRGPAALHTLHASRDRPALXAXXAXXXXXGPPXXQQQQSDKLK
*************************** 7. row ***************************
    frame: 2
orfnumber: 5
 orfstart: 356
  orfstop: 1003
   length: 154
  peptide: KKYNQTAQTLTVRTTHEGRDEXXXSCLNXARTVARPSLCPGTRDPVHDARRGAAESEPHDPXSGRXRDRTPTAETGDAPRAAPADAVAGIRAGDDPPAGAVRAAGAGAGPSACAGSGRARQGRERAPRPTLAPRAGXCAETRVGSGAGNEVXKRIEMEKKEVXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*************************** 8. row ***************************
    frame: 3
orfnumber: 1
 orfstart: 3
  orfstop: 341
   length: 89
  peptide: QTAQXXPXYKKGTXXXXXHGGARGAGGGGAVGGGXGGXXIVKQNTKYKYTNSSVLQCDSEXIENELSYPPSINGDXPHCTLCMHPATDPHXKLXXXXXXXXXXPPRNSNNQIN
*************************** 9. row ***************************
    frame: 3
orfnumber: 3
 orfstart: 348
  orfstop: 707
   length: 113
  peptide: RXGLHTRAATKXXXPVSIXLAQSHDLHSVQGLGTPXTMRGAGPRRASHTTRRAAXTAIGRPPPRPATRRARRRQTPSRASGPATTRRRARSARRGRVRGRVLARARGARGRDASARRGLH
9 rows in set (0.00 sec)



</entry>



<entry [Mon Oct  8 20:32:59 EDT 2007] INTERPRO MISCALCULATING ORF LENGTH>



PROBLEM:

INTERPRO IS MISCALCULATING ORF LENGTH FOR SOME SEQUENCES

E.G., funnybase11 COLLECTION SEQUENCE 70 DOES NOT HAVE A FRAME 3 NUMBER 2 ORF OF LENGTH 120 BP (THE THIRD protein HIT BELOW:


 <interpro_matches>

   <protein id="70_1_ORF2" length="98" crc64="BDF06F6F2FE03039" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="6" end="30" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="70_2_ORF1" length="196" crc64="798F8A6BC4E21062" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="127" end="149" score="NA" status="?" evidence="Seg" />
            <location start="174" end="191" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="70_3_ORF2" length="120" crc64="CC717E72851AB9F4" >
        <interpro id="IPR006688" name="ADP-ribosylation factor" type="Family" parent_id="IPR006689">
          <contains>
            <rel_ref ipr_ref="IPR005225"/>
          </contains>
          <classification id="GO:0005525" class_type="GO">
            <category>Molecular Function</category>
            <description>GTP binding</description>
          </classification>
          <classification id="GO:0005622" class_type="GO">
            <category>Cellular Component</category>
            <description>intracellular</description>
          </classification>
          <classification id="GO:0007264" class_type="GO">
            <category>Biological Process</category>
            <description>small GTPase mediated signal transduction</description>
          </classification>
          <match id="SM00177" name="no description" dbname="SMART">
            <location start="7" end="120" score="0.00081" status="T" evidence="HMMSmart" />
          </match>
        </interpro>
        <interpro id="IPR006689" name="ARF/SAR superfamily" type="Family" parent_id="IPR001806">
          <child_list>
            <rel_ref ipr_ref="IPR006687"/>
            <rel_ref ipr_ref="IPR006688"/>
          </child_list>
          <contains>
            <rel_ref ipr_ref="IPR005225"/>
          </contains>
          <classification id="GO:0005525" class_type="GO">
            <category>Molecular Function</category>
            <description>GTP binding</description>
          </classification>
          <match id="PTHR11711" name="ADP RIBOSYLATION FACTOR-RELATED" dbname="PANTHER">
            <location start="7" end="120" score="8.8e-69" status="T" evidence="HMMPanther" />
          </match>
          <match id="PF00025" name="Arf" dbname="PFAM">
            <location start="12" end="120" score="5e-42" status="T" evidence="HMMPfam" />
          </match>
        </interpro>
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="G3DSA:3.40.50.300" name="no description" dbname="GENE3D">
            <location start="27" end="120" score="9.1e-29" status="T" evidence="Gene3D" />
          </match>
          <match id="PTHR11711:SF28" name="ADP-RIBOSYLATION FACTOR-LIKE 4, ARL4" dbname="PANTHER">
            <location start="7" end="120" score="8.8e-69" status="T" evidence="HMMPanther" />
          </match>
        </interpro>
   </protein>

</interpro_matches>


ORFS FOR SEQUENCE 70:


select frame, orfnumber, orfstart, orfstop, length from collectionsequencesorf where length> 49 and collectionid = 70 order by frame, orfnumber, orfstart;
+-------+-----------+----------+---------+--------+
| frame | orfnumber | orfstart | orfstop | length |
+-------+-----------+----------+---------+--------+
|     1 |         2 |        4 |     159 |     52 |
|     1 |         5 |      169 |     462 |     98 |
|     2 |         1 |        2 |     589 |    196 |
|     3 |         1 |        3 |     167 |     55 |
|     3 |         3 |      174 |     611 |    146 |
+-------+-----------+----------+---------+--------+
5 rows in set (0.02 sec)


OUTPUT OF collectionsequencesorf FOR SEQUENCE 70:


/collectionsequencesorf.pl -d funnybase11
Doing ORFs for collection sequences (total 11973)
Sequence: GTGGCCGGACGGCAGCAGCGCAGACGTGGAAAAAAGAAGCGTTTCCTTTCCGCTATGGCTGTGTTTATCTCCTGTGGGTGATCGTTCCTCCTCACATTTTACCAGGTCTCCGGTCGGTCATCGCCTCGTTTCGCCTGCCGAGAGGAACCTGCCCGCCGAGAAAGGATTAAGCAAGGAATCTCCCACCTCTCCTCCGCGGGTTGTGAATTCTTGGCCCCCGAACCATGGGGAACCAGCTGACCGACATCGCCCCCAACCCGTCGTTCCTGCCGAGCTTCCAGTGTGTGCACGTGGTTGTGATTGGTCTGGATTCGTCTGGGAAGACCTCGCTGCTCTACAGGCTCAAACTGAAGGAGTTTGTCAAAACGATCCCCACCAAGGGCTTCAACATGGAGAAGATCAAGGCGGCGGTGGGGGCGTCGCGGGCCACAAACTTCCAGGTGTGGGACGTGGGGGGCCAGGAGAAGCTGCGGCCCCTCTGGAAGTCCTACACCCGGCGAACGGACGGCATCATGTTTGTCGTGGACTCCGCCGAGCCGGAGCGCTTGGAGGAGGCCAGGGTGGAGCTGCACAAGATCACGCGCACCTCTGAGAACCAGGGGGTGCCGGTGCTGGTCCTGGCCAACAAACAGGACCTGGACTCGG
$VAR1 = {
          'orfnumber' => 2,
          'collectionaccession' => '1',
          'name' => 'fr1_orf2',
          'frame' => 1,
          'peptide' => 'SFLLTFYQVSGRSSPRFACREEPARRERIKQGISHLSSAGCEFLAPEPWGTS',
          'length' => 52,
          'collectionid' => '70',
          'orfstart' => 4,
          'orfstop' => 159,
          'id' => '70.1'
        };
TSVLINE :

70      1       1       2       4       159     52      SFLLTFYQVSGRSSPRFACREEPARRERIKQGISHLSSAGCEFLAPEPWGTS


$VAR1 = {
          'orfnumber' => 5,
          'collectionaccession' => '1',
          'name' => 'fr1_orf5',
          'frame' => 1,
          'peptide' => 'RSLSKRSPPRASTWRRSRRRWGRRGPQTSRCGTWGARRSCGPSGSPTPGERTASCLSWTPPSRSAWRRPGWSCTRSRAPLRTRGCRCWSWPTNRTWTR',
          'length' => 98,
          'collectionid' => '70',
          'orfstart' => 169,
          'orfstop' => 462,
          'id' => '70.1'
        };
TSVLINE :

70      1       1       5       169     462     98      RSLSKRSPPRASTWRRSRRRWGRRGPQTSRCGTWGARRSCGPSGSPTPGERTASCLSWTPPSRSAWRRPGWSCTRSRAPLRTRGCRCWSWPTNRTWTR


$VAR1 = {
          'orfnumber' => 1,
          'collectionaccession' => '1',
          'name' => 'fr2_orf1',
          'frame' => 2,
          'peptide' => 'WPDGSSADVEKRSVSFPLWLCLSPVGDRSSSHFTRSPVGHRLVSPAERNLPAEKGLSKESPTSPPRVVNSWPPNHGEPADRHRPQPVVPAELPVCARGCDWSGFVWEDLAALQAQTEGVCQNDPHQGLQHGEDQGGGGGVAGHKLPGVGRGGPGEAAAPLEVLHPANGRHHVCRGLRRAGALGGGQGGAAQDHAHL',
          'length' => 196,
          'collectionid' => '70',
          'orfstart' => 2,
          'orfstop' => 589,
          'id' => '70.1'
        };
TSVLINE :

70      1       2       1       2       589     196     WPDGSSADVEKRSVSFPLWLCLSPVGDRSSSHFTRSPVGHRLVSPAERNLPAEKGLSKESPTSPPRVVNSWPPNHGEPADRHRPQPVVPAELPVCARGCDWSGFVWEDLAALQAQTEGVCQNDPHQGLQHGEDQGGGGGVAGHKLPGVGRGGPGEAAAPLEVLHPANGRHHVCRGLRRAGALGGGQGGAAQDHAHL


$VAR1 = {
          'orfnumber' => 1,
          'collectionaccession' => '1',
          'name' => 'fr3_orf1',
          'frame' => 3,
          'peptide' => 'GRTAAAQTWKKEAFPFRYGCVYLLWVIVPPHILPGLRSVIASFRLPRGTCPPRKD',
          'length' => 55,
          'collectionid' => '70',
          'orfstart' => 3,
          'orfstop' => 167,
          'id' => '70.1'
        };
TSVLINE :

70      1       3       1       3       167     55      GRTAAAQTWKKEAFPFRYGCVYLLWVIVPPHILPGLRSVIASFRLPRGTCPPRKD


$VAR1 = {
          'orfnumber' => 3,
          'collectionaccession' => '1',
          'name' => 'fr3_orf3',
          'frame' => 3,
          'peptide' => 'ILGPRTMGNQLTDIAPNPSFLPSFQCVHVVVIGLDSSGKTSLLYRLKLKEFVKTIPTKGFNMEKIKAAVGASRATNFQVWDVGGQEKLRPLWKSYTRRTDGIMFVVDSAEPERLEEARVELHKITRTSENQGVPVLVLANKQDLDS',
          'length' => 146,
          'collectionid' => '70',
          'orfstart' => 174,
          'orfstop' => 611,
          'id' => '70.1'
        };
TSVLINE :

70      1       3       3       174     611     146     ILGPRTMGNQLTDIAPNPSFLPSFQCVHVVVIGLDSSGKTSLLYRLKLKEFVKTIPTKGFNMEKIKAAVGASRATNFQVWDVGGQEKLRPLWKSYTRRTDGIMFVVDSAEPERLEEARVELHKITRTSENQGVPVLVLANKQDLDS


TSV file printed:

/Users/young/FUNNYBASE/pipeline/funnybase11/collection/funnybase11.collectionsequencesorf.tsv

70      1       1       2       4       159     52      SFLLTFYQVSGRSSPRFACREEPARRERIKQGISHLSSAGCEFLAPEPWGTS
70      1       1       5       169     462     98      RSLSKRSPPRASTWRRSRRRWGRRGPQTSRCGTWGARRSCGPSGSPTPGERTASCLSWTPPSRSAWRRPGWSCTRSRAPLRTRGCRCWSWPTNRTWTR
70      1       2       1       2       589     196     WPDGSSADVEKRSVSFPLWLCLSPVGDRSSSHFTRSPVGHRLVSPAERNLPAEKGLSKESPTSPPRVVNSWPPNHGEPADRHRPQPVVPAELPVCARGCDWSGFVWEDLAALQAQTEGVCQNDPHQGLQHGEDQGGGGGVAGHKLPGVGRGGPGEAAAPLEVLHPANGRHHVCRGLRRAGALGGGQGGAAQDHAHL
70      1       3       1       3       167     55      GRTAAAQTWKKEAFPFRYGCVYLLWVIVPPHILPGLRSVIASFRLPRGTCPPRKD
70      1       3       3       174     611     146     ILGPRTMGNQLTDIAPNPSFLPSFQCVHVVVIGLDSSGKTSLLYRLKLKEFVKTIPTKGFNMEKIKAAVGASRATNFQVWDVGGQEKLRPLWKSYTRRTDGIMFVVDSAEPERLEEARVELHKITRTSENQGVPVLVLANKQDLDS



OUTPUT OF DNATranslator FOR SEQUENCE 70:


1: V  A  G  R  Q  Q  R  R  R  G  K  K  K  R  F  L  S  A  M  A  
     2:  W  P  D  G  S  S  A  D  V  E  K  R  S  V  S  F  P  L  W  L 
     3:   G  R  T  A  A  A  Q  T  W  K  K  E  A  F  P  F  R  Y  G  C
        GTGGCCGGACGGCAGCAGCGCAGACGTGGAAAAAAGAAGCGTTTCCTTTCCGCTATGGCT
      1 ---------!---------!---------!---------!---------!---------! 60
        CACCGGCCTGCCGTCGTCGCGTCTGCACCTTTTTTCTTCGCAAAGGAAAGGCGATACCGA


     1: V  F  I  S  C  G  *  S  F  L  L  T  F  Y  Q  V  S  G  R  S  
     2:  C  L  S  P  V  G  D  R  S  S  S  H  F  T  R  S  P  V  G  H 
     3:   V  Y  L  L  W  V  I  V  P  P  H  I  L  P  G  L  R  S  V  I
        GTGTTTATCTCCTGTGGGTGATCGTTCCTCCTCACATTTTACCAGGTCTCCGGTCGGTCA
     61 ---------!---------!---------!---------!---------!---------! 120
        CACAAATAGAGGACACCCACTAGCAAGGAGGAGTGTAAAATGGTCCAGAGGCCAGCCAGT


     1: S  P  R  F  A  C  R  E  E  P  A  R  R  E  R  I  K  Q  G  I  
     2:  R  L  V  S  P  A  E  R  N  L  P  A  E  K  G  L  S  K  E  S 
     3:   A  S  F  R  L  P  R  G  T  C  P  P  R  K  D  *  A  R  N  L
        TCGCCTCGTTTCGCCTGCCGAGAGGAACCTGCCCGCCGAGAAAGGATTAAGCAAGGAATC
    121 ---------!---------!---------!---------!---------!---------! 180
        AGCGGAGCAAAGCGGACGGCTCTCCTTGGACGGGCGGCTCTTTCCTAATTCGTTCCTTAG


     1: S  H  L  S  S  A  G  C  E  F  L  A  P  E  P  W  G  T  S  *  
     2:  P  T  S  P  P  R  V  V  N  S  W  P  P  N  H  G  E  P  A  D 
     3:   P  P  L  L  R  G  L  *  I  L  G  P  R  T  M  G  N  Q  L  T
        TCCCACCTCTCCTCCGCGGGTTGTGAATTCTTGGCCCCCGAACCATGGGGAACCAGCTGA
    181 ---------!---------!---------!---------!---------!---------! 240
        AGGGTGGAGAGGAGGCGCCCAACACTTAAGAACCGGGGGCTTGGTACCCCTTGGTCGACT


     1: P  T  S  P  P  T  R  R  S  C  R  A  S  S  V  C  T  W  L  *  
     2:  R  H  R  P  Q  P  V  V  P  A  E  L  P  V  C  A  R  G  C  D 
     3:   D  I  A  P  N  P  S  F  L  P  S  F  Q  C  V  H  V  V  V  I
        CCGACATCGCCCCCAACCCGTCGTTCCTGCCGAGCTTCCAGTGTGTGCACGTGGTTGTGA
    241 ---------!---------!---------!---------!---------!---------! 300
        GGCTGTAGCGGGGGTTGGGCAGCAAGGACGGCTCGAAGGTCACACACGTGCACCAACACT


     1: L  V  W  I  R  L  G  R  P  R  C  S  T  G  S  N  *  R  S  L  
     2:  W  S  G  F  V  W  E  D  L  A  A  L  Q  A  Q  T  E  G  V  C 
     3:   G  L  D  S  S  G  K  T  S  L  L  Y  R  L  K  L  K  E  F  V
        TTGGTCTGGATTCGTCTGGGAAGACCTCGCTGCTCTACAGGCTCAAACTGAAGGAGTTTG
    301 ---------!---------!---------!---------!---------!---------! 360
        AACCAGACCTAAGCAGACCCTTCTGGAGCGACGAGATGTCCGAGTTTGACTTCCTCAAAC


     1: S  K  R  S  P  P  R  A  S  T  W  R  R  S  R  R  R  W  G  R  
     2:  Q  N  D  P  H  Q  G  L  Q  H  G  E  D  Q  G  G  G  G  G  V 
     3:   K  T  I  P  T  K  G  F  N  M  E  K  I  K  A  A  V  G  A  S
        TCAAAACGATCCCCACCAAGGGCTTCAACATGGAGAAGATCAAGGCGGCGGTGGGGGCGT
    361 ---------!---------!---------!---------!---------!---------! 420
        AGTTTTGCTAGGGGTGGTTCCCGAAGTTGTACCTCTTCTAGTTCCGCCGCCACCCCCGCA


     1: R  G  P  Q  T  S  R  C  G  T  W  G  A  R  R  S  C  G  P  S  
     2:  A  G  H  K  L  P  G  V  G  R  G  G  P  G  E  A  A  A  P  L 
     3:   R  A  T  N  F  Q  V  W  D  V  G  G  Q  E  K  L  R  P  L  W
        CGCGGGCCACAAACTTCCAGGTGTGGGACGTGGGGGGCCAGGAGAAGCTGCGGCCCCTCT
    421 ---------!---------!---------!---------!---------!---------! 480
        GCGCCCGGTGTTTGAAGGTCCACACCCTGCACCCCCCGGTCCTCTTCGACGCCGGGGAGA


     1: G  S  P  T  P  G  E  R  T  A  S  C  L  S  W  T  P  P  S  R  
     2:  E  V  L  H  P  A  N  G  R  H  H  V  C  R  G  L  R  R  A  G 
     3:   K  S  Y  T  R  R  T  D  G  I  M  F  V  V  D  S  A  E  P  E
        GGAAGTCCTACACCCGGCGAACGGACGGCATCATGTTTGTCGTGGACTCCGCCGAGCCGG
    481 ---------!---------!---------!---------!---------!---------! 540
        CCTTCAGGATGTGGGCCGCTTGCCTGCCGTAGTACAAACAGCACCTGAGGCGGCTCGGCC


     1: S  A  W  R  R  P  G  W  S  C  T  R  S  R  A  P  L  R  T  R  
     2:  A  L  G  G  G  Q  G  G  A  A  Q  D  H  A  H  L  *  E  P  G 
     3:   R  L  E  E  A  R  V  E  L  H  K  I  T  R  T  S  E  N  Q  G
        AGCGCTTGGAGGAGGCCAGGGTGGAGCTGCACAAGATCACGCGCACCTCTGAGAACCAGG
    541 ---------!---------!---------!---------!---------!---------! 600
        TCGCGAACCTCCTCCGGTCCCACCTCGACGTGTTCTAGTGCGCGTGGAGACTCTTGGTCC


     1: G  C  R  C  W  S  W  P  T  N  R  T  W  T  R  
     2:  G  A  G  A  G  P  G  Q  Q  T  G  P  G  L  
     3:   V  P  V  L  V  L  A  N  K  Q  D  L  D  S  
        GGGTGCCGGTGCTGGTCCTGGCCAACAAACAGGACCTGGACTCGG
    601 ---------!---------!---------!---------!----- 645
        CCCACGGCCACGACCAGGACCGGTTGTTTGTCCTGGACCTGAGCC


>Untitled frame1
VAGRQQRRRGKKKRFLSAMAVFISCG*SFLLTFYQVSGRSSPRFACREEPARRERIKQGI
SHLSSAGCEFLAPEPWGTS*PTSPPTRRSCRASSVCTWL*LVWIRLGRPRCSTGSN*RSL
SKRSPPRASTWRRSRRRWGRRGPQTSRCGTWGARRSCGPSGSPTPGERTASCLSWTPPSR
SAWRRPGWSCTRSRAPLRTRGCRCWSWPTNRTWTR


>Untitled frame1_ORF1
SFLLTFYQVSGRSSPRFACREEPARRERIKQGISHLSSAGCEFLAPEPWGTS*
 
>Untitled frame1_ORF2
RSLSKRSPPRASTWRRSRRRWGRRGPQTSRCGTWGARRSCGPSGSPTPGERTASCLSWTP
PSRSAWRRPGWSCTRSRAPLRTRGCRCWSWPTNRTWTR
 

>Untitled frame2
WPDGSSADVEKRSVSFPLWLCLSPVGDRSSSHFTRSPVGHRLVSPAERNLPAEKGLSKES
PTSPPRVVNSWPPNHGEPADRHRPQPVVPAELPVCARGCDWSGFVWEDLAALQAQTEGVC
QNDPHQGLQHGEDQGGGGGVAGHKLPGVGRGGPGEAAAPLEVLHPANGRHHVCRGLRRAG
ALGGGQGGAAQDHAHL*EPGGAGAGPGQQTGPGL


>Untitled frame2_ORF1
WPDGSSADVEKRSVSFPLWLCLSPVGDRSSSHFTRSPVGHRLVSPAERNLPAEKGLSKES
PTSPPRVVNSWPPNHGEPADRHRPQPVVPAELPVCARGCDWSGFVWEDLAALQAQTEGVC
QNDPHQGLQHGEDQGGGGGVAGHKLPGVGRGGPGEAAAPLEVLHPANGRHHVCRGLRRAG
ALGGGQGGAAQDHAHL*
 

>Untitled frame3
GRTAAAQTWKKEAFPFRYGCVYLLWVIVPPHILPGLRSVIASFRLPRGTCPPRKD*ARNL
PPLLRGL*ILGPRTMGNQLTDIAPNPSFLPSFQCVHVVVIGLDSSGKTSLLYRLKLKEFV
KTIPTKGFNMEKIKAAVGASRATNFQVWDVGGQEKLRPLWKSYTRRTDGIMFVVDSAEPE
RLEEARVELHKITRTSENQGVPVLVLANKQDLDS


>Untitled frame3_ORF1
GRTAAAQTWKKEAFPFRYGCVYLLWVIVPPHILPGLRSVIASFRLPRGTCPPRKD*
 
>Untitled frame3_ORF2
ILGPRTMGNQLTDIAPNPSFLPSFQCVHVVVIGLDSSGKTSLLYRLKLKEFVKTIPTKGF
NMEKIKAAVGASRATNFQVWDVGGQEKLRPLWKSYTRRTDGIMFVVDSAEPERLEEARVE
LHKITRTSENQGVPVLVLANKQDLDS





START POINTS MATCH BETWEEN COMMAND LINE IPRSCAN OF THE ORF (FRAME 3, ORF 3) AND THE ORF OUTPUT FROM THE EARLIER WHOLE-SEQUENCE IPRSCAN:


select frame, orfnumber, orfstart, orfstop, length, peptide from collectionsequencesorf where length> 49 and collectionid = 70 order by frame, orfnumber, orfstart\G
*************************** 1. row ***************************
    frame: 1
orfnumber: 2
 orfstart: 4
  orfstop: 159
   length: 52
  peptide: SFLLTFYQVSGRSSPRFACREEPARRERIKQGISHLSSAGCEFLAPEPWGTS
*************************** 2. row ***************************
    frame: 1
orfnumber: 5
 orfstart: 169
  orfstop: 462
   length: 98
  peptide: RSLSKRSPPRASTWRRSRRRWGRRGPQTSRCGTWGARRSCGPSGSPTPGERTASCLSWTPPSRSAWRRPGWSCTRSRAPLRTRGCRCWSWPTNRTWTR
*************************** 3. row ***************************
    frame: 2
orfnumber: 1
 orfstart: 2
  orfstop: 589
   length: 196
  peptide: WPDGSSADVEKRSVSFPLWLCLSPVGDRSSSHFTRSPVGHRLVSPAERNLPAEKGLSKESPTSPPRVVNSWPPNHGEPADRHRPQPVVPAELPVCARGCDWSGFVWEDLAALQAQTEGVCQNDPHQGLQHGEDQGGGGGVAGHKLPGVGRGGPGEAAAPLEVLHPANGRHHVCRGLRRAGALGGGQGGAAQDHAHL
*************************** 4. row ***************************
    frame: 3
orfnumber: 1
 orfstart: 3
  orfstop: 167
   length: 55
  peptide: GRTAAAQTWKKEAFPFRYGCVYLLWVIVPPHILPGLRSVIASFRLPRGTCPPRKD
*************************** 5. row ***************************
    frame: 3
orfnumber: 3
 orfstart: 174
  orfstop: 611
   length: 146
  peptide: ILGPRTMGNQLTDIAPNPSFLPSFQCVHVVVIGLDSSGKTSLLYRLKLKEFVKTIPTKGFNMEKIKAAVGASRATNFQVWDVGGQEKLRPLWKSYTRRTDGIMFVVDSAEPERLEEARVELHKITRTSENQGVPVLVLANKQDLDS
5 rows in set (0.02 sec)


COMMAND LINE IPRSCAN OF FRAME 3, ORF 3:

<interpro_matches>

   <protein id="70" length="146" crc64="B5B52C7547D68A0A" >
        <interpro id="IPR001806" name="Ras GTPase" type="Family">
          <child_list>
            <rel_ref ipr_ref="IPR006689"/>
            <rel_ref ipr_ref="IPR013753"/>
          </child_list>
          <contains>
            <rel_ref ipr_ref="IPR005225"/>
            <rel_ref ipr_ref="IPR013684"/>
          </contains>
          <match id="PR00449" name="RASTRNSFRMNG" dbname="PRINTS">
            <location start="27" end="48" score="4.3e-09" status="T" evidence="FPrintScan" />
            <location start="68" end="90" score="4.3e-09" status="T" evidence="FPrintScan" />
            <location start="131" end="144" score="4.3e-09" status="T" evidence="FPrintScan" />
          </match>
        </interpro>
        <interpro id="IPR006688" name="ADP-ribosylation factor" type="Family" parent_id="IPR006689">
          <contains>
            <rel_ref ipr_ref="IPR005225"/>
          </contains>
          <match id="SM00177" name="no description" dbname="SMART">
            <location start="7" end="146" score="7.7e-16" status="T" evidence="HMMSmart" />
          </match>
        </interpro>
        <interpro id="IPR006689" name="ARF/SAR superfamily" type="Family" parent_id="IPR001806">
          <child_list>
            <rel_ref ipr_ref="IPR006687"/>
            <rel_ref ipr_ref="IPR006688"/>
          </child_list>
          <contains>
            <rel_ref ipr_ref="IPR005225"/>
          </contains>
          <match id="PR00328" name="SAR1GTPBP" dbname="PRINTS">
            <location start="28" end="51" score="3e-06" status="T" evidence="FPrintScan" />
            <location start="133" end="146" score="3e-06" status="T" evidence="FPrintScan" />
          </match>
          <match id="PTHR11711" name="ADP RIBOSYLATION FACTOR-RELATED" dbname="PANTHER">
            <location start="7" end="144" score="5.1e-89" status="T" evidence="HMMPanther" />
          </match>
          <match id="PF00025" name="Arf" dbname="PFAM">
            <location start="12" end="146" score="1.2e-34" status="T" evidence="HMMPfam" />
          </match>
        </interpro>
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="G3DSA:3.40.50.300" name="no description" dbname="GENE3D">
            <location start="27" end="144" score="7.1e-39" status="T" evidence="Gene3D" />
          </match>
          <match id="PTHR11711:SF28" name="ADP-RIBOSYLATION FACTOR-LIKE 4, ARL4" dbname="PANTHER">
            <location start="7" end="144" score="5.1e-89" status="T" evidence="HMMPanther" />
          </match>
        </interpro>
   </protein>

</interpro_matches>


WHOLE-SEQUENCE IRPSCAN:

++++ Iprscan::collectionsequences_iprscan(dbh, iprscan_output, fields, collection_id, collection_accession)
Query hits: <interpro_matches>

   <protein id="70_1_ORF2" length="98" crc64="BDF06F6F2FE03039" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="6" end="30" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="70_2_ORF1" length="196" crc64="798F8A6BC4E21062" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="127" end="149" score="NA" status="?" evidence="Seg" />
            <location start="174" end="191" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="70_3_ORF2" length="120" crc64="CC717E72851AB9F4" >
        <interpro id="IPR006688" name="ADP-ribosylation factor" type="Family" parent_id="IPR006689">
          <contains>
            <rel_ref ipr_ref="IPR005225"/>
          </contains>
          <classification id="GO:0005525" class_type="GO">
            <category>Molecular Function</category>
            <description>GTP binding</description>
          </classification>
          <classification id="GO:0005622" class_type="GO">
            <category>Cellular Component</category>
            <description>intracellular</description>
          </classification>
          <classification id="GO:0007264" class_type="GO">
            <category>Biological Process</category>
            <description>small GTPase mediated signal transduction</description>
          </classification>
          <match id="SM00177" name="no description" dbname="SMART">
            <location start="7" end="120" score="0.00081" status="T" evidence="HMMSmart" />
          </match>
        </interpro>
        <interpro id="IPR006689" name="ARF/SAR superfamily" type="Family" parent_id="IPR001806">
          <child_list>
            <rel_ref ipr_ref="IPR006687"/>
            <rel_ref ipr_ref="IPR006688"/>
          </child_list>
          <contains>
            <rel_ref ipr_ref="IPR005225"/>
          </contains>
          <classification id="GO:0005525" class_type="GO">
            <category>Molecular Function</category>
            <description>GTP binding</description>
          </classification>
          <match id="PTHR11711" name="ADP RIBOSYLATION FACTOR-RELATED" dbname="PANTHER">
            <location start="7" end="120" score="8.8e-69" status="T" evidence="HMMPanther" />
          </match>
          <match id="PF00025" name="Arf" dbname="PFAM">
            <location start="12" end="120" score="5e-42" status="T" evidence="HMMPfam" />
          </match>
        </interpro>
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="G3DSA:3.40.50.300" name="no description" dbname="GENE3D">
            <location start="27" end="120" score="9.1e-29" status="T" evidence="Gene3D" />
          </match>
          <match id="PTHR11711:SF28" name="ADP-RIBOSYLATION FACTOR-LIKE 4, ARL4" dbname="PANTHER">
            <location start="7" end="120" score="8.8e-69" status="T" evidence="HMMPanther" />
          </match>
        </interpro>
   </protein>

</interpro_matches>




SAME WITH SEQUENCE 74:
 
select frame, orfnumber, orfstart, orfstop, length, peptide from collectionsequencesorf where length> 49 and collectionid = 74 order by frame, orfnumber, orfstart\G
*************************** 1. row ***************************
    frame: 3
orfnumber: 1
 orfstart: 3
  orfstop: 1169
   length: 389
  peptide: GAGIPGFKGDFSMNDLMAIFSVLQKWETMEDDKPVLSVYSRVSGQEWFFADMSKETIRNIITALSPASWKSSPLQAMIESLQRGISWHYTRPFLTWEARYIQATTLGLPVEISKYYNSITAINFNAKVAVNPPPTESLLQLLNSEMILETDGFAGVTKDFWVFYGINTKLFQSGSELKTKNPTAVPWKLSAKINIPEMKFELDFPPCKEELQLFSIRSDLYAVSRNIEEPDSAKRTSIIPPSDDPRDDVLKPNTWHPTSRICYKCPVSGMGLCMEYELKRQYYYEESPLYYVLGYSNMALKMVPAQTMKAVDKIHFELDAGPSRHPESAHQLLATLRLSKAQNSTTEAVLNFKAFAISENQKPEGYEAAMYYTPEASIQNAQLIVSQVG

COMMAND LINE IPRSCAN OF ORF:

<interpro_matches>

   <protein id="74.1_frame" length="389" crc64="D1A3009B5DC12F60" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="G3DSA:2.20.50.20" name="no description" dbname="GENE3D">
            <location start="23" end="54" score="1.1e-05" status="T" evidence="Gene3D" />
          </match>
          <match id="G3DSA:2.20.80.10" name="no description" dbname="GENE3D">
            <location start="71" end="321" score="3.3e-76" status="T" evidence="Gene3D" />
          </match>
          <match id="PTHR23345" name="VITELLOGENIN-RELATED" dbname="PANTHER">
            <location start="15" end="248" score="1.9e-43" status="T" evidence="HMMPanther" />
          </match>
          <match id="seg" name="seg" dbname="SEG">
            <location start="237" end="248" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>

</interpro_matches>


WHOLE-SEQUENCE IPRSCAN:

++++ Iprscan::collectionsequences_iprscan(dbh, iprscan_output, fields, collection_id, collection_accession)
Query hits: <interpro_matches>

   <protein id="74_3_ORF1" length="360" crc64="EA1645E458B74C10" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="G3DSA:2.20.50.20" name="no description" dbname="GENE3D">
            <location start="23" end="54" score="1.1e-05" status="T" evidence="Gene3D" />
          </match>
          <match id="G3DSA:2.20.80.10" name="no description" dbname="GENE3D">
            <location start="71" end="321" score="3.3e-76" status="T" evidence="Gene3D" />
          </match>
          <match id="PTHR23345" name="VITELLOGENIN-RELATED" dbname="PANTHER">
            <location start="15" end="248" score="1.9e-43" status="T" evidence="HMMPanther" />
          </match>
          <match id="seg" name="seg" dbname="SEG">
            <location start="237" end="248" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>

</interpro_matches>



DIAGNOSIS:

ERROR IN IPRSCAN THAT IT SHORTENS THE LENGTH OF THE ORF BUT THE START POINT OF THE ORF REMAINS THE SAME


SOLUTION:

ACCEPT IPRSCAN HITS FOR THESE MISCALCULATED ORFS (BECAUSE THE IPRSCAN HIT POSITIONS ARE CORRECT ALTHOUGH MAYBE NOT ALL POSSIBLE HITS FOR THESE ORFS ARE OUTPUT)


</entry>



<entry [Wed Sep 26 21:18:52 EDT 2007] IPRSCAN OF UNIQUE ORFS SEEMS AN EFFECTIVE TIME-SAVING STRATEGY>



(FORMATTED AS 'sequences_vs_unique-orfs.xls' IN images FOLDER)

			total_iprscans		non-seg_iprscans	%_reduction
sequences	34647				25566				

unique orfs	25255				18314


mysql> select count(*) from collectionsequencesiprscan;
+----------+
| count(*) |
+----------+
|    36467 |
+----------+
1 row in set (0.00 sec)

mysql> select count(*) from collectionsequencesuniqueorfiprscan;
+----------+
| count(*) |
+----------+
|    25255 |
+----------+
1 row in set (0.00 sec)

mysql> select count(*) from collectionsequencesiprscan where targetsource != 'seg';
+----------+
| count(*) |
+----------+
|    25566 |
+----------+
1 row in set (0.29 sec)

mysql> select count(*) from collectionsequencesiprscan where targetid != 'seg';
+----------+
| count(*) |
+----------+
|    25566 |
+----------+
1 row in set (0.15 sec)

mysql> select count(*) from collectionsequencesiprscan where targetname != 'seg';
+----------+
| count(*) |
+----------+
|    25566 |
+----------+
1 row in set (0.15 sec)

mysql> select count(*) from collectionsequencesuniqueorfiprscan where targetsource !='seg'
    -> ;
+----------+
| count(*) |
+----------+
|    18314 |
+----------+
1 row in set (0.22 sec)



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


http://bio-mirror.mirror.ac.za/iprscan/ReleaseNotes.txt

As of version 4.2:

match_complete.xml FILE IS NOW USED TO LOOKUP MATCHES BY CRC.

	-	contains unintegrated signatures (which have no taxonomy information associated with
  them)
  
match.xml

	-	contains all signatures that have already been integrated into InterPro



</entry>



<entry [Sun Aug 12 18:38:39 EDT 2007] REMOVED SUPERFAMILY FROM 'Supported applications' LIST IN iprscan.conf>



SUPERFAMILY 


sudo em /common/iprscan/conf/iprscan.conf
>>>...
#Supported applications
####applications=blastprodom,coils,gene3d,hmmpanther,hmmpir,hmmpfam,hmmsmart,hmmtigr,fprintscan,scanregexp,profilescan,superfamily,seg
applications=blastprodom,coils,gene3d,hmmpanther,hmmpir,hmmpfam,hmmsmart,hmmtigr,fprintscan,scanregexp,profilescan,seg
...<<<

BECAUSE SUPERFAMILY TAKES MUCH LONGER THAN ANY OTHER APPLICATION TO COMPLETE
BUT IT DOES NOT PROVIDE ANY SIGNIFICANT INCREASE IN THE HIT COVERAGE (5958
WITHOUT SUPERFAMILY, COMPARED TO 5984 WITH SUPERFAMILY).

SUPERFAMILY ALSO SHOWS A HIGH CORRELATION WITH SEVERAL OTHER IPRSCAN DATABASES:

DATABASE NAME                                           CORRELATION

		PFAM			SUPERFAMILY							0.652777778
GENE3D	PANTHER											0.617914381
	PANTHER				SUPERFAMILY							0.549350266
				PROFILE						PRODOM		0.540061725
GENE3D					SUPERFAMILY							0.536739439
GENE3D								PROSITE				0.494413232
GENE3D									SMART			0.494413232
				PROFILE	SUPERFAMILY							0.490990253


THE HIT COVERAGES WERE FOUND USING THIS NEW APPLICATION:


	NAME     collectionsequencesiprscanvennunion.pl
	
	PURPOSE
    
        GET THE UNIONS OF SETS OF HITS IN THE collectionsequencesiprscan
		
		TABLE FOR ALL OF THE DATABASES USED BY iprscan
	
	INPUT
	
		1. collectionsequencesiprscan TABLE
		
		2. DATABASE FAMILIES TO USE IN THE VENN DIAGRAM
		
	OUTPUT
	
		1. FILE CONTAINING COUNTS OF SEQUENCES IN EACH PORTION OF THE VENN DIAGRAM
		
			<database>.collectionsequencesiprscanvennunion.tsv 
		
	NOTES
		
		- USES (non)cumulative_combination AND recurse_combination 
		
			SUBROUTINES OF LOCAL StatsUtil.pm TO CREATE THE ALL
		
			NON-CUMULATIVE, NON-REDUNDANT COMBINATIONS OF THE NUMBERS
			
			FROM 1 TO 12 (OR THE TOTAL NUMBER OF INPUT DATABASES) IN
			
			A COMBINATION OF SPECIFIED LENGTH. THIS SUBROUTINE IS CALLED
			
			ONLY BY THE noncumulative_combination SUBROUTINE.

		- THE cumulative_combination SUBROUTINE CALLS THE noncumulative
		
			TO GENERATE ALL OF THE NON-CUMULATIVE COMBINATIONS OF LENGTH FROM
			
			1 TO 12 (OR THE TOTAL NUMBER OF INPUT DATABASES), WHICH IS
			
			EQUAL TO THE CUMULATIVE COMBINATION.
	
	USAGE
	  
	./collectionsequencesiprscanvennunion.pl <-d database> [-t target_sources] [-h]
	
	-d database         :  name of database
	-h print_help       :  print out help info

	EXAMPLES
	
./collectionsequencesiprscanvennunion.pl -d funnybase11 -t GENE3D,SUPERFAMILY,PANTHER,PFAM,COILS
			

THE OUTPUT FILE CONTAINS THE FOLLOWING:

>>>...
GENE3D	PANTHER	PFAM	SEG	PROFILE	SUPERFAMILY	PRINTS	PIR	PROSITE	SMART	PRODOM		5984
GENE3D	PANTHER	PFAM	SEG	PROFILE	SUPERFAMILY	PRINTS	PIR	PROSITE	SMART		TIGRFAMs	5978
GENE3D	PANTHER	PFAM	SEG	PROFILE	SUPERFAMILY	PRINTS	PIR	PROSITE		PRODOM	TIGRFAMs	5984
GENE3D	PANTHER	PFAM	SEG	PROFILE	SUPERFAMILY	PRINTS	PIR		SMART	PRODOM	TIGRFAMs	5958
GENE3D	PANTHER	PFAM	SEG	PROFILE	SUPERFAMILY	PRINTS		PROSITE	SMART	PRODOM	TIGRFAMs	5984
GENE3D	PANTHER	PFAM	SEG	PROFILE	SUPERFAMILY		PIR	PROSITE	SMART	PRODOM	TIGRFAMs	5983
GENE3D	PANTHER	PFAM	SEG	PROFILE		PRINTS	PIR	PROSITE	SMART	PRODOM	TIGRFAMs	5958
GENE3D	PANTHER	PFAM	SEG		SUPERFAMILY	PRINTS	PIR	PROSITE	SMART	PRODOM	TIGRFAMs	5983
GENE3D	PANTHER	PFAM		PROFILE	SUPERFAMILY	PRINTS	PIR	PROSITE	SMART	PRODOM	TIGRFAMs	4156
GENE3D	PANTHER		SEG	PROFILE	SUPERFAMILY	PRINTS	PIR	PROSITE	SMART	PRODOM	TIGRFAMs	5971
GENE3D		PFAM	SEG	PROFILE	SUPERFAMILY	PRINTS	PIR	PROSITE	SMART	PRODOM	TIGRFAMs	5871
	PANTHER	PFAM	SEG	PROFILE	SUPERFAMILY	PRINTS	PIR	PROSITE	SMART	PRODOM	TIGRFAMs	5982
GENE3D	PANTHER	PFAM	SEG	PROFILE	SUPERFAMILY	PRINTS	PIR	PROSITE	SMART	PRODOM	TIGRFAMs	5984
<<<


THE INTERSECTIONS OF THE SETS OF HITS AGAINST DIFFERENT IPRSCAN DATABASES
WERE FOUND USING THIS NEW APPLICATION (LATER ON CAN BE USED IN A VENN DIAGRAM):

	NAME     collectionsequencesiprscanvennintersection.pl
	
	PURPOSE
    
        GET THE INTERSECTIONS OF SETS OF HITS IN THE collectionsequencesiprscan
		
		TABLE FOR ALL OF THE DATABASES USED BY iprscan
	
	INPUT
	
		1. collectionsequencesiprscan TABLE
		
		2. DATABASE FAMILIES TO USE IN THE VENN DIAGRAM
		
	OUTPUT
	
		1. FILE CONTAINING COUNTS OF SEQUENCES IN EACH PORTION OF THE VENN DIAGRAM
		
			<database>.collectionsequencesiprscanvennintersection.tsv 
		
	NOTES
		
		- USES (non)cumulative_combination AND recurse_combination 
		
			SUBROUTINES OF LOCAL StatsUtil.pm TO CREATE THE ALL
		
			NON-CUMULATIVE, NON-REDUNDANT COMBINATIONS OF THE NUMBERS
			
			FROM 1 TO 12 (OR THE TOTAL NUMBER OF INPUT DATABASES) IN
			
			A COMBINATION OF SPECIFIED LENGTH. THIS SUBROUTINE IS CALLED
			
			ONLY BY THE noncumulative_combination SUBROUTINE.

		- THE cumulative_combination SUBROUTINE CALLS THE noncumulative
		
			TO GENERATE ALL OF THE NON-CUMULATIVE COMBINATIONS OF LENGTH FROM
			
			1 TO 12 (OR THE TOTAL NUMBER OF INPUT DATABASES), WHICH IS
			
			EQUAL TO THE CUMULATIVE COMBINATION.
	
	USAGE
	  
	./collectionsequencesiprscanvennintersection.pl <-d database> [-t target_sources] [-h]
	
	-d database         :  name of database
	-h print_help       :  print out help info

	EXAMPLES
	
./collectionsequencesiprscanvennintersection.pl -d funnybase11 -t GENE3D,SUPERFAMILY,PANTHER,PFAM,COILS
			


THE CORRELATIONS BETWEEN IPRSCAN HITS WERE FOUND USING THIS NEW APPLICATION
(NOTE THIS ALSO SUGGESTED PANTHER AND PFAM, PROFILE AND SMART AND OTHER
DATABASE PAIRS COULD BE MUTUALLY EXCLUDED BECAUSE OF THEIR HIGH DEGREE OF
CORRELATION):

	NAME     collectionsequencesiprscancorrelation.pl
	
	PURPOSE
    
        GET THE PAIRWISE CORRELATION BETWEEN ALL OF THE DATABASES USED BY iprscan
		
		OF HITS IN THE collectionsequencesiprscan TABLE
		
	
	INPUT
	
		1. collectionsequencesiprscan TABLE
		
		2. DATABASE FAMILIES TO BE CORRELATED
		
	OUTPUT
	
		1. FILE CONTAINING DATABASE PAIRS AND CORRELATION STATISTIC (-1 TO 1)
		
			<database>.collectionsequencesiprscancorrelation.tsv 
		
	NOTES
	
		- USES Correlation OBJECT FROM THE EXTERNAL MODULE Math::NumberCruncher
		
			TO CALCULATE THE CORRELATION BETWEEN THE ARRAY FOR EACH IPRSCAN
			
			DATABASE OF HIT/NO HIT (1 VS 0) FOR ALL OF THE SEQUENCES:
		
			$correlation = Math::NumberCruncher::Correlation(\@array1,\@array2);
			
			Returns the correlation of two variables. Correlation ranges from 1 to -1,
			
			with a correlation of zero meaning no correlation exists between the two variables.


		- USES noncumulative_combination SUBROUTINE OF LOCAL StatsUtil.pm
		
			MODULE (A SIMPLE COLLECTION OF HANDY SUBROUTINES FOR STATISTICS)
			
			TO CALCULATE THE NON-CUMULATIVE, NON-REDUNDANT PAIRWISE
			
			COMBINATIONS OF THE NUMBERS FROM 1 TO 12 (OR THE TOTAL NUMBER
			
			OF INPUT DATABASES)
			
	USAGE
	  
	./collectionsequencesiprscancorrelation.pl <-d database> [-t target_sources] [-h]
	
	-d database         :  name of database
	-h print_help       :  print out help info

	EXAMPLES
	
./collectionsequencesiprscancorrelation.pl -d funnybase11 -t GENE3D,SUPERFAMILY,PANTHER,PFAM,COILS
			

	PANTHER	PFAM										0.887411967
				PROFILE					SMART			0.788810638
GENE3D		PFAM										0.696310624
						PRINTS	PIR					0.684653197
							PIR	PROSITE				0.684653197
									SMART	PRODOM		0.684653197
		PFAM			SUPERFAMILY							0.652777778
GENE3D	PANTHER											0.617914381
	PANTHER				SUPERFAMILY							0.549350266
				PROFILE						PRODOM		0.540061725
GENE3D					SUPERFAMILY							0.536739439
GENE3D								PROSITE				0.494413232
GENE3D									SMART			0.494413232
				PROFILE	SUPERFAMILY							0.490990253
		PFAM		PROFILE								0.43643578
						PRINTS		PROSITE				0.433333333
	PANTHER			PROFILE								0.387298335
					SUPERFAMILY	PRINTS						0.387298335
					SUPERFAMILY			PROSITE				0.387298335
					SUPERFAMILY				SMART			0.387298335
		PFAM				PRINTS						0.344265186
		PFAM						PROSITE				0.344265186
		PFAM							SMART			0.344265186
GENE3D							PIR					0.3385016
GENE3D										PRODOM		0.3385016
	PANTHER					PRINTS						0.305505046
	PANTHER							PROSITE				0.305505046
	PANTHER								SMART			0.305505046
GENE3D				PROFILE								0.303894871
					SUPERFAMILY		PIR					0.265165043
					SUPERFAMILY					PRODOM		0.265165043
		PFAM					PIR					0.23570226
		PFAM								PRODOM		0.23570226
	PANTHER						PIR					0.209165007
	PANTHER									PRODOM		0.209165007
			SEG	PROFILE								0.169030851
			SEG			PRINTS						0.133333333
			SEG					PROSITE				0.133333333
			SEG						SMART			0.133333333
GENE3D						PRINTS						0.112366644
			SEG				PIR					0.091287093
			SEG							PRODOM		0.091287093
			SEG		SUPERFAMILY							-0.021516574
							PIR			PRODOM		-0.0625
						PRINTS				PRODOM		-0.091287093
							PIR		SMART			-0.091287093
								PROSITE		PRODOM		-0.091287093
GENE3D			SEG									-0.112366644
				PROFILE			PIR					-0.115727512
						PRINTS			SMART			-0.133333333
								PROSITE	SMART			-0.133333333
				PROFILE		PRINTS						-0.169030851
				PROFILE				PROSITE				-0.169030851
	PANTHER		SEG									-0.305505046
		PFAM	SEG									-0.344265186




++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

funnybase11 IPRSCANS SHOW HIGH DEGREE OF DUPLICATION BETWEEN SUPERFAMILY (SLOW) AND PANTHER (FAST)

use funnybase11;
select count(distinct collectionid) from collectionsequencesiprscan where targetsource='panther';
+------------------------------+
| count(distinct collectionid) |
+------------------------------+
|                         3750 |
+------------------------------+

mysql> select count(distinct collectionid) from collectionsequencesiprscan where targetsource='superfamily';
+------------------------------+
| count(distinct collectionid) |
+------------------------------+
|                         2806 |
+------------------------------+

mysql> select count(distinct collectionid) from collectionsequencesiprscan where targetsource='superfamily' or targetsource = 'panther';
+------------------------------+
| count(distinct collectionid) |
+------------------------------+
|                         3975 |
+------------------------------+


</entry>



<entry [Mon Aug  6 18:27:32 EDT 2007] SPED UP iprscan BY STOPPING TROUBLESHOOTING PRINTS TO OUTFILE>



Index.pm LINE 517:

    #my $outfile = "/Users/local/FUNNYBASE/pipeline/orthologues/fasta/iprscan.error";
    #open(OUTFILE, ">>$outfile");
    #print OUTFILE "\n" if $word eq "IPR014039";
    #print OUTFILE $0, "sub getIndex\n"  if $word eq "IPR014039";
    #print OUTFILE "\tword: $word\n" if $word eq "IPR014039";
    #print OUTFILE "\tinsensitive: $insensitive\n" if $word eq "IPR014039";
    #print OUTFILE "\tfile: $file\n" if $word eq "IPR014039";
    #print OUTFILE "\tindex file: $self->{inx}\n" if $word eq "IPR014039";

REPLACED IprMatches.pm v1.4 (CONTAINING PRINTS TO OUTFILE) WITH v1.5

</entry>



<entry [Mon Aug  6 18:22:45 EDT 2007] FIXED IprMatches.pm ERROR ON LINE 421 - NOT AN ARRAY REF WHERE "STRICT">



DID NOT TAKE INTO CONSIDERATION WHERE $go_entry = '':

    $go_entry = $go_entry->[0]; #get_entry returns an array ref. A database member acc/i

BUT FIXED IT BY ADDING THIS BEFORE LINE 421:

    if ( not ref($go_entry) eq "ARRAY" )
    { 
      return(1, '');  
    }


NB: CHECK WHY Index::getEntry IS NOT RETURNING AN ARRAY REF FOR SOME SEQUENCES

</entry>



<entry [Mon Aug  6 17:46:27 EDT 2007] PROBLEM PARSING FOR PANTHER WITH LOCATION>



>566.2
MDITVREHDFKHGFIKSNSTFDGLNIDNSKNKKKIQKGFQILYVLLFCSVMCGLFYYVYENVWLQRDNEMNEILKNSEHLTIGFKVENAHDRILKTIKTHKLKNYIKESVNFLNSGLTKTNYLGSSNDNIELVDFQNIMFYGDAEVGDNQQPFTFI\
LDTGSANLWVPSVKCTTAGCLTKHLYDSSKSRTYEKDGTKVEMNYVSGTVSGFFSKDLVTVGNLSLPYKFIEVIDTNGFEPTYTASTFDGILGLGWKDLSIGSVDPIVVELKNQNKIENALFTFYLPVHDKHTGFLTIGGIEERFYEGPLTYEKLN\
HDLYWQITLDAHVGNIMLEKANCIVDSGTSAITVPTDFLNKMLQNLDVIKVPFLPFYVTLCNNSKLPTFEFTSENGKYTLEPEYYLQHIEDVGPGLCMLNIIGLDFPVPTFILGDPFMRKYFTVFDYDNQSVGIALAKKNL

supervise: doRawResults: failed to create raw result: createRawResult: cannot parse result Parsing Problem for Panther with location <br><br>To help us solv\
e the problem, please copy and paste this error<br>and email it to the administrator <a href="mailto:nobody@localhost.com?subject=Error in InterProScan job \
iprscan-20070806-17415505">nobody@localhost.com</a>.<br><br>Sorry about the inconvenience and thank you for using InterProScan.<br><br>

LOOKED IN PANTHER OUTPUT:

cd /common/iprscan/tmp/20070806/iprscan-20070806-17415505/chunk_1
ll | grep panther

    -rwxr-xr-x    1 root  admin     1K Aug  6 17:43 iprscan-20070806-17415505-hmmpanther-cnk1.dcmd
    -rw-rw-rw-    1 root  admin     6B Aug  6 17:43 iprscan-20070806-17415505-hmmpanther-cnk1.djob
    -rwxr-xr-x    1 root  admin   378B Aug  6 17:43 iprscan-20070806-17415505-hmmpanther-cnk1.dsub
    -rw-r--r--    1 root  admin     0B Aug  6 17:43 iprscan-20070806-17415505-hmmpanther-cnk1.errors
    -rw-r--r--    1 root  admin     5B Aug  6 17:43 iprscan-20070806-17415505-hmmpanther-cnk1.exitcode
    -rw-r--r--    1 root  admin     1K Aug  6 17:43 iprscan-20070806-17415505-hmmpanther-cnk1.output

em iprscan-20070806-17415505-hmmpanther-cnk1.output

>>>
pantherHmmScoreBlast.pl - a script to score protein sequences against the
PANTHER HMM library. First, uses blast to prefilter results and determine
which HMMs to score against


score_hmm - a program score HMMs in a library against a DB

Usage:
    pantherScore.pl -l <PANTHER library> -D B -V
                    -i <fasta file> -o <output file>


Where args are:
        -h for help (this message)
        -l PANTER (l)ibrary with HMMs
        -D display type for results
                options: I (interproscan), B (best hit), A (all hits)
        -E user defined hmm (E)value cutoff to over ride default
                default: depends on display type; determined by PANTHER group
        -B user defined path to (B)last binary
                default: blastall binary in $PATH
        -H user defined path to (H)mmsearch binary
                default: hmmsearch binary in $PATH
        -z user defined path to gun(z)ip path
                default: gunzip binary in $PATH
        -n to display family and subfamily names in the output
        -T user defined path for tmp directory
                default: /usr/tmp/
        -P P=1 to for (P)roduction format of lib; P=0 for build version
                default: 1
        -J J=1 to for (J)oin alignments in output; J=0 to leave unjoined
        if J=1, if gaps between alignment ranges are <= 15 positions, then the
        alignment ranges are joined
                default: 1
        -t for entry (t)ype
                default: nr
        -n to display family and subfamily names in the output
        -c for number of (c)pu on machine to use for blast/hmmsearch
                default: 1
        -i (i)nput fasta file to score
        -o (o)utput file (redirect STDOUT)
        -e (e)rror file (redirect STDERR)
        -v (v)erbose (debug info to STDERR)
Error: User defined tmp dir path /common/iprscan/tmp/tmp cannot be accessed
<<<

SO CHANGED PERMISSIONS ON /common/iprscan/tmp/tmp TO 777 WHICH FIXED THE PROBLEM:

sudo ./orthologuesarrayiprscan.pl -d inparanoid_sanPF_ensAG
    Doing iprscan for all orthologues (total 3496)
    TEST Orthologue counter: 1289
    Input file: /Users/young/FUNNYBASE/pipeline/inparanoid_sanPF_ensAG/iprscan/inparanoid_sanPF_ensAG.fasta.566.2
    Printing query file... done.
    Executing IPRSCAN for sequence number 1289 (Mon Aug  6 17:51:32 EDT 2007)
    Stdoutfile: /Users/young/FUNNYBASE/pipeline/inparanoid_sanPF_ensAG/iprscan/inparanoid_sanPF_ensAG.fasta.566.2.all.STDOUT
    Stderrfile: /Users/young/FUNNYBASE/pipeline/inparanoid_sanPF_ensAG/iprscan/inparanoid_sanPF_ensAG.fasta.566.2.all.STDERR
    
    486
    Completed: 1

em /Users/young/FUNNYBASE/pipeline/inparanoid_sanPF_ensAG/iprscan/inparanoid_sanPF_ensAG.out.566.2
>>>
<interpro_matches>

   <protein id="566.2" length="453" crc64="0D78A8B9600C80B5" >
        <interpro id="IPR001461" name="Peptidase A1, pepsin" type="Family">
...<<<


</entry>



<entry [Mon Aug  6 11:38:50 EDT 2007] CHANGED finished_jobs >



finished_jobs

SunGridEngine software stores a certain number of just finished jobs to provide
post mortem status information. The finished_jobs parameter defines the number of
finished jobs being stored. If this maximum number is reached, the eldest finished
job will be discarded for every now job being added to the finished job list.

Changing finished_jobs will take immediate effect.

DEFAULT VALUE OF finished_jobs IS 100:

qconf -sconf


global:
execd_spool_dir              /common/sge/default/spool
mailer                       /usr/bin/mail
xterm                        /usr/bin/X11/xterm
load_sensor                  none
prolog                       none
epilog                       none
shell_start_mode             posix_compliant
login_shells                 sh,ksh,csh,tcsh
min_uid                      0
min_gid                      0
user_lists                   none
xuser_lists                  none
projects                     none
xprojects                    none
enforce_project              false
enforce_user                 auto
load_report_time             00:00:40
max_unheard                  00:05:00
reschedule_unknown           00:00:00
loglevel                     log_info
administrator_mail           local@gems.rsmas.miami.edu
set_token_cmd                none
pag_cmd                      none
token_extend_time            none
shepherd_cmd                 none
qmaster_params               none
execd_params                 none
reporting_params             accounting=true reporting=false \
                             flush_time=00:00:15 joblog=false sharelog=00:00:00
finished_jobs                100
gid_range                    20000-20100
qlogin_command               telnet
qlogin_daemon                /usr/libexec/telnetd
rlogin_daemon                /usr/libexec/rlogind
max_aj_instances             2000
max_aj_tasks                 75000
max_u_jobs                   0
max_jobs                     0
auto_user_oticket            0
auto_user_fshare             0
auto_user_default_project    none
auto_user_delete_time        86400
delegated_file_staging       false
reprioritize                 0


CHANGE VALUE OF finished_jobs TO 1000 USING THE -mconf OPTION OF qconf:

sudo qconf -mconf

AND ALTERING THE finished_jobs LINE IN THE vi EDITOR BY HITTING 'ESC':

finished_jobs 1000

AND USING :wq TO SAVE AND QUIT, AND THEN CONFIRM ALTERED LINE:

qconf -sconf



global:
execd_spool_dir              /common/sge/default/spool
mailer                       /usr/bin/mail
xterm                        /usr/bin/X11/xterm
load_sensor                  none
prolog                       none
epilog                       none
shell_start_mode             posix_compliant
login_shells                 sh,ksh,csh,tcsh
min_uid                      0
min_gid                      0
user_lists                   none
xuser_lists                  none
projects                     none
xprojects                    none
enforce_project              false
enforce_user                 auto
load_report_time             00:00:40
max_unheard                  00:05:00
reschedule_unknown           00:00:00yp6
loglevel                     log_info
administrator_mail           local@gems.rsmas.miami.edu
set_token_cmd                none
pag_cmd                      none007 from dlc-genomics.rs
token_extend_time            none
shepherd_cmd                 none
qmaster_params               none
execd_params                 none
reporting_params             accounting=true reporting=false \
                             flush_time=00:00:15 joblog=false sharelog=00:00:00
finished_jobs                1000
gid_range                    20000-20100
qlogin_command               telnet
qlogin_daemon                /usr/libexec/telnetd
rlogin_daemon                /usr/libexec/rlogind
max_aj_instances             2000
max_aj_tasks                 75000
max_u_jobs                   0
max_jobs                     0
auto_user_oticket            0
auto_user_fshare             0
auto_user_default_project    none
auto_user_delete_time        86400
delegated_file_staging       false
reprioritize                 0


</entry>



<entry [Mon Aug  6 11:16:44 EDT 2007] INSTALLED PATCHES FOR iprscan>



PATCH FILES ARE FOUND IN ~/FUNNYBASE/dev/iprscan/070123.PATCH/patch.files DIRECTORY:

Date 	File 	Instructions
17th Jan 2007 	interpro.xml 	The interpro.xml file was not updated to release 14.0 in the InterProScan data release (iprscan_DATA_14.0.tar.gz). Users who downloaded the file before today should download the interpro.xml.gz file to their iprscan/data directory, gunzip it and re-index it using index_data.pl -f interpro.xml -inx -iforce. The tar file now contains the correct version of the file

    SKIPPED THIS BECAUSE NOT SURE WHICH VERSION WAS DOWNLOADED BY bioteam SUPPORT SO DOWNLOADED
    VERSION 15.0 INSTEAD.
    
    NB: DATA_14.0 (RELEASED ON 13-12-2006)
    
    
16th Jan 2007 	FingerPrintScan.pm 	The start position of a match is occasionally reported as negative (i.e. less than 1) in the FingerPrintScan program. This means that the converter.pl script cannot parse the raw results into other formats, such as XML. This affects a very small number of signatures, however it means that for these matches no result is reported in the XML file and instead an empty record is output. To fix this, please copy this file to your iprscan/lib/Dispatcher/Tool/ directory.

    COPIED v1.3 (2007/01/16) OF FingerPrintScan.pm (REPLACING TO v1.2) iprscan/lib/Dispatcher/Tool DIRECTORY.

10th Jan 2007 	converter.pl 	Fixes bug where, if the description of a signature contains a ">" character, the txt format is not produced correctly. This affects a very small number of signatures. Please copy this file to your iprscan/bin directory. Then replace the top line (shebang) with the path to your perl installation and edit the value of $IPRSCAN_HOME so that it is the iprscan root directory (e.g. /usr/software/bio/iprscan ).

    converter.pl -format <format> -input <raw file> -jobid <jobid>  > output_file
        - Is used to reformat results from raw into [html, xml, ebixml, txt,
          gff3] format.
        - NOTE: ebixml format just adds an EBI header to the top of the xml file.
        - NOTE: to get gff3 format, you must first run iprscan and output raw
         format.

    COPIED v1.5 (2007/01/10) OF converter.pl (REPLACING v1.4) TO iprscan/bin


9th Jan 2007 	Patch release 4.3.1 made 	Because of the severity of the two bugs listed below, a patch release has been made. If you have already downloaded 4.3 and updated the InterProScan.pm and IprMatches.pm files as instructed below, you do not need to install this version, however, new installations will automatically be v4.3.1 from now on until the next release. Any questions, please contact support.

    SKIPPED THIS BECAUSE ALREADY PERFORMED PATCHES BELOW.

4th Dec 2006 	IprMatches.pm
Fixes error produced when performing GO term lookup ("getRawEntryFromIprMatches: query iprmatches failed: Can\'t use string (\"\") as an ARRAY ref while \"strict refs\" in use at /share/apps/iprscan/lib/Index/IprMatches.pm line 425.")This can be fixed by copying the new perlmodule from the PATCH directory into iprscan/lib/Index/ and then re-indexing your match_complete.xml file

    COPIED v1.5 OF IprMatches.pm TO lib/Index (REPLACING v1.4) AND REINDEXED THE match_complete.xml FILE:

    sudo perl /common/iprscan/bin/index_data.pl -f match_complete.xml -inx -iforce -v

        WARNING: No data path specified, defaulting to /common/iprscan/data/!
        Creating Tool Object...Done
        Parsing Configuration file...Done
        
        ========================================================
                Indexing for match_complete.xml
        ========================================================
        Creating Index::IprMatches module ... DONE
        
        ... File /common/iprscan/data//match_complete.xml being reindexed ...
        removing /common/iprscan/data//match_complete.xml.inx ... DONE
        Building index from file /common/iprscan/data//match_complete.xml, may take long time ... DONE
        
        Creating Index file for datafile /common/iprscan/data//match_complete.xml ... DONE
        
        
        *********************************************************
        - Indexing for match_complete.xml DONE
        *********************************************************


15th Nov 2006 	InterProScan.pm
There is a bug in the formatSequences() method of the InterProScan.pm perl module that means that when you run a search on a single sequence, an error is produced saying the sequence is zero length. This can be fixed by copying the new perlmodule from the PATCH directory into iprscan/lib/Dispatcher/Tool/.

    COPIED v1.9 (2007/01/08) OF InterProScan.pm (REPLACING v1.8) TO iprscan/lib/Dispatcher/Tool.


THEN TESTED ON test.nuc.seq AND WORKED FINE.


</entry>



<entry [Thu Jun 28 23:56:13 EDT 2007] funnybase11 IPRSCAN FINISHED (8 DAYS)>



CHECK THIS LATER: ONE MINOR ERROR WITH coils - CHECK coils.conf FILE

========================================================================
Chunk: 1
Jobid: iprscan-20070628-06124851-coils-cnk1
Application: coils
Please fix error(s) if necessary or check 'coils.conf' file.
Errors :

       2 sequences      202 aas        0 in coil


========================================================================

em /common/iprscan/conf/coils.conf

>>>
# generic definitions
queue=sge6
usergroup=
toolgroup=iprscan
jobname=[%jobid]

# queue/local definitions
resource=
queue.name=
host.exec=

# application name used with InterProScan
applname=Coil

# matrix
matrix=[%env IPRSCAN_HOME]/data/new_coil.mat

# work directory, depends on search type
date=[%YYYY][%MM][%DD]
sworkdir=[%env IPRSCAN_HOME]/tmp/[%date]/[%session]/[%chunk]
aworkdir=[%env IPRSCAN_HOME]/tmp/[%date]/[%session]/[%chunk]
workdir=[%if %srchtype eq email ? %aworkdir : %sworkdir]

# work files
tooloutput=[%workdir]/[%jobid].output
toolrunning=[%workdir]/[%jobid].running
toolerrors=[%workdir]/[%jobid].errors
toolexitcode=[%workdir]/[%jobid].exitcode
uniquefiles=toolrunning,toolerrors

# command line
binary=[%env IPRSCAN_HOME]/bin/binaries/ncoils
cmdline=  -c -m [%matrix]

#template configuration for queue and local
template=[%env IPRSCAN_HOME]/bin/executor.pl

# other
recdel=%newline>
htmlerror=Error in Coils job submission
adminaddr=nobody@localhost.com
mailerrorsubject=Error in Coils submission [%jobid]
mailfromaddr=nobody@localhost.com
mailerrorto=
<<<

CHECKED THAT new_coil.mat IS IN THE DATA DIRECTORY (YES):

ls /common/iprscan/data

FingerPRINTSparser.db   confirm.patterns        prints.pval.inx         sf_hmm
Gene3D.hmm              formatdb.log            prodom.ipr              sf_hmm.bin
Gene3D.hmm.bin          interpro.dtd            prodom.ipr.phr          sf_hmm.inx
Gene3D.hmm.inx          interpro.xml            prodom.ipr.pin          sf_hmm_sub
Panther                 interpro.xml.inx        prodom.ipr.psd          sf_hmm_sub.bin
Pfam                    match.dtd               prodom.ipr.psi          smart.HMMs
Pfam-A.seed             match.xml               prodom.ipr.psq          smart.HMMs.bin
Pfam-A.seed.inx         match.xml.inx           prosite.dat             smart.HMMs.inx
Pfam-C                  match_complete.dtd      prosite.patterns        superfamily.acc
Pfam-C.inx              match_complete.xml      prosite_prerelease.prf  superfamily.hmm
Pfam.bin                match_complete.xml.bkp1 sf.seq                  superfamily.hmm.bin
Pfam.inx                match_complete.xml.inx  sf.seq.phr              superfamily.hmm.inx
TIGRFAMs_HMM.LIB        new_coil.mat            sf.seq.pin              superfamily.tab
TIGRFAMs_HMM.LIB.bin    pirsf.dat               sf.seq.psq
TIGRFAMs_HMM.LIB.inx    prints.pval             sf.tb

AND THAT THE coils BINARY IS PRESENT (YES):

ll /common/iprscan/bin/binaries/ncoils

-rwxr-xr-x   1 www  admin    19K May 13  2003 /common/iprscan/bin/binaries/ncoils




</entry>



<entry [Fri Apr 20 23:20:06 EDT 2007] QSTAT SHOWS NODE 004 IN 'E' STATE>



REMOVE ERROR WITH:

sudo qmod -cq all.q@node004

I disabled them with this:

qmod -d all.q@node012


</entry>



<entry [Thu Apr 12 13:32:17 EDT 2007] QSTAT SHOWS NODE IN 'dr' STATE>



qstat -f
>>>...
all.q@node004.cluster.private  BIP   1/2       0.00     darwin        
 642089 0.55500 readsarray root         dr    04/11/2007 16:13:10     1 12
...<<<


dlc-genomics:~/FUNNYBASE/bin/pipeline4 young$ sudo qdel -f 642089
warning: root forced the deletion of job-array task 642089.12

ENABLE WITH:

qmod -e all.q@node004

DISABLE WITH:

qmod -d all.q@node004


</entry>



<entry [Wed Apr 11 11:52:45 EDT 2007] THREE OF FOUR ORF IPRSCAN PROCESSES QUEUED AT SAME TIME AND FROZE>



root     19744   0.0  0.3    33756   3208  p7  S     1:00AM  23:53.71 /usr/bin/perl /common/iprscan/bin/iprscan -cli -i /Users/local/FUNNYBASE/pipeline/funnybase10/iprscan/funnybase10.fasta.2110.1.all.l30.fr1.orf19.364-537 -o /Users/local/FUNNYBASE/pipeline/funnybase10/iprscan/funnyba
root     19770   0.0  0.4    33756   3952  p3  S     1:00AM  23:56.49 /usr/bin/perl /common/iprscan/bin/iprscan -cli -i /Users/local/FUNNYBASE/pipeline/funnybase10/iprscan/funnybase10.fasta.2111.1.all.l30.fr1.orf1.1-177 -o /Users/local/FUNNYBASE/pipeline/funnybase10/iprscan/funnybase1
root     20178   0.0  0.4    33756   3876  p2  S     1:00AM  23:44.48 /usr/bin/perl /common/iprscan/bin/iprscan -cli -i /Users/local/FUNNYBASE/pipeline/funnybase10/iprscan/funnybase10.fasta.2108.1.all.l30.fr1.orf12.403-528 -o /Users/local/FUNNYBASE/pipeline/funnybase10/iprscan/funnyba
postfix   9941   0.0  0.1    27372    692  ??  S    10:23AM   0:00.02 pickup -l -t fifo -u

SOLUTION:

1. KILLED THE FROZEN PROCESSES

sudo kill -9 19744 19770 9941
etc.

2. RESTARTED THE ORF IPRSCANS


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Mar 14 17:30:06 EDT 2007

Hi Sarah,

Thanks for getting back to me. I tried using '-nofirst -nolast' in the 'translatecmd=' line of iprscan.conf
but I still get ORF labels in my iprscan output that don't correspond to the way the ORFs are labelled by sixpack. 

For example, for input sequence 1.1 (results copied below), the second 'protein_id' is a 114aa stretch in frame 3. It is labelled '1.1_3_ORF6' (without '-nofirst -nolast') and '1.1_3_ORF5' (with '-nofirst') or is simply ignored (with '-nofirst -nolast'). However, sixpack gives the 114aa stretch in frame 3 as the 12th ORF:

>1.1_3_ORF1  Translation of 1.1 in frame 3, ORF 1, threshold 1, 8aa
EHNRXPLL
>1.1_3_ORF2  Translation of 1.1 in frame 3, ORF 2, threshold 1, 74aa
PSPATTQCTVTTKAPQPSKGPTAQPSQEPYSTAVQKPYSTALQKPYSRPCWHNSSAFAKT
VRNLWFASTKKGTN
>1.1_3_ORF3  Translation of 1.1 in frame 3, ORF 3, threshold 1, 66aa
DLWGTESQPWGNPLAGFCATKDKELQHEIQPRLRRSSDRKLLGSDGWTLHRTKQGYAGGD
GYLSTG
>1.1_3_ORF4  Translation of 1.1 in frame 3, ORF 4, threshold 1, 8aa
ARSISSNS
>1.1_3_ORF5  Translation of 1.1 in frame 3, ORF 5, threshold 1, 1aa
S
>1.1_3_ORF6  Translation of 1.1 in frame 3, ORF 6, threshold 1, 86aa
HCNCARELQGDFFGCLQRHSFVKTQGGKRILCKRDPVCEDSLSAKWPTHRWDGVHHFWMG
RHRDLGLWHQPPVTGQCSADQPGKVL
>1.1_3_ORF7  Translation of 1.1 in frame 3, ORF 7, threshold 1, 8aa
THCLRKRH
>1.1_3_ORF8  Translation of 1.1 in frame 3, ORF 8, threshold 1, 13aa
WQHVLRRLPAGRG
>1.1_3_ORF9  Translation of 1.1 in frame 3, ORF 9, threshold 1, 15aa
FLPRGFWRAVDLQAE
>1.1_3_ORF10  Translation of 1.1 in frame 3, ORF 10, threshold 1, 83aa
HQCCLWSGELGRSVWKEKQTRGLRKSHQLPGLDQVKDSSIFLNDPVRNSLRTWQHVWPGV
KLWLRVMWCCFITALHLTGLLCN
>1.1_3_ORF11  Translation of 1.1 in frame 3, ORF 11, threshold 1, 22aa
KKDLRSSQHILQYFLTITGKYE
>1.1_3_ORF12  Translation of 1.1 in frame 3, ORF 12, threshold 1, 114aa
NNNKGYLTVKKKKKKKKKKTRKNFKKKKKKNGDRGGGKLSLADVLLVECTLMLEEKFPAI
LKDYPNLKVFPGQDDPDSRHQQVSAAGQQRKPQPDEGYVKTVMEVFNINAPLSL
>1.1_3_ORF13  Translation of 1.1 in frame 3, ORF 13, threshold 1, 17aa
CLANINPVINNNNKKTK
>1.1_3_ORF14  Translation of 1.1 in frame 3, ORF 14, threshold 1, 9aa
VIQTLCEHL
>1.1_3_ORF15  Translation of 1.1 in frame 3, ORF 15, threshold 1, 17aa
LELCWKITTKKKLTHPI
>1.1_3_ORF16  Translation of 1.1 in frame 3, ORF 16, threshold 1, 37aa
FPQSDTCADPILNNELAYPGHGGVNASKLVTIKFLKK

Please note that, when I used '-nofirst -nolast', I got a new second hit (!) that didn't appear when I didn't use '-nofirst -nolast' (results copied below):

<protein id="1.1_3_ORF5" length="60" crc64="29BF585ECFE62E99" >

I initially though that some kind of cutoff threshold was at work, which would rename ORFs in the order that they passed through the translated length filter. For example, naming the frame 3 translations according to a trlen filter of 30aa would result in the following renamings:

>1.1_3_ORF1  Translation of 1.1 in frame 3, ORF 1, threshold 1, 8aa   ----> IGNORE (TOO SHORT)
EHNRXPLL
>1.1_3_ORF2  Translation of 1.1 in frame 3, ORF 2, threshold 1, 74aa  ---> 1.1_3_ORF1
PSPATTQCTVTTKAPQPSKGPTAQPSQEPYSTAVQKPYSTALQKPYSRPCWHNSSAFAKT
VRNLWFASTKKGTN
>1.1_3_ORF3  Translation of 1.1 in frame 3, ORF 3, threshold 1, 66aa  ---> 1.1_3_ORF2
DLWGTESQPWGNPLAGFCATKDKELQHEIQPRLRRSSDRKLLGSDGWTLHRTKQGYAGGD
GYLSTG
>1.1_3_ORF4  Translation of 1.1 in frame 3, ORF 4, threshold 1, 8aa   ---> IGNORE
ARSISSNS
>1.1_3_ORF5  Translation of 1.1 in frame 3, ORF 5, threshold 1, 1aa   ---> IGNORE
S
>1.1_3_ORF6  Translation of 1.1 in frame 3, ORF 6, threshold 1, 86aa ---> 1.1_3_ORF3
HCNCARELQGDFFGCLQRHSFVKTQGGKRILCKRDPVCEDSLSAKWPTHRWDGVHHFWMG
RHRDLGLWHQPPVTGQCSADQPGKVL
>1.1_3_ORF7  Translation of 1.1 in frame 3, ORF 7, threshold 1, 8aa  ---> IGNORE
THCLRKRH
>1.1_3_ORF8  Translation of 1.1 in frame 3, ORF 8, threshold 1, 13aa---> IGNORE
WQHVLRRLPAGRG
>1.1_3_ORF9  Translation of 1.1 in frame 3, ORF 9, threshold 1, 15aa---> IGNORE
FLPRGFWRAVDLQAE
>1.1_3_ORF10  Translation of 1.1 in frame 3, ORF 10, threshold 1, 83aa ---> 1.1_3_ORF4
HQCCLWSGELGRSVWKEKQTRGLRKSHQLPGLDQVKDSSIFLNDPVRNSLRTWQHVWPGV
KLWLRVMWCCFITALHLTGLLCN
>1.1_3_ORF11  Translation of 1.1 in frame 3, ORF 11, threshold 1, 22aa ---> IGNORE
KKDLRSSQHILQYFLTITGKYE
>1.1_3_ORF12  Translation of 1.1 in frame 3, ORF 12, threshold 1, 114aa ---> 1.1_3_ORF5
NNNKGYLTVKKKKKKKKKKTRKNFKKKKKKNGDRGGGKLSLADVLLVECTLMLEEKFPAI
LKDYPNLKVFPGQDDPDSRHQQVSAAGQQRKPQPDEGYVKTVMEVFNINAPLSL
>1.1_3_ORF13  Translation of 1.1 in frame 3, ORF 13, threshold 1, 17aa   ---> IGNORE
CLANINPVINNNNKKTK
>1.1_3_ORF14  Translation of 1.1 in frame 3, ORF 14, threshold 1, 9aa     ---> IGNORE
VIQTLCEHL
>1.1_3_ORF15  Translation of 1.1 in frame 3, ORF 15, threshold 1, 17aa   ---> IGNORE
LELCWKITTKKKLTHPI
>1.1_3_ORF16  Translation of 1.1 in frame 3, ORF 16, threshold 1, 37aa   ---> 1.1_3_ORF6
FPQSDTCADPILNNELAYPGHGGVNASKLVTIKFLKK

But it transpired that setting a cutoff of 30aa (the default 'trlen' threshold) and checking which ORFs passed through did not result in a numbering that was consistent with the iprscan output.

Here are my iprscan outputs for 1) without '-nofirst -nolast', 2) with both, and 3)with only '-nofirst':

1)  without '-nofirst -nolast'

<interpro_matches>

   <protein id="1.1_1_ORF2" length="310" crc64="009671DE85AA1F2F" >
	<interpro id="IPR001254" name="Peptidase S1 and S6, chymotrypsin/Hap" type="Domain" parent_id="IPR009003">
	  <child_list>
	    <rel_ref ipr_ref="IPR001314"/>
	  </child_list>
	  <found_in>
	    <rel_ref ipr_ref="IPR001316"/>
	    <rel_ref ipr_ref="IPR001940"/>
	    <rel_ref ipr_ref="IPR008256"/>
	    <rel_ref ipr_ref="IPR008353"/>
	    <rel_ref ipr_ref="IPR011782"/>
	    <rel_ref ipr_ref="IPR011783"/>
	 </found_in>
	  <contains>
	    <rel_ref ipr_ref="IPR000126"/>
	  </contains>
	  <classification id="GO:0004252" class_type="GO">
	    <category>Molecular Function</category>
	    <description>serine-type endopeptidase activity</description>
	  </classification>
	  <classification id="GO:0006508" class_type="GO">
	    <category>Biological Process</category>
	    <description>proteolysis</description>
	  </classification>
	  <match id="PF00089" name="Trypsin" dbname="PFAM">
	    <location start="66" end="302" score="7.6e-85" status="T" evidence="HMMPfam" />
	  </match>
	  <match id="SM00020" name="no description" dbname="SMART">
	    <location start="65" end="302" score="1.7e-92" status="T" evidence="HMMSmart" />
	  </match>
	  <match id="PS50240" name="TRYPSIN_DOM" dbname="PROFILE">
	    <location start="66" end="307" score="36.461" status="T" evidence="ProfileScan" />
	  </match>
	  <match id="PS00134" name="TRYPSIN_HIS" dbname="PROSITE">
	    <location start="109" end="114" score="NA" status="?" evidence="ScanRegExp" />
	  </match>
	  <match id="PS00135" name="TRYPSIN_SER" dbname="PROSITE">
	    <location start="253" end="264" score="8e-5" status="T" evidence="ScanRegExp" />
	  </match>
	</interpro>
	<interpro id="IPR001314" name="Peptidase S1A, chymotrypsin" type="Domain" parent_id="IPR001254">
	  <found_in>
	    <rel_ref ipr_ref="IPR008292"/>
	    <rel_ref ipr_ref="IPR008293"/>
	    <rel_ref ipr_ref="IPR011163"/>
	    <rel_ref ipr_ref="IPR011164"/>
	    <rel_ref ipr_ref="IPR011357"/>
	    <rel_ref ipr_ref="IPR011358"/>
	    <rel_ref ipr_ref="IPR011359"/>
	    <rel_ref ipr_ref="IPR011360"/>
	    <rel_ref ipr_ref="IPR011361"/>
	    <rel_ref ipr_ref="IPR012051"/>
	    <rel_ref ipr_ref="IPR012224"/>
	    <rel_ref ipr_ref="IPR012267"/>
	 </found_in>
	  <classification id="GO:0004252" class_type="GO">
	    <category>Molecular Function</category>
	    <description>serine-type endopeptidase activity</description>
	  </classification>
	  <classification id="GO:0006508" class_type="GO">
	    <category>Biological Process</category>
	    <description>proteolysis</description>
	  </classification>
	  <match id="PR00722" name="CHYMOTRYPSIN" dbname="PRINTS">
	    <location start="99" end="114" score="2.9e-13" status="T" evidence="FPrintScan" />
	    <location start="157" end="171" score="2.9e-13" status="T" evidence="FPrintScan" />
	    <location start="252" end="264" score="2.9e-13" status="T" evidence="FPrintScan" />
	  </match>
	</interpro>
	<interpro id="IPR009003" name="Peptidase, trypsin-like serine and cysteine" type="Domain">
	  <child_list>
	    <rel_ref ipr_ref="IPR000930"/>
	    <rel_ref ipr_ref="IPR001254"/>
	    <rel_ref ipr_ref="IPR004109"/>
	    <rel_ref ipr_ref="IPR011565"/>
	  </child_list>
	  <contains>
	    <rel_ref ipr_ref="IPR000126"/>
	    <rel_ref ipr_ref="IPR000199"/>
	  </contains>
	  <match id="SSF50494" name="Trypsin-like serine proteases" dbname="SUPERFAMILY">
	    <location start="53" end="307" score="5.6e-76" status="T" evidence="superfamily" />
	  </match>
	</interpro>
	<interpro id="noIPR" name="unintegrated" type="unintegrated">
	  <match id="G3DSA:2.40.10.10" name="no description" dbname="GENE3D">
	    <location start="98" end="304" score="9.1e-58" status="T" evidence="Gene3D" />
	  </match>
	  <match id="PTHR19355" name="SERINE PROTEASE-RELATED" dbname="PANTHER">
	    <location start="61" end="303" score="1.9e-80" status="T" evidence="HMMPanther" />
	  </match>
	  <match id="PTHR19355:SF34" name="HYALURONAN BINDING PROTEIN 2" dbname="PANTHER">
	    <location start="61" end="303" score="1.9e-80" status="T" evidence="HMMPanther" />
	  </match>
	</interpro>
   </protein>
   <protein id="1.1_3_ORF6" length="114" crc64="9DBE09F69E45B687" >
	<interpro id="IPR004046" name="Glutathione S-transferase, C-terminal" type="Domain" parent_id="IPR010987">
	  <found_in>
	    <rel_ref ipr_ref="IPR003080"/>
	    <rel_ref ipr_ref="IPR003081"/>
	    <rel_ref ipr_ref="IPR003082"/>
	    <rel_ref ipr_ref="IPR003083"/>
	    <rel_ref ipr_ref="IPR005442"/>
	    <rel_ref ipr_ref="IPR005955"/>
	 </found_in>
	  <match id="PF00043" name="GST_C" dbname="PFAM">
	    <location start="35" end="70" score="5.1e-05" status="T" evidence="HMMPfam" />
	  </match>
	</interpro>
	<interpro id="IPR010987" name="Glutathione S-transferase, C-terminal-like" type="Domain">
	  <child_list>
	    <rel_ref ipr_ref="IPR004046"/>
	    <rel_ref ipr_ref="IPR007494"/>
	  </child_list>
	  <found_in>
	    <rel_ref ipr_ref="IPR002946"/>
	    <rel_ref ipr_ref="IPR005442"/>
	    <rel_ref ipr_ref="IPR005955"/>
	 </found_in>
	  <match id="SSF47616" name="Glutathione S-transferase (GST), C-terminal domain" dbname="SUPERFAMILY">
	    <location start="36" end="92" score="0.00037" status="T" evidence="superfamily" />
	  </match>
	</interpro>
	<interpro id="noIPR" name="unintegrated" type="unintegrated">
	  <match id="PTHR11571" name="GLUTATHIONE S-TRANSFERASE" dbname="PANTHER">
	    <location start="36" end="107" score="4e-11" status="T" evidence="HMMPanther" />
	  </match>
	  <match id="PTHR11571:SF4" name="GLUTATHIONE S-TRANSFERASE CLASS ALPHA" dbname="PANTHER">
	    <location start="36" end="107" score="4e-11" status="T" evidence="HMMPanther" />
	  </match>
	  <match id="seg" name="seg" dbname="SEG">
	    <location start="10" end="30" score="NA" status="?" evidence="Seg" />
	  </match>
	</interpro>
   </protein>

</interpro_matches>


2) with '-nofirst -nolast'


<interpro_matches>

   <protein id="1.1_1_ORF1" length="310" crc64="009671DE85AA1F2F" >
	<interpro id="IPR001254" name="Peptidase S1 and S6, chymotrypsin/Hap" type="Domain" parent_id="IPR009003">
	  <child_list>
	    <rel_ref ipr_ref="IPR001314"/>
	  </child_list>
	  <found_in>
	    <rel_ref ipr_ref="IPR001316"/>
	    <rel_ref ipr_ref="IPR001940"/>
	    <rel_ref ipr_ref="IPR008256"/>
	    <rel_ref ipr_ref="IPR008353"/>
	    <rel_ref ipr_ref="IPR011782"/>
	    <rel_ref ipr_ref="IPR011783"/>
	 </found_in>
	  <contains>
	    <rel_ref ipr_ref="IPR000126"/>
	  </contains>
	  <classification id="GO:0004252" class_type="GO">
	    <category>Molecular Function</category>
	    <description>serine-type endopeptidase activity</description>
	  </classification>
	  <classification id="GO:0006508" class_type="GO">
	    <category>Biological Process</category>
	    <description>proteolysis</description>
	  </classification>
	  <match id="PF00089" name="Trypsin" dbname="PFAM">
	    <location start="66" end="302" score="7.6e-85" status="T" evidence="HMMPfam" />
	  </match>
	  <match id="SM00020" name="no description" dbname="SMART">
	    <location start="65" end="302" score="1.7e-92" status="T" evidence="HMMSmart" />
	  </match>
	  <match id="PS50240" name="TRYPSIN_DOM" dbname="PROFILE">
	    <location start="66" end="307" score="36.461" status="T" evidence="ProfileScan" />
	  </match>
	  <match id="PS00134" name="TRYPSIN_HIS" dbname="PROSITE">
	    <location start="109" end="114" score="NA" status="?" evidence="ScanRegExp" />
	  </match>
	  <match id="PS00135" name="TRYPSIN_SER" dbname="PROSITE">
	    <location start="253" end="264" score="8e-5" status="T" evidence="ScanRegExp" />
	  </match>
	</interpro>
	<interpro id="IPR001314" name="Peptidase S1A, chymotrypsin" type="Domain" parent_id="IPR001254">
	  <found_in>
	    <rel_ref ipr_ref="IPR008292"/>
	    <rel_ref ipr_ref="IPR008293"/>
	    <rel_ref ipr_ref="IPR011163"/>
	    <rel_ref ipr_ref="IPR011164"/>
	    <rel_ref ipr_ref="IPR011357"/>
	    <rel_ref ipr_ref="IPR011358"/>
	    <rel_ref ipr_ref="IPR011359"/>
	    <rel_ref ipr_ref="IPR011360"/>
	    <rel_ref ipr_ref="IPR011361"/>
	    <rel_ref ipr_ref="IPR012051"/>
	    <rel_ref ipr_ref="IPR012224"/>
	    <rel_ref ipr_ref="IPR012267"/>
	 </found_in>
	  <classification id="GO:0004252" class_type="GO">
	    <category>Molecular Function</category>
	    <description>serine-type endopeptidase activity</description>
	  </classification>
	  <classification id="GO:0006508" class_type="GO">
	    <category>Biological Process</category>
	    <description>proteolysis</description>
	  </classification>
	  <match id="PR00722" name="CHYMOTRYPSIN" dbname="PRINTS">
	    <location start="99" end="114" score="2.9e-13" status="T" evidence="FPrintScan" />
	    <location start="157" end="171" score="2.9e-13" status="T" evidence="FPrintScan" />
	    <location start="252" end="264" score="2.9e-13" status="T" evidence="FPrintScan" />
	  </match>
	</interpro>
	<interpro id="IPR009003" name="Peptidase, trypsin-like serine and cysteine" type="Domain">
	  <child_list>
	    <rel_ref ipr_ref="IPR000930"/>
	    <rel_ref ipr_ref="IPR001254"/>
	    <rel_ref ipr_ref="IPR004109"/>
	    <rel_ref ipr_ref="IPR011565"/>
	  </child_list>
	  <contains>
	    <rel_ref ipr_ref="IPR000126"/>
	    <rel_ref ipr_ref="IPR000199"/>
	  </contains>
	  <match id="SSF50494" name="Trypsin-like serine proteases" dbname="SUPERFAMILY">
	    <location start="53" end="307" score="5.6e-76" status="T" evidence="superfamily" />
	  </match>
	</interpro>
	<interpro id="noIPR" name="unintegrated" type="unintegrated">
	  <match id="G3DSA:2.40.10.10" name="no description" dbname="GENE3D">
	    <location start="98" end="304" score="9.1e-58" status="T" evidence="Gene3D" />
	  </match>
	  <match id="PTHR19355" name="SERINE PROTEASE-RELATED" dbname="PANTHER">
	    <location start="61" end="303" score="1.9e-80" status="T" evidence="HMMPanther" />
	  </match>
	  <match id="PTHR19355:SF34" name="HYALURONAN BINDING PROTEIN 2" dbname="PANTHER">
	    <location start="61" end="303" score="1.9e-80" status="T" evidence="HMMPanther" />
	  </match>
	</interpro>
   </protein>
   <protein id="1.1_3_ORF5" length="60" crc64="29BF585ECFE62E99" >
	<interpro id="noIPR" name="unintegrated" type="unintegrated">
	  <match id="seg" name="seg" dbname="SEG">
	    <location start="10" end="30" score="NA" status="?" evidence="Seg" />
	  </match>
	</interpro>
   </protein>

</interpro_matches>


3)with only '-nofirst':

<interpro_matches>

   <protein id="1.1_1_ORF1" length="310" crc64="009671DE85AA1F2F" >
	<interpro id="IPR001254" name="Peptidase S1 and S6, chymotrypsin/Hap" type="Domain" parent_id="IPR009003">
	  <child_list>
	    <rel_ref ipr_ref="IPR001314"/>
	  </child_list>
	  <found_in>
	    <rel_ref ipr_ref="IPR001316"/>
	    <rel_ref ipr_ref="IPR001940"/>
	    <rel_ref ipr_ref="IPR008256"/>
	    <rel_ref ipr_ref="IPR008353"/>
	    <rel_ref ipr_ref="IPR011782"/>
	    <rel_ref ipr_ref="IPR011783"/>
	 </found_in>
	  <contains>
	    <rel_ref ipr_ref="IPR000126"/>
	  </contains>
	  <classification id="GO:0004252" class_type="GO">
	    <category>Molecular Function</category>
	    <description>serine-type endopeptidase activity</description>
	  </classification>
	  <classification id="GO:0006508" class_type="GO">
	    <category>Biological Process</category>
	    <description>proteolysis</description>
	  </classification>
	  <match id="PF00089" name="Trypsin" dbname="PFAM">
	    <location start="66" end="302" score="7.6e-85" status="T" evidence="HMMPfam" />
	  </match>
	  <match id="SM00020" name="no description" dbname="SMART">
	    <location start="65" end="302" score="1.7e-92" status="T" evidence="HMMSmart" />
	  </match>
	  <match id="PS50240" name="TRYPSIN_DOM" dbname="PROFILE">
	    <location start="66" end="307" score="36.461" status="T" evidence="ProfileScan" />
	  </match>
	  <match id="PS00134" name="TRYPSIN_HIS" dbname="PROSITE">
	    <location start="109" end="114" score="NA" status="?" evidence="ScanRegExp" />
	  </match>
	  <match id="PS00135" name="TRYPSIN_SER" dbname="PROSITE">
	    <location start="253" end="264" score="8e-5" status="T" evidence="ScanRegExp" />
	  </match>
	</interpro>
	<interpro id="IPR001314" name="Peptidase S1A, chymotrypsin" type="Domain" parent_id="IPR001254">
	  <found_in>
	    <rel_ref ipr_ref="IPR008292"/>
	    <rel_ref ipr_ref="IPR008293"/>
	    <rel_ref ipr_ref="IPR011163"/>
	    <rel_ref ipr_ref="IPR011164"/>
	    <rel_ref ipr_ref="IPR011357"/>
	    <rel_ref ipr_ref="IPR011358"/>
	    <rel_ref ipr_ref="IPR011359"/>
	    <rel_ref ipr_ref="IPR011360"/>
	    <rel_ref ipr_ref="IPR011361"/>
	    <rel_ref ipr_ref="IPR012051"/>
	    <rel_ref ipr_ref="IPR012224"/>
	    <rel_ref ipr_ref="IPR012267"/>
	 </found_in>
	  <classification id="GO:0004252" class_type="GO">
	    <category>Molecular Function</category>
	    <description>serine-type endopeptidase activity</description>
	  </classification>
	  <classification id="GO:0006508" class_type="GO">
	    <category>Biological Process</category>
	    <description>proteolysis</description>
	  </classification>
	  <match id="PR00722" name="CHYMOTRYPSIN" dbname="PRINTS">
	    <location start="99" end="114" score="2.9e-13" status="T" evidence="FPrintScan" />
	    <location start="157" end="171" score="2.9e-13" status="T" evidence="FPrintScan" />
	    <location start="252" end="264" score="2.9e-13" status="T" evidence="FPrintScan" />
	  </match>
	</interpro>
	<interpro id="IPR009003" name="Peptidase, trypsin-like serine and cysteine" type="Domain">
	  <child_list>
	    <rel_ref ipr_ref="IPR000930"/>
	    <rel_ref ipr_ref="IPR001254"/>
	    <rel_ref ipr_ref="IPR004109"/>
	    <rel_ref ipr_ref="IPR011565"/>
	  </child_list>
	  <contains>
	    <rel_ref ipr_ref="IPR000126"/>
	    <rel_ref ipr_ref="IPR000199"/>
	  </contains>
	  <match id="SSF50494" name="Trypsin-like serine proteases" dbname="SUPERFAMILY">
	    <location start="53" end="307" score="5.6e-76" status="T" evidence="superfamily" />
	  </match>
	</interpro>
	<interpro id="noIPR" name="unintegrated" type="unintegrated">
	  <match id="G3DSA:2.40.10.10" name="no description" dbname="GENE3D">
	    <location start="98" end="304" score="9.1e-58" status="T" evidence="Gene3D" />
	  </match>
	  <match id="PTHR19355" name="SERINE PROTEASE-RELATED" dbname="PANTHER">
	    <location start="61" end="303" score="1.9e-80" status="T" evidence="HMMPanther" />
	  </match>
	  <match id="PTHR19355:SF34" name="HYALURONAN BINDING PROTEIN 2" dbname="PANTHER">
	    <location start="61" end="303" score="1.9e-80" status="T" evidence="HMMPanther" />
	  </match>
	</interpro>
   </protein>
   <protein id="1.1_3_ORF5" length="114" crc64="9DBE09F69E45B687" >
	<interpro id="IPR004046" name="Glutathione S-transferase, C-terminal" type="Domain" parent_id="IPR010987">
	  <found_in>
	    <rel_ref ipr_ref="IPR003080"/>
	    <rel_ref ipr_ref="IPR003081"/>
	    <rel_ref ipr_ref="IPR003082"/>
	    <rel_ref ipr_ref="IPR003083"/>
	    <rel_ref ipr_ref="IPR005442"/>
	    <rel_ref ipr_ref="IPR005955"/>
	 </found_in>
	  <match id="PF00043" name="GST_C" dbname="PFAM">
	    <location start="35" end="70" score="5.1e-05" status="T" evidence="HMMPfam" />
	  </match>
	</interpro>
	<interpro id="IPR010987" name="Glutathione S-transferase, C-terminal-like" type="Domain">
	  <child_list>
	    <rel_ref ipr_ref="IPR004046"/>
	    <rel_ref ipr_ref="IPR007494"/>
	  </child_list>
	  <found_in>
	    <rel_ref ipr_ref="IPR002946"/>
	    <rel_ref ipr_ref="IPR005442"/>
	    <rel_ref ipr_ref="IPR005955"/>
	 </found_in>
	  <match id="SSF47616" name="Glutathione S-transferase (GST), C-terminal domain" dbname="SUPERFAMILY">
	    <location start="36" end="92" score="0.00037" status="T" evidence="superfamily" />
	  </match>
	</interpro>
	<interpro id="noIPR" name="unintegrated" type="unintegrated">
	  <match id="PTHR11571" name="GLUTATHIONE S-TRANSFERASE" dbname="PANTHER">
	    <location start="36" end="107" score="4e-11" status="T" evidence="HMMPanther" />
	  </match>
	  <match id="PTHR11571:SF4" name="GLUTATHIONE S-TRANSFERASE CLASS ALPHA" dbname="PANTHER">
	    <location start="36" end="107" score="4e-11" status="T" evidence="HMMPanther" />
	  </match>
	  <match id="seg" name="seg" dbname="SEG">
	    <location start="10" end="30" score="NA" status="?" evidence="Seg" />
	  </match>
	</interpro>
   </protein>

</interpro_matches>


And here is the sequence file and sixpack output for sequence 1.1:

Sequence file:
>1.1
CAGAGCACAACAGGTANCCCCTTCTGTAACCGTCACCTGCTACCACTCAGTGTACGGTGACCACCAAGGCTCCACAGCCGTCCAAAGGTCCTACAGCACAGCCGTCCCAGGAGCCCTACAGCACAGCCGTCCAAAAGCCCTACAGCACAGCCCTCCAAAAGCCCTACAGCCGACCCTGTTGGCACAACTCCAGCGCCTTCGCCAAAACAGTGCGAAACCTGTGGTTTGCCTCAACCAAAAAAGGCACTAACTAGGATCTTTGGGGGACTGAAAGTCAGCCCTGGGGCAATCCCCTGGCAGGTTTCTGTGCAACAAAAGACAAAGAACTCCAACATGAAATACAGCCCCGTCTGCGGAGGAGTTCTGATCGCAAGCTGCTGGGTTCTGACGGCTGGACACTGCATAGAACAAAACAAGGATATGCGGGTGGTGATGGGTACCTTAGCACTGGATAGGCCCGATCCATCAGCTCAAATAGTTGAAGTTGACACTGCAATTGTGCACGAGAACTACAGGGAGACTTCTTCGGCTGTTTACAACGACATAGCTTTGTTAAGACTCAGGGGGGCAAACGGATTTTGTGCAAACGAGACCCAGTTTGTGAAGACAGCTTGTCTGCCAAGTGGCCAACTCACCGATGGGACGGAGTGCACCATTTCTGGATGGGGCGCCACCGAGACCTCGGACTATGGCACCAACCACCTGTTACAGGCCAATGTTCTGCTGATCAGCCAGGCAAAGTGCTCTGAACCCACTGTTTACGGAAGCGCCATTGATGGCAGCATGTTCTGCGCCGGCTACCTGCAGGGAGGGGTTGATTCCTGCCAAGGGGATTCTGGAGGGCCGTTGACTTGCAAGCAGAATGACACCAGTGTTGTTTATGGTCTGGTGAGCTGGGGAGATCAGTGTGGAAAGAAAAACAAACCAGGGGTTTACGCAAGAGTCACCAACTTCCTGGATTGGATCAAGTCAAAGACTCAAGCATTTTCCTAAATGACCCGGTTAGAAATTCGTTACGAACATGGCAGCATGTTTGGCCAGGCGTTAAGCTGTGGTTGAGGGTTATGTGGTGCTGTTTCATAACTGCCCTGCACCTGACTGGACTATTATGCAACTGAAAGAAAGACCTGAGATCCTCTCAGCACATTTTGCAATATTTCCTCACAATAACGGGCAAATATGAATAATAATAATAATGAAATAACAATAAAGGTTACTTGACTGTTAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCAGAAAAAACTTTAAAAAAAAAAAAAAAAAAAACGGGGACCGGGGGGGAGGTAAACTAAGCCTTGCAGATGTGCTTCTGGTGGAATGTACTTTGATGCTGGAGGAGAAATTTCCTGCTATCCTCAAGGATTACCCCAACCTCAAAGTCTTTCCAGGGCAGGATGACCCAGATTCCCGCCATCAGCAGGTTTCTGCAGCCGGGCAGCAGAGGAAGCCGCAGCCAGATGAGGGTTACGTCAAGACTGTCATGGAAGTGTTCAACATCAACGCACCCCTCTCATTATAATGTCTTGCCAACATAAATCCGGTTATTAATAATAATAATAAAAAAACTAAATGAGTAATTCAGACATTATGTGAGCATCTGTGATTGGAGTTATGTTGGAAGATTACCACAAAAAAGAAATTGACGCACCCGATCTAGTTTCCCCAATCCGATACTTGTGCCGATCCGATACTAAATAATGAACTTGCCTACCCTGGACATGGTGGAGTAAACGCATCCAAGCTTGTGACCATAAAATTTTTAAAAAAA

Sixpack output:
>1.1_1_ORF1  Translation of 1.1 in frame 1, ORF 1, threshold 1, 19aa
QSTTGXPFCNRHLLPLSVR
>1.1_1_ORF2  Translation of 1.1 in frame 1, ORF 2, threshold 1, 310aa
PPRLHSRPKVLQHSRPRSPTAQPSKSPTAQPSKSPTADPVGTTPAPSPKQCETCGLPQPK
KALTRIFGGLKVSPGAIPWQVSVQQKTKNSNMKYSPVCGGVLIASCWVLTAGHCIEQNKD
MRVVMGTLALDRPDPSAQIVEVDTAIVHENYRETSSAVYNDIALLRLRGANGFCANETQF
VKTACLPSGQLTDGTECTISGWGATETSDYGTNHLLQANVLLISQAKCSEPTVYGSAIDG
SMFCAGYLQGGVDSCQGDSGGPLTCKQNDTSVVYGLVSWGDQCGKKNKPGVYARVTNFLD
WIKSKTQAFS
>1.1_1_ORF3  Translation of 1.1 in frame 1, ORF 3, threshold 1, 21aa
MTRLEIRYEHGSMFGQALSCG
>1.1_1_ORF4  Translation of 1.1 in frame 1, ORF 4, threshold 1, 7aa
GLCGAVS
>1.1_1_ORF5  Translation of 1.1 in frame 1, ORF 5, threshold 1, 4aa
LPCT
>1.1_1_ORF6  Translation of 1.1 in frame 1, ORF 6, threshold 1, 10aa
LDYYATERKT
>1.1_1_ORF7  Translation of 1.1 in frame 1, ORF 7, threshold 1, 12aa
DPLSTFCNISSQ
>1.1_1_ORF8  Translation of 1.1 in frame 1, ORF 8, threshold 1, 16aa
RANMNNNNNEITIKVT
>1.1_1_ORF9  Translation of 1.1 in frame 1, ORF 9, threshold 1, 31aa
LLKKKKKKKKKKPEKTLKKKKKKTGTGGEVN
>1.1_1_ORF10  Translation of 1.1 in frame 1, ORF 10, threshold 1, 11aa
ALQMCFWWNVL
>1.1_1_ORF11  Translation of 1.1 in frame 1, ORF 11, threshold 1, 68aa
CWRRNFLLSSRITPTSKSFQGRMTQIPAISRFLQPGSRGSRSQMRVTSRLSWKCSTSTHP
SHYNVLPT
>1.1_1_ORF12  Translation of 1.1 in frame 1, ORF 12, threshold 1, 13aa
IRLLIIIIKKLNE
>1.1_1_ORF13  Translation of 1.1 in frame 1, ORF 13, threshold 1, 21aa
FRHYVSICDWSYVGRLPQKRN
>1.1_1_ORF14  Translation of 1.1 in frame 1, ORF 14, threshold 1, 16aa
RTRSSFPNPILVPIRY
>1.1_1_ORF15  Translation of 1.1 in frame 1, ORF 15, threshold 1, 11aa
IMNLPTLDMVE
>1.1_1_ORF16  Translation of 1.1 in frame 1, ORF 16, threshold 1, 5aa
THPSL
>1.1_1_ORF17  Translation of 1.1 in frame 1, ORF 17, threshold 1, 1aa
P
>1.1_1_ORF18  Translation of 1.1 in frame 1, ORF 18, threshold 1, 2aa
NF
>1.1_1_ORF19  Translation of 1.1 in frame 1, ORF 19, threshold 1, 2aa
KX
>1.1_2_ORF1  Translation of 1.1 in frame 2, ORF 1, threshold 1, 82aa
RAQQVXPSVTVTCYHSVYGDHQGSTAVQRSYSTAVPGALQHSRPKALQHSPPKALQPTLL
AQLQRLRQNSAKPVVCLNQKRH
>1.1_2_ORF2  Translation of 1.1 in frame 2, ORF 2, threshold 1, 6aa
LGSLGD
>1.1_2_ORF3  Translation of 1.1 in frame 2, ORF 3, threshold 1, 21aa
KSALGQSPGRFLCNKRQRTPT
>1.1_2_ORF4  Translation of 1.1 in frame 2, ORF 4, threshold 1, 9aa
NTAPSAEEF
>1.1_2_ORF5  Translation of 1.1 in frame 2, ORF 5, threshold 1, 6aa
SQAAGF
>1.1_2_ORF6  Translation of 1.1 in frame 2, ORF 6, threshold 1, 5aa
RLDTA
>1.1_2_ORF7  Translation of 1.1 in frame 2, ORF 7, threshold 1, 8aa
NKTRICGW
>1.1_2_ORF8  Translation of 1.1 in frame 2, ORF 8, threshold 1, 3aa
WVP
>1.1_2_ORF9  Translation of 1.1 in frame 2, ORF 9, threshold 1, 10aa
HWIGPIHQLK
>1.1_2_ORF10  Translation of 1.1 in frame 2, ORF 10, threshold 1, 22aa
LKLTLQLCTRTTGRLLRLFTTT
>1.1_2_ORF11  Translation of 1.1 in frame 2, ORF 11, threshold 1, 2aa
LC
>1.1_2_ORF12  Translation of 1.1 in frame 2, ORF 12, threshold 1, 15aa
DSGGQTDFVQTRPSL
>1.1_2_ORF13  Translation of 1.1 in frame 2, ORF 13, threshold 1, 40aa
RQLVCQVANSPMGRSAPFLDGAPPRPRTMAPTTCYRPMFC
>1.1_2_ORF14  Translation of 1.1 in frame 2, ORF 14, threshold 1, 40aa
SARQSALNPLFTEAPLMAACSAPATCREGLIPAKGILEGR
>1.1_2_ORF15  Translation of 1.1 in frame 2, ORF 15, threshold 1, 13aa
LASRMTPVLFMVW
>1.1_2_ORF16  Translation of 1.1 in frame 2, ORF 16, threshold 1, 34aa
AGEISVERKTNQGFTQESPTSWIGSSQRLKHFPK
>1.1_2_ORF17  Translation of 1.1 in frame 2, ORF 17, threshold 1, 2aa
PG
>1.1_2_ORF18  Translation of 1.1 in frame 2, ORF 18, threshold 1, 13aa
KFVTNMAACLARR
>1.1_2_ORF19  Translation of 1.1 in frame 2, ORF 19, threshold 1, 44aa
AVVEGYVVLFHNCPAPDWTIMQLKERPEILSAHFAIFPHNNGQI
>1.1_2_ORF20  Translation of 1.1 in frame 2, ORF 20, threshold 1, 6aa
IIIIMK
>1.1_2_ORF21  Translation of 1.1 in frame 2, ORF 21, threshold 1, 1aa
Q
>1.1_2_ORF22  Translation of 1.1 in frame 2, ORF 22, threshold 1, 5aa
RLLDC
>1.1_2_ORF23  Translation of 1.1 in frame 2, ORF 23, threshold 1, 14aa
KKKKKKKKKNQKKL
>1.1_2_ORF24  Translation of 1.1 in frame 2, ORF 24, threshold 1, 12aa
KKKKKKRGPGGR
>1.1_2_ORF25  Translation of 1.1 in frame 2, ORF 25, threshold 1, 36aa
TKPCRCASGGMYFDAGGEISCYPQGLPQPQSLSRAG
>1.1_2_ORF26  Translation of 1.1 in frame 2, ORF 26, threshold 1, 20aa
PRFPPSAGFCSRAAEEAAAR
>1.1_2_ORF27  Translation of 1.1 in frame 2, ORF 27, threshold 1, 28aa
GLRQDCHGSVQHQRTPLIIMSCQHKSGY
>1.1_2_ORF28  Translation of 1.1 in frame 2, ORF 28, threshold 1, 2aa
KN
>1.1_2_ORF29  Translation of 1.1 in frame 2, ORF 29, threshold 1, 7aa
MSNSDIM
>1.1_2_ORF30  Translation of 1.1 in frame 2, ORF 30, threshold 1, 34aa
ASVIGVMLEDYHKKEIDAPDLVSPIRYLCRSDTK
>1.1_2_ORF31  Translation of 1.1 in frame 2, ORF 31, threshold 1, 23aa
TCLPWTWWSKRIQACDHKIFKKX
>1.1_3_ORF1  Translation of 1.1 in frame 3, ORF 1, threshold 1, 8aa
EHNRXPLL
>1.1_3_ORF2  Translation of 1.1 in frame 3, ORF 2, threshold 1, 74aa
PSPATTQCTVTTKAPQPSKGPTAQPSQEPYSTAVQKPYSTALQKPYSRPCWHNSSAFAKT
VRNLWFASTKKGTN
>1.1_3_ORF3  Translation of 1.1 in frame 3, ORF 3, threshold 1, 66aa
DLWGTESQPWGNPLAGFCATKDKELQHEIQPRLRRSSDRKLLGSDGWTLHRTKQGYAGGD
GYLSTG
>1.1_3_ORF4  Translation of 1.1 in frame 3, ORF 4, threshold 1, 8aa
ARSISSNS
>1.1_3_ORF5  Translation of 1.1 in frame 3, ORF 5, threshold 1, 1aa
S
>1.1_3_ORF6  Translation of 1.1 in frame 3, ORF 6, threshold 1, 86aa
HCNCARELQGDFFGCLQRHSFVKTQGGKRILCKRDPVCEDSLSAKWPTHRWDGVHHFWMG
RHRDLGLWHQPPVTGQCSADQPGKVL
>1.1_3_ORF7  Translation of 1.1 in frame 3, ORF 7, threshold 1, 8aa
THCLRKRH
>1.1_3_ORF8  Translation of 1.1 in frame 3, ORF 8, threshold 1, 13aa
WQHVLRRLPAGRG
>1.1_3_ORF9  Translation of 1.1 in frame 3, ORF 9, threshold 1, 15aa
FLPRGFWRAVDLQAE
>1.1_3_ORF10  Translation of 1.1 in frame 3, ORF 10, threshold 1, 83aa
HQCCLWSGELGRSVWKEKQTRGLRKSHQLPGLDQVKDSSIFLNDPVRNSLRTWQHVWPGV
KLWLRVMWCCFITALHLTGLLCN
>1.1_3_ORF11  Translation of 1.1 in frame 3, ORF 11, threshold 1, 22aa
KKDLRSSQHILQYFLTITGKYE
>1.1_3_ORF12  Translation of 1.1 in frame 3, ORF 12, threshold 1, 114aa
NNNKGYLTVKKKKKKKKKKTRKNFKKKKKKNGDRGGGKLSLADVLLVECTLMLEEKFPAI
LKDYPNLKVFPGQDDPDSRHQQVSAAGQQRKPQPDEGYVKTVMEVFNINAPLSL
>1.1_3_ORF13  Translation of 1.1 in frame 3, ORF 13, threshold 1, 17aa
CLANINPVINNNNKKTK
>1.1_3_ORF14  Translation of 1.1 in frame 3, ORF 14, threshold 1, 9aa
VIQTLCEHL
>1.1_3_ORF15  Translation of 1.1 in frame 3, ORF 15, threshold 1, 17aa
LELCWKITTKKKLTHPI
>1.1_3_ORF16  Translation of 1.1 in frame 3, ORF 16, threshold 1, 37aa
FPQSDTCADPILNNELAYPGHGGVNASKLVTIKFLKK


Here are also the contents of my iprscan.conf file:


# $Id: iprscan.conf,v 1.6 2006/07/13 15:39:22 hunter Exp $
# generic definitions
queue=sge6
usergroup=
toolgroup=iprscan
date=[%YYYY][%MM][%DD]
user=[%env USER]
#User can give a project name to his/her session. If not given user name is used. type iprscan -cli -h for info.
head=[%if %project ? %project : %user]
jobid=iprscan-[%date]-[%hh][%mm][%ss][%random 100]

# queue/local definitions
# Resource to use for InterProScan with a queueing system (option '-R' with LSF for instance)
resource=
queue.name=
# Execution host when using local installation, without queueing system.
host.exec=

# work directory, depends on search type
# You can store email and interactive submissions in two differents locations.
sworkdir=[%env IPRSCAN_HOME]/tmp/[%date]/[%jobid]
aworkdir=[%env IPRSCAN_HOME]/tmp/[%date]/[%jobid]
workdir=[%if %srchtype eq email ? %aworkdir : %sworkdir]

# work URL
workserver=
workurl=[%workserver]/iprscan/iprscan?tool=iprscan&jobid=[%jobid]&cnk=
# URL for resubmissions jobs
resuburl=[%workserver]/iprscan/ResubmitJobs.pl?tool=iprscan&jobid=[%jobid]&cnk=
# URL used to query indexed flat files.
wgeturl.index=[%workserver]/iprscan/wget.pl?

# work files
toolinput=[%workdir]/[%jobid].input
toolseqs=[%workdir]/[%jobid].seqs
toolnocrc=[%workdir]/[%jobid].nocrc
toolerrors=[%workdir]/[%jobid].errors
toolparams=[%workdir]/[%jobid].params
toolraw=[%workdir]/merged.raw
toolhtml=[%workdir]/[%jobid].html
tooltxt=[%workdir]/[%jobid].txt
toolxml=[%workdir]/[%jobid].xml
#This xml file contains an additional header made by EBI. Check iprscan/bin/converter.pl (getHeader subroutine).
toolebixml=[%toolxml]
toolstatus=[%workdir]/[%jobid].status
toolexitcode=[%workdir]/[%jobid].exitcode
tooloutput=[%workdir]/[%jobid].output
tooldebug=[%workdir]/[%jobid].debug
toolreport=[%workdir]/[%jobid].report
nostarted=[%workdir]/index.html
mailconfirm=[%workdir]/[%jobid].confirm


#If you want to apply a job time limit set this field to 1. Check iprscan/bin/iprscan_wrapper.pl
job.time.limit=1

# poll interval in seconds (sleep time between polls) (poll interval to 5 sec and 3000 rounds is bit more than 4hrs)
pollinterval=5
maxpollrounds=1000
# maxpollrounds=3000
#
#You can add these limits to your InterProScan installation.
#max amino acids for input sequence
#
maxinputseqs.aa=

#max nucleic acids for input sequene
maxinputseqs.nt=

#max length for the nucleotide input sequence
maxseqlen.nt=

#min length for the protein input sequence
minseqlen.aa=

#default minimum orf size for translation
minorfsize=

#default codon translation table ( for Standard Code)
codon.table=0

#
# Chunks
#
#Here is the number of sequences splitted into one chunk.
chunk=10

#the number of chunk to be displayed on a line in the result page(web interface)
chunk.display=50

#If you want CRC64, InterPro look up and goterms checkboxes checked by default, set tags to 1, otherwise leave it empty.
checkbox.crc64=
checkbox.iprlookup=1
checkbox.goterms=1

#If the user wants to have different right on the session directory created with 'dirmode' from tooldefaults.conf
#If you leave this value empty, the default value from tooldefault.conf will be used then.
#Effect of that tag: create temporary session like that : iprscan/tmp/[%date]/[%session]
# [%date] will have rights from tooldefault.conf
# [%session] will have right from iprscan.conf(usermode) if not empty, otherwise from tooldefault.conf.
usermode=

#Supported applications
applications=blastprodom,coils,gene3d,hmmpanther,hmmpir,hmmpfam,hmmsmart,hmmtigr,fprintscan,scanregexp,profilescan,superfamily,seg

#Applications list needed for InterProScan installation. Don't touch it!
defaults.applications=blastprodom,coils,gene3d,hmmpanther,hmmpir,hmmpfam,hmmsmart,hmmtigr,fprintscan,scanregexp,profilescan,superfamily,seg,signalp,tmhmm

#InterPro Taxonomy list
taxonomy.interpro=Arabidopsis thaliana,Archaea,Arthropoda,Bacteria,Caenorhabditis elegans,Chordata,Cyanobacteria,Eukaryota,Fruit Fly,Fungi,Green Plants,Human,Metazoa,Mouse,Nematoda,Other Eukaryotes,Plastid Group,Rice spp,Saccharomyces cerevisiae,Synechocystis PCC 6803,Unclassified,Virus

#User want to use taxonomy filtering? 1->yes, 0->no
taxonomy.use=

#Update the status of the jobs?
updatestatus=0

# format and translate commands
# Here you can use different program to translate nucleotide sequences in 6 frames (default sixpack form EMBOSS package)
# and also another program to reformat your input sequences into a right and good FASTA format (default seqret from EMBOSS package).
# We recommend to use EMBOSS applications (www.emboss.org) to do so. But it is up to you to change it.
#
formatcmd=[%env IPRSCAN_HOME]/conf/seqret.sh  $in > $out

# CHANGED TO FIX ORF LABELLING PROBLEM
#translatecmd=[%env IPRSCAN_HOME]/conf/sixpack.sh $in $out -table $table -orfminsize $trlen
translatecmd=[%env IPRSCAN_HOME]/conf/sixpack.sh $in $out -table $table -orfminsize $trlen -nofirst 

# InterProScan perl distribution parameters
iprscan.iprnullac=NULL
iprscan.iprnullname=NULL
iprscan.converter=[%env IPRSCAN_HOME]/bin/converter.pl
iprscan.xml.converter=[%iprscan.converter] -format %format -input %toolraw -jobid [%jobid]

#Record delimiter for input indexes
recdel=%newline>

# binaries
binary=[%env IPRSCAN_HOME]/bin/iprscan_wrapper.pl

# command line
#use this command with Dispatcher::Queue::submit subroutine
#cmdline=[%binary] < [%toolparams] 2> [%toolerrors]
#template=[%env IPRSCAN_HOME]/conf/iprscan.sh

#use this command with Dispatcher::Queue::submitWithArgs subroutine
cmdline= 
template=[%env IPRSCAN_HOME]/bin/executor.pl

# other
submiturl=[%workserver]/iprscan/iprscan?
resubmiturl=[%workserver]/iprscan/ResubmitJobs.pl?
origsequrl=[%workserver]/tmp/[%date]/[%jobid]/[%jobid].seqs
iprscansequrl=[%workserver]/tmp/[%date]/[%jobid]/[%jobid].input
pictureurl=[%workurl]&view=picture
rawurl=[%workserver]/tmp/[%date]/[%jobid]/merged.raw
xmlurl=[%workserver]/tmp/[%date]/[%jobid]/[%jobid].xml
txturl=[%workserver]/tmp/[%date]/[%jobid]/[%jobid].txt
nostarturl=[%workserver]/tmp/[%date]/[%jobid]/index.html
reporturl=[%workserver]/tmp/[%date]/[%jobid]/[%jobid].report

#Parameters that will be added to the confirmation email.
confparams=title,email,appl
htmlerror=Error in InterProScan job
#Email address of the administrator who will receive the error messages.
adminaddr=nobody@localhost.com
debug=0
#You can add here a file with news or messages to be send to user when they use email submission. This will be added at the top of the confirmation email.
mailconfmsg=
#Subject of the error email
mailerrorsubject=Error in InterProScan job [%jobid]
mailconfsubject=Thanks for your InterProScan submission
mailhelpsubject=InterProScan Help
mailresultsubject=Your InterProScan result [%jobid]
#Email address of the administrator or another part who will receive the error messages.
mailerrorto=
#Email address of the sender (Can be an automatic address e.g. : robot@somewhere.com)
mailfromaddr=nobody@localhost.com

# job status information
# used by Dispatcher::Tool::InterProScan::createStatusPage(). This subroutine is no longer used in InterProScan.
info.jobpending=PEND
info.jobrunning=RUN
info.jobdone=DONE
info.jobfailed=FAIL
info.jobnotfound=UNKN

# values used for unintegrated hits on the output page.
unintegrated.id=noIPR
unintegrated.name=unintegrated
unintegrated.type=unintegrated

# Initial image width for the picture view
defaultimgwidth=400

# image URLs for applications depending evidence status
imgbasedef=http://www.ebi.ac.uk/InterProScan/images
imgbasehome=[%workserver]/images

#If user cannot or don't want to install a webserver, (s)he can produce html outputs anyway on command line
#and all the pictures will be downloaded from EBI.
imgbase=[%if %workserver ? %imgbasehome : %imgbasedef]
imgurl.iprs=[%imgbase]/iprs.png
imgurl.interpro=[%imgbase]/interpro_small.jpg
imgurl.srs=[%imgbase]/srs_small.gif
imgurl.zoomin=[%imgbase]/mag+_dark.gif
imgurl.zoomout=[%imgbase]/mag-_dark.gif
imgurl.line=[%imgbase]/1.gif
imgurl.end=[%imgbase]/0.gif
imgurl.default=[%imgbase]/G.gif
imgurl.blastprodom.T=[%imgbase]/D.gif
imgurl.blastprodom.?=[%imgbase]/D.gif
imgurl.gene3d.T=[%imgbase]/X2.gif
imgurl.gene3d.?=[%imgbase]/X2.gif
imgurl.hmmpanther.T=[%imgbase]/X4.gif
imgurl.hmmpanther.?=[%imgbase]/X4.gif
imgurl.hmmpir.T=[%imgbase]/S.gif
imgurl.hmmpir.?=[%imgbase]/S.gif
imgurl.hmmpfam.T=[%imgbase]/H.gif
imgurl.hmmpfam.?=[%imgbase]/H.gif
imgurl.hmmsmart.T=[%imgbase]/R.gif
imgurl.hmmsmart.?=[%imgbase]/RU.gif
imgurl.hmmtigr.T=[%imgbase]/T.gif
imgurl.hmmtigr.?=[%imgbase]/T.gif
imgurl.fprintscan.T=[%imgbase]/F.gif
imgurl.fprintscan.?=[%imgbase]/FU.gif
imgurl.scanregexp.T=[%imgbase]/P.gif
imgurl.scanregexp.?=[%imgbase]/PU.gif
imgurl.profilescan.T=[%imgbase]/M.gif
imgurl.profilescan.?=[%imgbase]/M.gif
imgurl.superfamily.T=[%imgbase]/B.gif
imgurl.superfamily.?=[%imgbase]/B.gif
imgurl.pa=[%imgbase]/m_result_small.gif
imgurl.critical=[%imgbase]/critical.png
imgurl.mol=[%imgbase]/molecule.gif
imgurl.inx.cross=[%imgbase]/cross.gif
imgurl.inx.ok=[%imgbase]/checkok.gif
imgurl.inx.notok=[%imgbase]/checknotok.gif
imgurl.icon=[%imgbase]/bookmark.gif
imgurl.toolbox=[%imgbase]/logo_toolbox_mid.jpg
imgurl.toolbox2=[%imgbase]/logo_toolbox_mid2.jpg
imgurl.interprosmall=[%imgbase]/interpro.jpg
imgurl.interprosmall2=[%imgbase]/interpro2.jpg
imgurl.tree=[%imgbase]/ebitree_light_lores.jpg
imgurl.tree2=[%imgbase]/ebitree_light_lores2.jpg

# URLs for entries
# This url is in test at the moment. Maybe will never be used.
url.pdbview=http://www.ebi.ac.uk/msd-srv/msdsite/barChartPattern?pattern1=
wgeturl=[%workserver]/iprscan/wget.pl?tool=%toolname&file=[%toolinput]&entry=
entryurl.interpro=http://www.ebi.ac.uk/interpro/IEntry?ac=
entryurl.prints=http://bioinf.man.ac.uk/cgi-bin/dbbrowser/sprint/searchprintss.cgi?display_opts=Prints&category=None&queryform=false&prints_accn=
entryurl.profile=http://www.expasy.org/prosite/
entryurl.smart=http://smart.embl-heidelberg.de/smart/do_annotation.pl?BLAST=DUMMY&ACC=
entryurl.pfam=http://www.sanger.ac.uk/cgi-bin/Pfam/getacc?
entryurl.tigrfams=http://www.tigr.org/tigr-scripts/CMR2/hmm_report.spl?user=access&password=access&acc=
entryurl.prodom=http://prodes.toulouse.inra.fr/prodom/current/cgi-bin/request.pl?question=DBEN&query=
entryurl.prosite=http://www.expasy.org/prosite/
entryurl.pir=http://pir.georgetown.edu/cgi-bin/ipcSF?id=
entryurl.go=http://www.ebi.ac.uk/ego/DisplayGoTerm?id=
entryurl.superfamily=http://supfam.org/SUPERFAMILY/cgi-bin/scop.cgi?ipid=
entryurl.panther=http://www.pantherdb.org/panther/family.do?clsAccession=
entryurl.gene3d=http://cathwww.biochem.ucl.ac.uk/cgi-bin/cath/GotoCath.pl?cath=

#Url Informations for each Applications
infourl.blastprodom=http://protein.toulouse.inra.fr/prodom/current/html/home.php
infourl.coils=http://npsa-pbil.ibcp.fr/cgi-bin/npsa_automat.pl?page=npsa_lupas.html
infourl.fprintscan=http://www.bioinf.man.ac.uk/fingerPRINTScan/
infourl.hmmpir=http://pir.georgetown.edu/
infourl.hmmsmart=http://smart.embl-heidelberg.de/
infourl.hmmpfam=http://www.sanger.ac.uk/Software/Pfam/search.shtml
infourl.hmmtigr=http://www.tigr.org/TIGRFAMs/
infourl.scanregexp=http://www.expasy.org/prosite/
infourl.seg=http://www.ncbi.nlm.nih.gov/Education/BLASTinfo/Seg.html
infourl.tmhmm=http://www.cbs.dtu.dk/services/TMHMM/
infourl.profilescan=http://www.isrec.isb-sib.ch/profile/profile.html
infourl.superfamily=http://supfam.mrc-lmb.cam.ac.uk/SUPERFAMILY/
infourl.signalp=http://www.cbs.dtu.dk/services/SignalP/
infourl.hmmpanther=http://www.pantherdb.org/
infourl.gene3d=http://www.cathdb.info/Gene3D/

#Name to display for each applications on the web page
appl.name.blastprodom=BlastProDom
appl.name.coils=Coils
appl.name.fprintscan=FPrintScan
appl.name.gene3d=Gene3D
appl.name.hmmpanther=HMMPanther
appl.name.hmmpir=HMMPIR
appl.name.hmmsmart=HMMSmart
appl.name.hmmtigr=HMMTigr
appl.name.hmmpfam=HMMPfam
appl.name.scanregexp=ScanRegExp
appl.name.seg=Seg
appl.name.tmhmm=TMHMM
appl.name.profilescan=ProfileScan
appl.name.superfamily=Superfamily
appl.name.signalp=SignalPHMM


#Description of each applications
appl.desc.blastprodom=(a wrapper script on top of Blast package) is used to search against PRODOM families
appl.desc.coils=is used to predict coiled-coil segments with the algorithm of Lupas et al.
appl.desc.fprintscan=(FingerPRINTScan) is used to search against the PRINTS collection of protein signatures
appl.desc.gene3d=Gene3D is supplementary to the CATH database. This protein sequence database contains proteins from complete genomes which have been clustered into protein families and annotated with CATH domains, Pfam domains and functional information from KEGG, GO, COG, Affymetrix and STRINGS.
appl.desc.hmmpanther=(hmmpfam from HMMER 2.3.2 package) PANTHER classifies proteins into families and specific functional subfamilies, which are then mapped to ontology terms and pathways.
appl.desc.hmmpir=PIR produces the Protein  Sequence Database (PSD) of functionally annotated protein sequences, which grew out of the Atlas of Protein Sequence and Structure (1965-1978) edited by Margaret Dayhoff and has been incorporated into an integrated knowledge base system of value-added databases and analytical tools.
appl.desc.hmmsmart=(hmmpfam from HMMER 2.3.2 package) is used to search against the Smart HMM (Profile hidden Markov models) database
appl.desc.hmmtigr=(hmmpfam from HMMER 2.3.2 package) is used to search against TIGRFAMs collection of HMMs (Profile hidden Markov models)
appl.desc.hmmpfam=(hmmpfam from HMMER 2.3.2 package) is used to search against the Pfam HMM (Profile hidden Markov models) database
appl.desc.scanregexp=(ScanProsite) is used to search against the PROSITE patterns collection and verify the  matches by statistically significant CONFIRM patterns
appl.desc.seg=is used for identifying and masking segments of low compositional complexity in amino acid sequences
appl.desc.tmhmm=is used for prediction of transmembrane helices in proteins using a Hidden Markov Model (under commercial license : contact software@cbs.dtu.dk)
appl.desc.profilescan=(ProfileScan (pfscan & ps_scan.pl) from Pftools 2.2) package is used  to search against the PROSITE profiles collection
appl.desc.superfamily=A structural classification of proteins database for the investigation of sequences and structures.
appl.desc.signalp=predicts the presence and location of signal peptide cleavage sites, using eukaryotic HMM (under commercial license : contact software@cbs.dtu.dk)



I should also mention that I changed this line in 'bin/translate.pl':

$IPRSCAN_HOME/bin/iterator.pl -i $infile -o $outfile -c "$IPRSCAN_HOME/bin/binaries/sixpack -filter -auto -stdout -osformat fasta -outfile /dev/null $* -outseq $tmpfile %infile; cat $tmpfile"

by adding the sixpack argument '-noreverse' so that I only get frames 1 to 3:

$IPRSCAN_HOME/bin/iterator.pl -i $infile -o $outfile -c "$IPRSCAN_HOME/bin/binaries/sixpack -noreverse -filter -auto -stdout -osformat fasta -outfile /dev/null $* -outseq $tmpfile %infile; cat $tmpfile"

I hope this helps in tracking down the problem. Please let me know if you need any more information from me.

Cheers,

Stuart.





>From: Sarah Hunter <hunter@ebi.ac.uk>
>To: youngstuart@hotmail.com
>CC: support@ebi.ac.uk, interhelp@ebi.ac.uk
>Subject: Re: EBI HELP: INTERPRO: Standalone InterProScan (youngstuart@hotmail.com) (hpm) (SUP#345913)
>Date: Tue, 13 Mar 2007 17:16:58 +0000
>
>Dear Stuart,
>
>Could you try something for me, please and let me know if it works?
>
>I think if you edit your conf/iprscan.conf file and add "-nofirst 
>-nolast" to the following line:
>
>translatecmd=[%env IPRSCAN_HOME]/conf/sixpack.sh $in $out -table 
>$table -orfminsize $trlen
>
>so it reads
>
>translatecmd=[%env IPRSCAN_HOME]/conf/sixpack.sh $in $out -table 
>$table -orfminsize $trlen -nofirst -nolast
>
>This should fix the problem.  The extra commands will remove the 
>first and last ORF predicitons for the frame.  As these are usually 
>incomplete (and occasionally padded with "X"s), they normally 
>shouldn't get through the minlength cut-off anyway.
>
>I hope this is what you wanted.  If not, please email support again.
>
>Best regards,
>
>Sarah

</entry>



<entry [Wed Mar  7 18:37:22 EST 2007] QUESTION TO IPRSCAN SUPPORT ABOUT POSITIONS OF ORFS>


MSG: http://www.ebi.ac.uk/support/ (ONLINE FORM)
PROBLEM/QUERY: InterPro: Standalone Interproscan

Hi there,

I'm using iprscan to scan nucleotide sequences but I've found a rather perplexing error (I think): the iprscan results seem to be labelled wrongly, i.e., the ORF labels in the iprscan output do not  correspond to the actual ORFs of the translated nucleotide sequence.

Please see my output below for an example (it happens in most cases where there is more than one ORF hit). You'll notice that only two ORFs give iprscan hits; the first ORF is correct but the second ORF appears to be incorrectly labelled:

 id="9.1_2_ORF2" length="138"

which is the 2nd ORF of frame 2, but as you'll see from the iprscan-*.input file, that particular ORF is actually one amino acid residue long! 

I thought it might be due to the fact that the ORFs are renamed according to their order in the iprscan-*.input file AND whether or not they pass through the 'trlen' filter. But, according to my tests, that is not the case either. That is, some kind of length cutoff seems to be occurring but the cutoff threshold is not the same between reading frames from the same nucleotide sequence.

Could you please also clarify whether the querystart and querystop positions of iprscan matches on the reverse frames (4,5,6) are with respect to the positive strand or the negative strand?

Otherwise, a great application!

Many thanks,

Stuart Young.



Sequence file:
>9.1
AAGGCGCTGTCAGCCTCCCGCTGGTCTCATTTCGCGTTCTCTCTGACGCTGAGCGCTACAGACATGCAGCTGAGGAGCGTCTGCTTGATGTGGGGGCCGGCTGTGGGCCGCCGCCGCAGCCGAAGTGTTTGTAGAGAAACAGGAAGCCGCCTCAGTGCTGCGCAGATGGAGGAGAGCAAACAGCGGCTTTCTGGAGGAGCTGAAGCAGGGCAACCTGGAGAGGGAGTGTGTGGAGGAGATCTGCGACTACGAGGAGGCCCGGGAGGTGTTTGAGGACGACCAGCAGACGAGGGATTTCTGGAAGACCTACAACAAAAAGGAGCCCTGCCTGGAGAACCCCTGCCGTAACAACGCCACATGTCTCTACCTGGGAACCAGCTATGAATGCCAGTGTCTGGAGGGGTTTGAAGGACGCTACTGCCAGACAGTTTTTGAAGACTCGTTGGGATGTCTGTACCAGAACGGACACTGCGAGCACTTCTGCGACGGCTCTAAGGAAAGACG


iprscan-*.input file:

>9.1_1_ORF1  Translation of 9.1 in frame 1, ORF 1, threshold 1, 90aa
KALSASRWSHFAFSLTLSATDMQLRSVCLMWGPAVGRRRSRSVCRETGSRLSAAQMEESK
QRLSGGAEAGQPGEGVCGGDLRLRGGPGGV
>9.1_1_ORF2  Translation of 9.1 in frame 1, ORF 2, threshold 1, 24aa
GRPADEGFLEDLQQKGALPGEPLP
>9.1_1_ORF3  Translation of 9.1 in frame 1, ORF 3, threshold 1, 11aa
QRHMSLPGNQL
>9.1_1_ORF4  Translation of 9.1 in frame 1, ORF 4, threshold 1, 7aa
MPVSGGV
>9.1_1_ORF5  Translation of 9.1 in frame 1, ORF 5, threshold 1, 8aa
RTLLPDSF
>9.1_1_ORF6  Translation of 9.1 in frame 1, ORF 6, threshold 1, 19aa
RLVGMSVPERTLRALLRRL
>9.1_1_ORF7  Translation of 9.1 in frame 1, ORF 7, threshold 1, 3aa
GKT
>9.1_2_ORF1  Translation of 9.1 in frame 2, ORF 1, threshold 1, 14aa
RRCQPPAGLISRSL
>9.1_2_ORF2  Translation of 9.1 in frame 2, ORF 2, threshold 1, 1aa
R
>9.1_2_ORF3  Translation of 9.1 in frame 2, ORF 3, threshold 1, 6aa
ALQTCS
>9.1_2_ORF4  Translation of 9.1 in frame 2, ORF 4, threshold 1, 4aa
GASA
>9.1_2_ORF5  Translation of 9.1 in frame 2, ORF 5, threshold 1, 138aa
CGGRLWAAAAAEVFVEKQEAASVLRRWRRANSGFLEELKQGNLERECVEEICDYEEAREV
FEDDQQTRDFWKTYNKKEPCLENPCRNNATCLYLGTSYECQCLEGFEGRYCQTVFEDSLG
CLYQNGHCEHFCDGSKER
>9.1_3_ORF1  Translation of 9.1 in frame 3, ORF 1, threshold 1, 43aa
GAVSLPLVSFRVLSDAERYRHAAEERLLDVGAGCGPPPQPKCL
>9.1_3_ORF2  Translation of 9.1 in frame 3, ORF 2, threshold 1, 22aa
RNRKPPQCCADGGEQTAAFWRS
>9.1_3_ORF3  Translation of 9.1 in frame 3, ORF 3, threshold 1, 100aa
SRATWRGSVWRRSATTRRPGRCLRTTSRRGISGRPTTKRSPAWRTPAVTTPHVSTWEPAM
NASVWRGLKDATARQFLKTRWDVCTRTDTASTSATALRKD

Sixpack output: 9.1


          K  A  L  S  A  S  R  W  S  H  F  A  F  S  L  T  L  S  A  T     F1
           R  R  C  Q  P  P  A  G  L  I  S  R  S  L  *  R  *  A  L  Q    F2
            G  A  V  S  L  P  L  V  S  F  R  V  L  S  D  A  E  R  Y  R   F3
        1 AAGGCGCTGTCAGCCTCCCGCTGGTCTCATTTCGCGTTCTCTCTGACGCTGAGCGCTACA 60
          ----:----|----:----|----:----|----:----|----:----|----:----|

          D  M  Q  L  R  S  V  C  L  M  W  G  P  A  V  G  R  R  R  S     F1
           T  C  S  *  G  A  S  A  *  C  G  G  R  L  W  A  A  A  A  A    F2
            H  A  A  E  E  R  L  L  D  V  G  A  G  C  G  P  P  P  Q  P   F3
       61 GACATGCAGCTGAGGAGCGTCTGCTTGATGTGGGGGCCGGCTGTGGGCCGCCGCCGCAGC 120
          ----:----|----:----|----:----|----:----|----:----|----:----|

          R  S  V  C  R  E  T  G  S  R  L  S  A  A  Q  M  E  E  S  K     F1
           E  V  F  V  E  K  Q  E  A  A  S  V  L  R  R  W  R  R  A  N    F2
            K  C  L  *  R  N  R  K  P  P  Q  C  C  A  D  G  G  E  Q  T   F3
      121 CGAAGTGTTTGTAGAGAAACAGGAAGCCGCCTCAGTGCTGCGCAGATGGAGGAGAGCAAA 180
          ----:----|----:----|----:----|----:----|----:----|----:----|

          Q  R  L  S  G  G  A  E  A  G  Q  P  G  E  G  V  C  G  G  D     F1
           S  G  F  L  E  E  L  K  Q  G  N  L  E  R  E  C  V  E  E  I    F2
            A  A  F  W  R  S  *  S  R  A  T  W  R  G  S  V  W  R  R  S   F3
      181 CAGCGGCTTTCTGGAGGAGCTGAAGCAGGGCAACCTGGAGAGGGAGTGTGTGGAGGAGAT 240
          ----:----|----:----|----:----|----:----|----:----|----:----|

          L  R  L  R  G  G  P  G  G  V  *  G  R  P  A  D  E  G  F  L     F1
           C  D  Y  E  E  A  R  E  V  F  E  D  D  Q  Q  T  R  D  F  W    F2
            A  T  T  R  R  P  G  R  C  L  R  T  T  S  R  R  G  I  S  G   F3
      241 CTGCGACTACGAGGAGGCCCGGGAGGTGTTTGAGGACGACCAGCAGACGAGGGATTTCTG 300
          ----:----|----:----|----:----|----:----|----:----|----:----|

          E  D  L  Q  Q  K  G  A  L  P  G  E  P  L  P  *  Q  R  H  M     F1
           K  T  Y  N  K  K  E  P  C  L  E  N  P  C  R  N  N  A  T  C    F2
            R  P  T  T  K  R  S  P  A  W  R  T  P  A  V  T  T  P  H  V   F3
      301 GAAGACCTACAACAAAAAGGAGCCCTGCCTGGAGAACCCCTGCCGTAACAACGCCACATG 360
          ----:----|----:----|----:----|----:----|----:----|----:----|

          S  L  P  G  N  Q  L  *  M  P  V  S  G  G  V  *  R  T  L  L     F1
           L  Y  L  G  T  S  Y  E  C  Q  C  L  E  G  F  E  G  R  Y  C    F2
            S  T  W  E  P  A  M  N  A  S  V  W  R  G  L  K  D  A  T  A   F3
      361 TCTCTACCTGGGAACCAGCTATGAATGCCAGTGTCTGGAGGGGTTTGAAGGACGCTACTG 420
          ----:----|----:----|----:----|----:----|----:----|----:----|

          P  D  S  F  *  R  L  V  G  M  S  V  P  E  R  T  L  R  A  L     F1
           Q  T  V  F  E  D  S  L  G  C  L  Y  Q  N  G  H  C  E  H  F    F2
            R  Q  F  L  K  T  R  W  D  V  C  T  R  T  D  T  A  S  T  S   F3
      421 CCAGACAGTTTTTGAAGACTCGTTGGGATGTCTGTACCAGAACGGACACTGCGAGCACTT 480
          ----:----|----:----|----:----|----:----|----:----|----:----|

          L  R  R  L  *  G  K  T                                         F1
           C  D  G  S  K  E  R                                           F2
            A  T  A  L  R  K  D                                          F3
      481 CTGCGACGGCTCTAAGGAAAGACG                                     504
          ----:----|----:----|----:----|----:----|----:----|----:----|

##############################
Minimum size of ORFs : 1

Total ORFs in frame 1 :     7
Total ORFs in frame 2 :     5
Total ORFs in frame 3 :     3

Total ORFs :    15
##############################


Orf number: 1, length: 90, known length: 90, peptide: KALSASRWSHFAFSLTLSATDMQLRSVCLMWGPAVGRRRSRSVCRETGSRLSAAQMEESKQRLSGGAEAGQPGEGVCGGDLRLRGGPGGV
Orf number: 2, length: 24, known length: 24, peptide: GRPADEGFLEDLQQKGALPGEPLP
Orf number: 3, length: 11, known length: 11, peptide: QRHMSLPGNQL
Orf number: 4, length: 7, known length: 7, peptide: MPVSGGV
Orf number: 5, length: 8, known length: 8, peptide: RTLLPDSF
Orf number: 6, length: 19, known length: 19, peptide: RLVGMSVPERTLRALLRRL
Orf number: 7, length: 3, known length: 3, peptide: GKT
Orf number: 1, length: 14, known length: 14, peptide: RRCQPPAGLISRSL
Orf number: 2, length: 1, known length: 1, peptide: R
Orf number: 3, length: 6, known length: 6, peptide: ALQTCS
Orf number: 4, length: 4, known length: 4, peptide: GASA
Orf number: 5, length: 138, known length: 138, peptide: CGGRLWAAAAAEVFVEKQEAASVLRRWRRANSGFLEELKQGNLERECVEEICDYEEAREVFEDDQQTRDFWKTYNKKEPCLENPCRNNATCLYLGTSYECQCLEGFEGRYCQTVFEDSLGCLYQNGHCEHFCDGSKER
Orf number: 1, length: 43, known length: 43, peptide: GAVSLPLVSFRVLSDAERYRHAAEERLLDVGAGCGPPPQPKCL
Orf number: 2, length: 22, known length: 22, peptide: RNRKPPQCCADGGEQTAAFWRS
Orf number: 3, length: 100, known length: 100, peptide: SRATWRGSVWRRSATTRRPGRCLRTTSRRGISGRPTTKRSPAWRTPAVTTPHVSTWEPAMNASVWRGLKDATARQFLKTRWDVCTRTDTASTSATALRKD
Iprscan output: <interpro_matches>

   <protein id="9.1_1_ORF1" length="90" crc64="A6F0CAD2D32B5B04" >
	<interpro id="noIPR" name="unintegrated" type="unintegrated">
	  <match id="seg" name="seg" dbname="SEG">
	    <location start="78" end="89" score="NA" status="?" evidence="Seg" />
	  </match>
	</interpro>
   </protein>
   <protein id="9.1_2_ORF2" length="138" crc64="9D61B2E4AF6136D9" >
	<interpro id="IPR000294" name="Vitamin K-dependent carboxylation/gamma-carboxyglutamic region" type="Domain">
	  <child_list>
	    <rel_ref ipr_ref="IPR002383"/>
	  </child_list>
	  <found_in>
	    <rel_ref ipr_ref="IPR002384"/>
	 </found_in>
	  <classification id="GO:0005509" class_type="GO">
	    <category>Molecular Function</category>
	    <description>calcium ion binding</description>
	  </classification>
	  <classification id="GO:0005576" class_type="GO">
	    <category>Cellular Component</category>
	    <description>extracellular region</description>
	  </classification>
	  <match id="PF00594" name="Gla" dbname="PFAM">
	    <location start="35" end="76" score="9.6e-19" status="T" evidence="HMMPfam" />
	  </match>
	  <match id="SM00069" name="no description" dbname="SMART">
	    <location start="11" end="75" score="3.2e-31" status="T" evidence="HMMSmart" />
	  </match>
	  <match id="PS50998" name="GLA_2" dbname="PROFILE">
	    <location start="30" end="76" score="19.436" status="T" evidence="ProfileScan" />
	  </match>
	  <match id="PS00011" name="GLA_1" dbname="PROSITE">
	    <location start="46" end="71" score="8e-5" status="T" evidence="ScanRegExp" />
	  </match>
	</interpro>
	<interpro id="IPR000742" name="EGF-like, type 3" type="Domain">
	  <child_list>
	    <rel_ref ipr_ref="IPR001336"/>
	    <rel_ref ipr_ref="IPR001881"/>
	  </child_list>
	  <found_in>
	    <rel_ref ipr_ref="IPR011357"/>
	 </found_in>
	  <contains>
	    <rel_ref ipr_ref="IPR000152"/>
	  </contains>
	  <match id="PS50026" name="EGF_3" dbname="PROFILE">
	    <location start="76" end="112" score="19.278" status="T" evidence="ProfileScan" />
	  </match>
	</interpro>
	<interpro id="IPR001881" name="EGF-like calcium-binding" type="Domain" parent_id="IPR000742">
	  <child_list>
	    <rel_ref ipr_ref="IPR001438"/>
	  </child_list>
	  <found_in>
	    <rel_ref ipr_ref="IPR001491"/>
	    <rel_ref ipr_ref="IPR003056"/>
	    <rel_ref ipr_ref="IPR011203"/>
	    <rel_ref ipr_ref="IPR011361"/>
	    <rel_ref ipr_ref="IPR011398"/>
	    <rel_ref ipr_ref="IPR013091"/>
	 </found_in>
	  <contains>
	    <rel_ref ipr_ref="IPR000152"/>
	    <rel_ref ipr_ref="IPR006209"/>
	    <rel_ref ipr_ref="IPR006210"/>
	  </contains>
	  <classification id="GO:0005509" class_type="GO">
	    <category>Molecular Function</category>
	    <description>calcium ion binding</description>
	  </classification>
	  <match id="SM00179" name="no description" dbname="SMART">
	    <location start="76" end="112" score="0.0082" status="T" evidence="HMMSmart" />
	  </match>
	</interpro>
	<interpro id="IPR002383" name="Coagulation factor, Gla region" type="Domain" parent_id="IPR000294">
	  <found_in>
	    <rel_ref ipr_ref="IPR012051"/>
	    <rel_ref ipr_ref="IPR012224"/>
	 </found_in>
	  <match id="PR00001" name="GLABLOOD" dbname="PRINTS">
	    <location start="34" end="47" score="1.5e-13" status="T" evidence="FPrintScan" />
	    <location start="48" end="61" score="1.5e-13" status="T" evidence="FPrintScan" />
	    <location start="62" end="76" score="1.5e-13" status="T" evidence="FPrintScan" />
	  </match>
	</interpro>
	<interpro id="IPR006209" name="EGF-like" type="Domain">
	  <found_in>
	    <rel_ref ipr_ref="IPR001438"/>
	    <rel_ref ipr_ref="IPR001491"/>
	    <rel_ref ipr_ref="IPR001881"/>
	    <rel_ref ipr_ref="IPR002396"/>
	    <rel_ref ipr_ref="IPR011170"/>
	 </found_in>
	  <contains>
	    <rel_ref ipr_ref="IPR000152"/>
	    <rel_ref ipr_ref="IPR001336"/>
	  </contains>
	  <match id="PF00008" name="EGF" dbname="PFAM">
	    <location start="80" end="111" score="2.2e-11" status="T" evidence="HMMPfam" />
	  </match>
	</interpro>
	<interpro id="IPR006210" name="EGF" type="Domain">
	  <found_in>
	    <rel_ref ipr_ref="IPR001438"/>
	    <rel_ref ipr_ref="IPR001881"/>
	    <rel_ref ipr_ref="IPR003056"/>
	    <rel_ref ipr_ref="IPR010423"/>
	    <rel_ref ipr_ref="IPR011170"/>
	    <rel_ref ipr_ref="IPR011203"/>
	    <rel_ref ipr_ref="IPR011357"/>
	    <rel_ref ipr_ref="IPR012111"/>
	    <rel_ref ipr_ref="IPR012224"/>
	    <rel_ref ipr_ref="IPR013091"/>
	 </found_in>
	  <contains>
	    <rel_ref ipr_ref="IPR000152"/>
	  </contains>
	  <match id="SM00181" name="no description" dbname="SMART">
	    <location start="79" end="112" score="1.9e-06" status="T" evidence="HMMSmart" />
	  </match>
	</interpro>
	<interpro id="IPR013032" name="EGF-like region" type="Domain">
	  <found_in>
	    <rel_ref ipr_ref="IPR001438"/>
	    <rel_ref ipr_ref="IPR013091"/>
	 </found_in>
	  <match id="PS00022" name="EGF_1" dbname="PROSITE">
	    <location start="100" end="111" score="8e-5" status="T" evidence="ScanRegExp" />
	  </match>
	  <match id="PS01186" name="EGF_2" dbname="PROSITE">
	    <location start="100" end="111" score="8e-5" status="T" evidence="ScanRegExp" />
	  </match>
	</interpro>
	<interpro id="noIPR" name="unintegrated" type="unintegrated">
	  <match id="G3DSA:2.10.25.10" name="no description" dbname="GENE3D">
	    <location start="31" end="118" score="7.6e-21" status="T" evidence="Gene3D" />
	  </match>
	  <match id="PTHR19355" name="SERINE PROTEASE-RELATED" dbname="PANTHER">
	    <location start="22" end="135" score="8e-63" status="T" evidence="HMMPanther" />
	  </match>
	  <match id="PTHR19355:SF67" name="COAGULATION FACTOR VIIB" dbname="PANTHER">
	    <location start="22" end="135" score="8e-63" status="T" evidence="HMMPanther" />
	  </match>
	  <match id="SSF57196" name="EGF/Laminin" dbname="SUPERFAMILY">
	    <location start="79" end="133" score="2.2e-10" status="T" evidence="superfamily" />
	  </match>
	  <match id="SSF57630" name="GLA-domain" dbname="SUPERFAMILY">
	    <location start="30" end="95" score="1.5e-24" status="T" evidence="superfamily" />
	  </match>
	</interpro>
   </protein>

</interpro_matches>

$VAR1 = {
          'length' => 90,
          'sequence' => 'KALSASRWSHFAFSLTLSATDMQLRSVCLMWGPAVGRRRSRSVCRETGSRLSAAQMEESKQRLSGGAEAGQPGEGVCGGDLRLRGGPGGV',
          'querystop' => 270,
          'querystart' => 1
        };
$VAR1 = {
          'length' => 138,
          'sequence' => 'CGGRLWAAAAAEVFVEKQEAASVLRRWRRANSGFLEELKQGNLERECVEEICDYEEAREVFEDDQQTRDFWKTYNKKEPCLENPCRNNATCLYLGTSYECQCLEGFEGRYCQTVFEDSLGCLYQNGHCEHFCDGSKER',
          'querystop' => 427,
          'querystart' => 14
        };
TSV:

9	1	SEG	seg	seg	noIPR	unintegrated	unintegrated			?		232	265				
9	1	PFAM	PF00594	Gla	IPR000294	Vitamin K-dependent carboxylation/gamma-carboxyglutamic region	Domain	9.60	-19	T		116	239	IPR002383 	IPR002384 	GO:0005509: Molecular Function: calcium ion binding ; GO:0005576: Cellular Component: extracellular region ; 	
9	1	SMART	SM00069	no description	IPR000294	Vitamin K-dependent carboxylation/gamma-carboxyglutamic region	Domain	3.20	-31	T		44	236	IPR002383 	IPR002384 	GO:0005509: Molecular Function: calcium ion binding ; GO:0005576: Cellular Component: extracellular region ; 	
9	1	PROFILE	PS50998	GLA_2	IPR000294	Vitamin K-dependent carboxylation/gamma-carboxyglutamic region	Domain	19.436	1	T		101	239	IPR002383 	IPR002384 	GO:0005509: Molecular Function: calcium ion binding ; GO:0005576: Cellular Component: extracellular region ; 	
9	1	PROSITE	PS00011	GLA_1	IPR000294	Vitamin K-dependent carboxylation/gamma-carboxyglutamic region	Domain	8.00	-5	T		149	224	IPR002383 	IPR002384 	GO:0005509: Molecular Function: calcium ion binding ; GO:0005576: Cellular Component: extracellular region ; 	
9	1	PROFILE	PS50026	EGF_3	IPR000742	EGF-like, type 3	Domain	19.278	1	T		239	347	IPR001336 IPR001881 	IPR011357 		
9	1	SMART	SM00179	no description	IPR001881	EGF-like calcium-binding	Domain	0.0082	1	T		239	347	IPR001438 	IPR001491 IPR003056 IPR011203 IPR011361 IPR011398 IPR013091 	GO:0005509: Molecular Function: calcium ion binding ; 	
9	1	PRINTS	PR00001	GLABLOOD	IPR002383	Coagulation factor, Gla region	Domain	1.50	-13	T		113	152		IPR012051 IPR012224 		
9	1	PRINTS	PR00001	GLABLOOD	IPR002383	Coagulation factor, Gla region	Domain	1.50	-13	T		155	194		IPR012051 IPR012224 		
9	1	PRINTS	PR00001	GLABLOOD	IPR002383	Coagulation factor, Gla region	Domain	1.50	-13	T		197	239		IPR012051 IPR012224 		
9	1	PFAM	PF00008	EGF	IPR006209	EGF-like	Domain	2.20	-11	T		251	344		IPR001438 IPR001491 IPR001881 IPR002396 IPR011170 		
9	1	SMART	SM00181	no description	IPR006210	EGF	Domain	1.90	-06	T		248	347		IPR001438 IPR001881 IPR003056 IPR010423 IPR011170 IPR011203 IPR011357 IPR012111 IPR012224 IPR013091 		
9	1	PROSITE	PS00022	EGF_1	IPR013032	EGF-like region	Domain	8.00	-5	T		311	344		IPR001438 IPR013091 		
9	1	PROSITE	PS01186	EGF_2	IPR013032	EGF-like region	Domain	8.00	-5	T		311	344		IPR001438 IPR013091 		
9	1	GENE3D	G3DSA:2.10.25.10	no description	noIPR	unintegrated	unintegrated	7.60	-21	T		104	365				
9	1	PANTHER	PTHR19355	SERINE PROTEASE-RELATED	noIPR	unintegrated	unintegrated	8.00	-63	T		77	416				
9	1	PANTHER	PTHR19355:SF67	COAGULATION FACTOR VIIB	noIPR	unintegrated	unintegrated	8.00	-63	T		77	416				
9	1	SUPERFAMILY	SSF57196	EGF/Laminin	noIPR	unintegrated	unintegrated	2.20	-10	T		248	410				
9	1	SUPERFAMILY	SSF57630	GLA-domain	noIPR	unintegrated	unintegrated	1.50	-24	T		101	296				




</entry>



<entry [Wed Feb 21 12:28:48 EST 2007] PROBLEM WITH FULL DISC>



Doing sequence: 15597
/common/iprscan/bin/iprscan: checkParams: unable to create input files: formatSequences: could not rename /common/iprscan/tmp/20070221/iprscan-20070221-09504812/iprscan-20070221-09504812.input.formatted.tmp: No space left on device
[Util::contents] Can't open file '/Users/local/FUNNYBASE/pipeline/funnybase10/iprscan/funnybase10.out.15598.1.all.0'
Incrementing lockfile:

/Users/local/FUNNYBASE/pipeline/funnybase10/iprscan/collectionsequencesiprscan.lock

DELETED /common/pipeline/funnybase10/collection BUT NUMBER OF FILES RETURN TO 


</entry>



<entry [man qconf] >>>...>



 -mconf [host,...|global]      <modify configuration>
              The configuration for the specified host is retrieved, an editor
              is executed (either vi(1) or the editor  indicated  by  $EDITOR)
              and  the changed configuration is registered with sge_qmaster(8)
              upon exit of the editor.  If the optional host argument is omit-
              ted  or if the special host name "global" is specified, the cell
              global configuration is modified.  The  format of the host  con-
              figuration is described in sge_conf(5).
              Requires root or manager privilege.

       -msconf                       <modify scheduler configuration>
              The  current  scheduler  configuration  (see  sched_conf(5))  is
              retrieved,  an  editor  is  executed (either vi(1) or the editor
              indicated by $EDITOR) and the changed  configuration  is  regis-
              tered  with  sge_qmaster(8)  upon  exit of the editor.  Requires
              root or manager privilege.

       -Msconf  fname                 <modify  scheduler  configuration   from
       file>
              The current scheduler configuration (see sched_conf(5)) is over-
              ridden  with  the configuration specified in the file.  Requires
              root or manager privilege.
       -sconf [host,...|global]      <show configuration>
              Print the cluster configuration being in effect globally  or  on
              specified  host(s).  If  the  optional comma separated host list
              argument is omitted or the special string global is  given,  the
              global  cell configuration is displayed.  For any other hostname
              in the list the merger of the global configuration and the  host
              specific  configuration  is  displayed.  The  format of the host
              configuration is described in sge_conf(5).

       -sconfl                       <show configuration list>
              Display a list of hosts for which configurations are  available.
              The special host name "global" refers to the cell global config-
              uration.

              ...<<<


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
IPRSCAN TWEAKS

=====================================================
I would like to apply a time limit to running jobs
How can I do it?
=====================================================
If is quite simple. Edit iprscan.conf and put 'job.time.limit' to 1.
Then configure the two following tags, 'poolinterval' (sleeping time in seconds
between two checking jobs) and maxpollrounds (number of checking jobs).

NOTE: BE AWARE THAT THIS CONFIGUATION IS NOT POSSIBLE WITH INSTALLTIONS USING
----- 'local' QUEUE!!! (MAYBE LATER).

=====================================================
I would like to have some different right on the session directory
to avoid other people to look in my session directory.
=====================================================
Top

Easy ;)!! Actually, the rights for the date and session directories
is stored in tooldefault.conf file under 'dirmode' tag. Default
value for this tag is 777 and the umask is set to 000. So this means
that anybody can creates/remove any directory under iprscan/tmp.
If you want to protect session directory, open iprscan.conf and
edit 'usermode' to put the value you want. If not value is set,
iprscan will use the default one stored in tooldefault.conf.


======================================================
I would like to use more than one cpu for my hmmer searches
using InterProScan. Is it possible to configure it?
======================================================
Top

Yes of course. Applications using hmmpfam or hmmsearch are configurable.
You just need to update/change the tag 'cpu_opt' in the applicaton's
configuration file you want to update/change.
Configuration file supporting this option are listed below :

- gene3d.conf      (GENE3D)
- hmmpanther.conf  (Panther)
- hmmpir.conf      (PIR superfamily)
- hmmpfam.conf     (Pfam)
- hmmsmart.conf    (Smart)
- hmmtigr.conf     (Tigr)
- superfamily.conf (SCOP/SUPERFAMILY)

If this tag 'cpu_opt' value is empty (default) the '--cpu' option is not used.

NOTE: By default, PIR is set to '--cpu 1'. 
-----

REM: Program Speed(s) (v4.3)
============================
HMMPfam	70
HMMPanther	12
HMMPIR	22
blastprodom	18
coils	7
gene3d	28
HMMSmart	8
HMMTigr	23
FPRINTScan	12
scanregexp	8
profilescan	17
superfamily	151
seg	8
signalp	7
tmhmm	7



</entry>



<entry [Thu Feb  8 17:27:31 EST 2007] ENABLED iprscan EXECUTION FROM NODE BY STARTING rsh>



SUMMARY (DID AS ROOT):

	service shell start
	service login start
	echo + >  /var/root/.rhosts
	chmod 600 /var/root/.rhosts

NOW CAN RUN iprscan2.pl FROM node001!!

NB: ALSO ADDED TO /private/var/root/.rhosts: 
node001.cluster.private vanwye


    http://www.bioinformatics.nl/iprscan/doc/FAQs.html#12
    -----------------------------------------------------

Do i have to start the jobs from the 'execution host' or is it
possible to start them from any host?

You can start it from any host which can do rsh to the
'execution host'.
Check that you have access to each host you want to run applications on
using 'rsh THE_HOST hostname' for example.
You could be also asked to edit the file called .rhost on each host
to allow connection to other host.
(e.g. : to allow connection from foo.bar.com to blah.co.uk as john user
your .rhost on blah must contain something like : 'foo.bar.com john').


    http://www.seriss.com/rush-current/misc/rsh-config.html
    -------------------------------------------------------

Configuring rsh(1) on Mac/OSX

    Enabling rsh(1) is not required for Rush, but having it enabled
    can simplify the task of administering large networks.

    Enabling rsh on the mac is easy; run these commands as root:

#      service login start    -- enables rlogin, eg. 'rlogin host'
#      service shell start    -- enables rsh, eg. 'rsh host cmd' 

	service shell start
	service login start

	echo + >  /var/root/.rhosts
	chmod 600 /var/root/.rhosts

    Be sure to disable (or configure) the software firewall
    so that it does not prevent the rsh protocol from working.

    You should then be able to run commands as root from other
    machines to this one, eg:

    rcp -rp /usr/local/rush newhost:/usr/local/rush
	rsh newhost /usr/local/rush/etc/bin/install.sh
	rsh newhost /usr/local/rush/etc/S99rush restart


</entry>



<entry [Wed Feb  7 22:46:11 EST 2007] PROBLEM WHEN LAUNCHING iprscan (USING /common/apps/iprscan) FROM AN EXECUTION NODE>



gems.rsmas.miami.edu CONNECTION REFUSED 

man rsh
>>>...

FILES
     /etc/hosts
     /etc/auth.conf
...<<<

em /etc/hosts
>>>
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1       localhost
255.255.255.255 broadcasthost
::1             localhost
# 192.168.2.3   node003.cluster.private
<<<

AND NO /etc/auth.conf PRESENT, SO ADDED node001 TO /etc/hosts:

192.168.2.1     node001.cluster.private


</entry>



<entry [Wed Feb  7 19:29:45 EST 2007] REINSTALL OF iprscan IN /common/apps/iprscan WITHOUT SGE SUPPORT >



gems:/common/apps/iprscan local$ sudo ./Config.pl 

####################################################################################################
#                Welcome to the InterProScan v4.2 installation script.
#
#  Tips:
#     - If you are happy with the suggestion between [], just press <enter>
#     - Whatever is shown between '/' and '/' is considered a valid input pattern
####################################################################################################


    Reconfigure everything? (first time install) [n] /(y|n)/? : y

! Tip:
!
! InterProScan needs to know where it is installed and where perl is installed
!
    Do you want to set paths to perl and the installation directory? [y] /y|n/? : y

! Tip:
!
! If your servers are using a shared file system, such as NFS, InterProScan is able to utilise this
! and perform distributed computing across multiple servers
! You will need set up the full UNIX path to the installation directory.
! You also need to ensure that the InterProScan installation is on a SHARED DISK.
!
    Please enter the full path for the InterProScan installation [/Volumes/gemshd4/common/apps/iprscan] /.+/? : /common/apps/iprscan
    Do you want to set another Perl command in place of [/usr/bin/perl]? [n] /y|n/? : n
>>Changing Perl path in scripts ... >>DONE

>>Checking if your Perl installation has the modules required by InterProScan...
>>All modules needed are already installed.


! Tip:
!
! To cope with bulk jobs and to enable efficient parallelization, InterProScan splits input files
! with FASTA formatted sequences into smaller parts (chunks). Default size is "10" sequences per chunk
!
    Do you want to setup chunk size [y] /(y|n)/? : y

! Tip:
!
! Here you should specify the maximum number of sequences allowed in each part (chunk).  Please note
! it is not recommended to have more then 3000 chunks.
!
    Enter chunk size [10] /[0-9]+/? : 50


! Tip:
!
! You can configure InterProScan to limit the number of input sequences (protein & nucleotide),
! set a maximum length for nucleotide sequences and a minimum length for protein sequences.
! You can also configure the minimum length for a translated orf and use the codon table value
! to translate the dna/rna sequence into six frames (http://www.ebi.ac.uk/cgi-bin/mutations/trtables.cgi).
!
    Do you want to configure this? [y] /(y|n)/? : y
    Enter the maximum number of input protein sequences allowed [1000] /\d+/? : 
    Enter the maximum number of input nucleic sequences allowed [100] /\d+/? : 
    Enter the maximum length (in nucleic acids) for a nucleotide sequence [10000] /\d+/? : 
    Enter the minimum length (in amino acids) for a protein sequence [5] /\d+/? : 10
    Enter the minimum allowed length of a translated orf [50] /\d+/? : 30
    Enter the default codon table value to use to translate dna/rna in six frames [0] /\d+/? : 


! Tip:
!
! InterProScan currently encorporates searching against 13 databases. You have the choice whether or not to
! set up a particular search tool against a database and how exactly that application will be configured to run
! in this next section. You will configure the queue systems (if any) you will use, first
!
    Do you want to setup applications (if you don't, no applications will be included in InterProScan by default)? [y] /(y|n)/? : y

! Tip:
!
! InterProScan is able to execute the applications via a UNIX QUEUING system which has an INTERACTIVE mode (such as LSF).
! Currently, we mainly support LSF (which is used at EBI) however, we also provide configuration files for other systems
! which are listed in the documentation
!
    Do you wish to use a queue system? [n] /(y|n)/? : n
>>Checking your local architecture for InterProScan compatibility

! Tip:
! Blastprodom (a wrapper script on top of Blast package) is used to search against PRODOM families
    Do you want to use blastprodom ? [y] /(y|n)/? : 
!
! PLEASE NOTE:
! InterProScan can be launched through a cluster machine using RSH. To be able to utilise this
! feature, all your machines must be visible to each other (see the .rhost file) and be mounted on
! a shared file system, such as NFS
! (If you are installing InterProScan for use on only one machine please ignore the above message)
!
    
Please enter the execution host name of blastprodom [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Coils is used to predict coiled-coil segments with the algorithm of Lupas et al.
    Do you want to use coils ? [y] /(y|n)/? : 
    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the executi "cd /common/apps/iprscan; hostname" 2> /common/apps/iprscan/rsh_err"
rcmd: getaddrinfo: No add) is not supported in this distribution, try another host.
    ING: The "" UNIX OS (
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? :     

! Tip:
! Gene3d Gene3D is supplementary to the CATH database. This protein sequence database contains proteins from complete genomes which have been clustered into protein families and annotated with CATH domains, Pfam domains and functional information from KEGG, GO, COG, Affymetrix and STRINGS.
    Do you want to use gene3d ? [y] /(y|n)/? : 
    
Please enter the execution host name of gene3d [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmpanther (hmmpfam from HMMER 2.3.2 package) PANTHER classifies proteins into families and specific functional subfamilies, which are then mapped to ontology terms and pathways.
    Do you want to use hmmpanther ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmpanther [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmpir PIR produces the Protein  Sequence Database (PSD) of functionally annotated protein sequences, which grew out of the Atlas of Protein Sequence and Structure (1965-1978) edited by Margaret Dayhoff and has been incorporated into an integrated knowledge base system of value-added databases and analytical tools.
    Do you want to use hmmpir ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmpir [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmpfam (hmmpfam from HMMER 2.3.2 package) is used to search against the Pfam HMM (Profile hidden Markov models) database
    Do you want to use hmmpfam ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmpfam [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmsmart (hmmpfam from HMMER 2.3.2 package) is used to search against the Smart HMM (Profile hidden Markov models) database
    Do you want to use hmmsmart ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmsmart [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmtigr (hmmpfam from HMMER 2.3.2 package) is used to search against TIGRFAMs collection of HMMs (Profile hidden Markov models)
    Do you want to use hmmtigr ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmtigr [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Fprintscan (FingerPRINTScan) is used to search against the PRINTS collection of protein signatures
    Do you want to use fprintscan ? [y] /(y|n)/? : 
    
Please enter the execution host name of fprintscan [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Scanregexp (ScanProsite) is used to search against the PROSITE patterns collection and verify the  matches by statistically significant CONFIRM patterns
    Do you want to use scanregexp ? [y] /(y|n)/? : 
    
Please enter the execution host name of scanregexp [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Profilescan (ProfileScan (pfscan & ps_scan.pl) from Pftools 2.2) package is used  to search against the PROSITE profiles collection
    Do you want to use profilescan ? [y] /(y|n)/? : 
    
Please enter the execution host name of profilescan [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Superfamily A structural classification of proteins database for the investigation of sequences and structures.
    Do you want to use superfamily ? [y] /(y|n)/? : 
    
Please enter the execution host name of superfamily [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Seg is used for identifying and masking segments of low compositional complexity in amino acid sequences
    Do you want to use seg ? [y] /(y|n)/? : 
    
Please enter the execution host name of seg [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Signalp predicts the presence and location of signal peptide cleavage sites, using eukaryotic HMM (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use signalp ? [n] /(y|n)/? : y
    
Please enter the execution host name of signalp [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Tmhmm is used for prediction of transmembrane helices in proteins using a Hidden Markov Model (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use tmhmm ? [n] /(y|n)/? : y
    
Please enter the execution host name of tmhmm [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
!
! InterProScan can use taxonomy-based filtering of results which uses the InterPro taxonomy listed for each InterPro
! entry to hide hits not relevant to that taxon.
!
    Do you want to make it available for users? [y] /y|n/? : 

! Tip:
!
! You can specify an email address for the administrator. By default it will be you (root)
! Administrators receive an email when a problem occurs in InterProScan.
!
    Do you want to set an administrator email address? [y] /y|n/? : syoung@rsmas.miami.edu
WARNING: The information input is not valid (doesn't match /y|n/), please try again.
    Do you want to set an administrator email address? [y] /y|n/? : y
    Please enter the email address of the administrator: [] /[\w\.\-]+\@[\w\.\-]+/? : syoung@rsmas.miami.edu
>>Writing to the configuration files ... 
>>Processing file : /common/apps/iprscan/conf/blastprodom.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/coils.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/fprintscan.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/gene3d.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpanther.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpfam.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpir.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmsmart.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmtigr.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/profilescan.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/scanregexp.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/seg.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/signalp.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/superfamily.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/tmhmm.conf ... >>DONE
>>Setting File /common/apps/iprscan/conf/iprscan.conf ... >>DONE

! Tip:
!
! InterProScan can be launched through a web interface. It can also be launched through secure HTTP (https)
!
    Do you want to run InterProScan using a web interface? [n] /y|n/? : y
    Do you want to run InterProScan through a secure HTTP server? [n] /y|n/? : 
    Enter the name of your server. You can specify a specific port to listen to. (e.g. "foo.bar.com:4000") [foobar.com] /[\w\.\_\-\:\d]+/? : gems.rsmas.miami.edu
!
! Please add the following lines to your web server configuration (httpd.conf):
#--------------------------------------
 Alias /doc/ "/Volumes/gemshd3/common/apps/iprscan/doc/html/"
 Alias /images/ "/Volumes/gemshd3/common/apps/iprscan/images/"
 Alias /tmp/ "/Volumes/gemshd3/common/apps/iprscan/tmp/"

 ScriptAlias /iprscan/ "/Volumes/gemshd3/common/apps/iprscan/bin/"

 <Directory "/Volumes/gemshd3/common/apps/iprscan/images">
   Options Indexes MultiViews
   AllowOverride None
   Order allow,deny
   Allow from all
 </Directory>

 <Directory "/Volumes/gemshd3/common/apps/iprscan/tmp/">
   Options FollowSymLinks Includes SymLinksIfOwnerMatch
    IndexIgnore */.??* *~ *# */HEADER* */README* */RCS
    AllowOverride AuthConfig Limit FileInfo

    Order deny,allow
    Deny from all
    Allow from .rsmas.miami.edu
 </Directory>
#--------------------------------------
! Be sure that 'apache' or 'httpd' virtual user (uid/gid) have right to read/write into your /Volumes/gemshd3/common/apps/iprscan iprscan directory.
! Otherwise, if you are running a standalone http(s) web server, be sure also the user running it have to enough
! rights to read/wrtie /Volumes/gemshd3/common/apps/iprscan iprscan directory
! If you encoutered problem with your web server running iprscan, read the FAQ or contact your system administrator.

! After restarting your web server try http://gems.rsmas.miami.edu/iprscan/iprscan
!
>>Indexing datafile ... 
No specific file(s) given, indexing all files:

Pfam
prints.pval
prodom.ipr
Pfam-C
smart.desc
sf_hmm
sf.seq
Pfam-A.seed
superfamily.hmm
smart.thresholds
smart.HMMs
match.xml
interpro.xml
TIGRFAMs_HMM.LIB
Gene3D.hmm


No specific file(s) given, converting all files:

Pfam
sf_hmm
superfamily.hmm
smart.HMMs
sf_hmm_sub
TIGRFAMs_HMM.LIB
Gene3D.hmm

WARNING: Some problems occured when trying to index the data files.
 Please fix the error below and index your datafiles using the /common/apps/iprscan/bin/index_data.pl script.
 For help, type './index_data.pl -h' 
errormsg: ERROR: File [/common/apps/iprscan/data/sf.seq] not found : No such file or directory at /common/apps/iprscan/bin/index_data.pl line 217.
>>DONE

! Tip:
!
!  We would appreciate it if you would register your installation of InterProScan.  This will allow us to notify you
! of bug fixes and new releases of the software. We will not use your email address for any other purpose.
!
    Would you like to register InterProScan? [y] /(y|n)/? : n

>>Installation is completed.  Please test your installation by running on the command line:

        /common/apps/iprscan/bin/iprscan -cli -i /common/apps/iprscan/test.seq -o /common/apps/iprscan/test.out -format raw -goterms -ipr

and comparing it with the merged.raw file that is supplied.
For help and options on the commandline, run /common/apps/iprscan/bin/iprscan -cli -h

</entry>



<entry [Thu Feb  1 21:12:27 EST 2007] JOB FAILED TO RUN ON MOST TASKS>



./orthologuesarrayiprscan.pl -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.6 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.6 -l 
>>>
Arguments: -l
Logfile opened:

/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.6.log

Use of uninitialized value in concatenation (.) or string at ./orthologuesarrayiprscan.pl line 233, <FILE> line 1.
#!sh
#$ -S /bin/bash

# Tell the SGE that this is an array job, with "tasks" to be numbered 1 to 'max_tasks' (default=16)
# #$ -t 1-10:1

Join output with errors and place in current working directory
#$ -j y
#$ -cwd

# When a single command in the array job is sent to a compute node,
# its task number is stored in the variable SGE TASK ID,
# so we can use the value of that variable to get the results we want:

 /Users/local/FUNNYBASE/bin/unigene/iprscan6.pl -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.6.$SGE_TASK_ID -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.6.$SGE_TASK_ID -l

############################# End of shell script

qsub command: qsub -sync y -t 1-11 /Users/local/FUNNYBASE/pipeline/orthologues/fasta/ortho.6.arrayiprscan.sh
Completed qsub. Job output:

qsub command: qsub -sync y -t 1-11 /Users/local/FUNNYBASE/pipeline/orthologues/fasta/ortho.6.arrayiprscan.sh
Completed qsub. Job output:

Your job-array 15615.1-11:1 ("ortho.6.arrayiprscan.sh") has been submitted
Job 15615.11 exited with exit code 2.
Unable to run job 15615.3
Unable to run job 15615.1
Unable to run job 15615.5
Unable to run job 15615.4
Unable to run job 15615.2
Unable to run job 15615.7
Unable to run job 15615.10
Unable to run job 15615.9
Unable to run job 15615.8
Unable to run job 15615.6

Job exit codes: 0 = Success, 99 = Requeue, Rest = Success: exit code in accounting file

Job ID: Your job-array 15615.1-11:1 ("ortho.6.arrayiprscan.sh") has been submitted
Job 15615.11 exited with exit code 2.
Unable to run job 15615.3
Unable to run job 15615.1
Unable to run job 15615.5
Unable to run job 15615.4
Unable to run job 15615.2
Unable to run job 15615.7
Unable to run job 15615.10
Unable to run job 15615.9
Unable to run job 15615.8
Unable to run job 15615.6


Run time: 03:17:43
Completed ./orthologuesarrayiprscan.pl
10:58PM, 1 February 2007
****************************************
<<<


reaping job "15615" ptf complains: Job does not exist

Unable to run job 15615.3


qacct -j 15615
==============================================================
qname        all.q               
hostname     node016.cluster.private
group        www                 
owner        www                 
project      NONE                
department   defaultdepartment   
jobname      ortho.6.arrayiprscan.sh
jobnumber    15615               
taskid       11                  
account      sge                 
priority     0                   
qsub_time    Thu Feb  1 19:40:31 2007
start_time   Thu Feb  1 19:42:18 2007
end_time     Thu Feb  1 19:42:18 2007
granted_pe   NONE                
slots        1                   
failed       0    
exit_status  2                   
ru_wallclock 0            
ru_utime     0            
ru_stime     0            
ru_maxrss    0                   
ru_ixrss     0                   
ru_ismrss    0                   
ru_idrss     0                   
ru_isrss     0                   
ru_minflt    0                   
ru_majflt    0                   
ru_nswap     0                   
ru_inblock   0                   
ru_oublock   57                  
ru_msgsnd    708                 
ru_msgrcv    705                 
ru_nsignals  2                   
ru_nvcsw     761                 
ru_nivcsw    0                   
cpu          0            
mem          0.000             
io           0.000             
iow          0.000             
maxvmem      0.000
==============================================================
qname        all.q               
hostname     node007.cluster.private
group        www                 
owner        www                 
project      NONE                
department   defaultdepartment   
jobname      ortho.6.arrayiprscan.sh
jobnumber    15615               
taskid       3                   
account      sge                 
priority     0                   
qsub_time    Thu Feb  1 19:40:31 2007
start_time   Thu Feb  1 20:02:14 2007
end_time     Thu Feb  1 23:19:48 2007
granted_pe   NONE                
slots        1                   
failed       100 : assumedly after job
exit_status  137                 
ru_wallclock 11854        
ru_utime     0            
ru_stime     0            
ru_maxrss    0                   
ru_ixrss     0                   
ru_ismrss    0                   
ru_idrss     0                   
ru_isrss     0                   
ru_minflt    0                   
ru_majflt    0                   
ru_nswap     0                   
ru_inblock   0                   
ru_oublock   40                  
ru_msgsnd    364                 
ru_msgrcv    364                 
ru_nsignals  1                   
ru_nvcsw     407                 
ru_nivcsw    0                   
cpu          0            
mem          0.000             
io           0.000             
iow          0.000             
maxvmem      0.000
============================================



</entry>



<entry [Thu Feb  1 19:44:54 EST 2007] QPING>



IN /etc/services:
>>>...
sge_qmaster     701/tcp # Sun GridEngine 6.0 qmaster daemon
sge_execd       702/tcp # Sun GridEngine 6.0 execution daemon
...<<<

qping -info gems.rsmas.miami.edu 701 qmaster 1
>>>
02/01/2007 19:56:01:
SIRM version:             0.1
SIRM message id:          1
start time:               01/30/2007 18:51:16 (1170201076)
run time [s]:             176685
messages in read buffer:  2
messages in write buffer: 0
nr. of connected clients: 21
status:                   0
info:                     TET: R (1.13) | EDT: R (0.13) | SIGT: R (176682.42) | MT(1): R (0.08) | MT(2): R (0.19) | OK
Monitor:                  disabled
<<<

qping -info node001 702 execd 1
>>>
02/01/2007 19:56:23:
SIRM version:             0.1
SIRM message id:          1
start time:               01/22/2007 21:52:03 (1169520723)
run time [s]:             858279
messages in read buffer:  0
messages in write buffer: 0
nr. of connected clients: 2
status:                   0
info:                     dispatcher: R (0.00) | OK
Monitor:                  disabled
<<<

man qping
=========
NAME
       qping - check application status of Grid Engine daemons.

SYNTAX
       qping [-help] [-noalias] [-ssl|-tcp] [ [ [-i <interval>] [-info] [-f] ]
       | [ [-dump_tag tag [param] ] [-dump] [-nonewline]  ]  ]  <host>  <port>
       <name> <id>

DESCRIPTION
       Qping  is  used to validate the runtime status of a Grid Engine service
       daemon. The current Grid Engine implementation allows one to query  the
       SGE_QMASTER  daemon and any running SGE_EXECD daemon. The qping command
       is used to send a SIM (Status Information Message) to  the  destination
       daemon.  The  communication  layer of the specified daemon will respond
       with a SIRM (Status Information Response Message) which contains status
       information about the consulted daemon.

       The  qping  -dump  and  -dump_tag  options allowing an administrator to
       observe the communication protocol data flow of a Grid  Engine  service
       daemon.  The  qping -dump instruction must be started with root account
       and on the same host where the observed daemon is running.

OPTIONS
   -f
       Show full status information on each ping interval.

       First output Line: The first output line shows the date and time of the
       request.

       SIRM  version:  Internal version number of the SIRM (Status Information
       Response Message)

       SIRM message id: Current message id for this connection

       start time: Start time of daemon. The format is as follows:

       MM/DD/YYYY HH:MM:SS (seconds since 01.01.1970)

       run time [s]: Run time in seconds since start time

       messages in read buffer: Nr.  of  buffered  messages  in  communication
       buffer.  The  messages  are buffered for the application (daemon). When
       this number grows too large the daemon is not able to handle  all  mes-
       sages sent to it.

       messages in write buffer: Nr. of buffered messages in the communication
       write buffer. The messages are sent from the  application  (daemon)  to
       the  connected clients, but the communication layer wasn't able to send
       the messages yet. If this number grows  too  large,  the  communication
       layer isn't able to send them as fast as the application (daemon) wants
       the messages to be sent.

       nr. of connected clients:  This  is  the  number  of  actual  connected
       clients to this daemon. This also implies the current qping connection.

       status: The status value of the  daemon.  This  value  depends  on  the
       application  which reply to the qping request.  If the application does
       not provide any information the status is 99999.  Here are the possible
       status information values for the Grid Engine daemons:

       qmaster:

       0 There is no unusual timing situation.

       1 One or more threads has reached warning timeout. This may happen when
       at least one thread does not increment his time stamp for a  not  usual
       long  time.  A  possible  reason  for  this is a high workload for this
       thread.

       2 One or more threads has reached error timeout. This may  happen  when
       at  least one thread has not incremented his time stamp for longer than
       10 minutes.

       3 The time measurement is not initialized.

      execd:

       0 There is no unusual timing situation.

       1 Dispatcher has reached warning timeout. This may happen when the dis-
       patcher  does  not  increment his time stamp for a unusual long time. A
       possible reason for this is a high workload.

       2 Dispatcher has reached error timeout. This may happen when  the  dis-
       patcher  has not incremented his time stamp for longer than 10 minutes.

       3 The time measurement is not initialized.

       info: Status message of the daemon. This value depends on the  applica-
       tion  which  reply  to  the qping request.  If the application does not
       provide any information the info message is "not available".  Here  are
       the possible status information values for the Grid Engine daemons:

       qmaster:

       The  info  message  contains information about the qmaster threads fol-
       lowed by a thread state and time information. Each time when one of the
       known  threads  pass  through  their  main loop the time information is
       updated. Since the qmaster has two message threads every message thread
       updates  the  time.  This means the timeout for the message thread (MT)
       can only occur when no message thread is active anymore:

       THREAD_NAME: THREAD_STATE (THREAD_TIME)

       THREAD_NAME:
       EDT:  Event Delivery Thread
       TET:  Timed Event Thread
       MT:   Message Thread(s)
       SIGT: SIGnal Thread

       THREAD_STATE:
       R: Running
       W: Warning
       E: Error

       THREAD_TIME:
       Time since last timestamp updating.

       After the dispatcher  information  follows  an  additional  information
       string which describes the complete application status.

       execd:

       The  info  message  contains  information for the execd job dispatcher:
       dispatcher: STATE (TIME)

       STATE:
       R: Running
       W: Warning
       E: Error

       TIME:
       Time since last timestamp updating.

       After the thread information follows an additional  information  string
       which describes the application status.

   -help
       Prints a list of all options.

   -i interval
       Set qping interval time.

       The  default interval time is one second. Qping will send a SIM (Status
       Information Message) on each interval time.

   -info
       Show full status information (see -f for more  information)  and  exit.
       The exit value 0 indicates no error. On errors qping returns with 1.

   -noalias
       Ignore  host_aliases  file,  which is located at <sge_root>/<cell>/com-
       mon/host_aliases.  If this option is used it is not  necessary  to  set
       any Grid Engine environment variable.

 -ssl
       This option can be used to specify an SSL (Secure Socket Layer) config-
       uration. The qping will use the configuration to  connect  to  services
       running  SSL.  If the SGE settings file is not sourced, you have to use
       the -noalias option to bypass the need  for  the  SGE_ROOT  environment
       variable.   The  following  environment  variables are used to specifiy
       your certificates:
         SSL_CA_CERT_FILE - CA certificate file
         SSL_CERT_FILE    - certificates file
         SSL_KEY_FILE     - key file
         SSL_RAND_FILE    - rand file

   -tcp
       This option is used to select TCP/IP as the protocol used to connect to
       other services.

   -nonewline
       Dump  output will not have a linebreak within a message and binary mes-
       sages are not unpacked.

   -dump
       This option allows an administrator to observe the communication proto-
       col data flow of a Grid Engine service daemon. The qping -dump instruc-
       tion must be started as root and on the same host  where  the  observed
       daemon is running.

       The   output   is   written   to   stdout.   The   enviroment  variable
       "SGE_QPING_OUTPUT_FORMAT" can be set to hide  columns,  set  a  default
       column width or to set a hostname output format. The value of the envi-
       ronment variable can be set to any combination of the following  speci-
       fiers separated by a space character:
              "h:X"   -> hide column X
              "s:X"   -> show column X
              "w:X:Y" -> set width of column X to Y
              "hn:X"  -> set hostname output parameter X.
                         X values are "long" or "short"

       Start qping -help to see which columns are available.

   -dump_tag tag [param]
       This  option  has  the  same the same meaning as -dump, but can provide
       more information by specifying the debug level and message types  qping
       should  print: -dump_tag ALL <debug level> This option shows all possi-
       ble debug messages (APP+MSG) for  the  debug  levels,  ERROR,  WARNING,
       INFO,  DEBUG  and DPRINTF. The contacted service must support this kind
       of debugging.  This option is not currently implemented.  -dump_tag APP
       <debug level> This option shows only application debug messages for the
       debug levels, ERROR, WARNING, INFO, DEBUG and  DPRINTF.  The  contacted
       service  must  support this kind of debugging.  This option is not cur-
       rently implemented.  -dump_tag MSG This option has the  same  behaviour
       as the -dump option.

   host
       Host where daemon is running.

   port
       Port which daemon has bound (used sge_qmaster/sge_execd port number).

   name
       Name  of communication endpoint ("qmaster" or "execd"). A communication
       endpoint is a  triplet  of  hostname/endpoint  name/endpoint  id  (e.g.
       hostA/qmaster/1 or subhost/qstat/4).

   id
       Id of communication endpoint ("1" for daemons)

EXAMPLES
       >qping master_host 31116 qmaster
       08/24/2004 16:41:15 endpoint master_host/qmaster/1 at port 31116 is up since 365761 seconds
       08/24/2004 16:41:16 endpoint master_host/qmaster/1 at port 31116 is up since 365762 seconds
       08/24/2004 16:41:17 endpoint master_host/qmaster/1 at port 31116 is up since 365763 seconds

       > qping -info master_host 31116 qmaster 1
       08/24/2004 16:42:47:
       SIRM version:             0.1
       SIRM message id:          1
       start time:               08/20/2004 11:05:14 (1092992714)
       run time [s]:             365853
       messages in read buffer:  0
       messages in write buffer: 0
       nr. of connected clients: 4
       status:                   0
       info:                     ok

       > qping -info execd_host 31117 execd 1
       08/24/2004 16:43:45:
       SIRM version:             0.1
       SIRM message id:          1
       start time:               08/20/2004 11:06:13 (1092992773)
       run time [s]:             365852
       messages in read buffer:  0
       messages in write buffer: 0
       nr. of connected clients: 2
       status:                   0
       info:                     ok

ENVIRONMENTAL VARIABLES
       SGE_ROOT       Specifies  the location of the Grid Engine standard con-
                      figuration files.

       SGE_CELL       If set, specifies the default Grid Engine cell.

SEE ALSO
       sge_intro(1), SGE_H_ALIASES(5), sge_qmaster(8), sge_execd(8).

COPYRIGHT
       See sge_intro(1) for a full statement of rights and permissions.




qping -f
========

GE 6.0u9
usage: qping [-help] [-noalias] [-ssl|-tcp] [ [ [-i <interval>] [-info] [-f] ] | [ [-dump_tag tag [param] ] [-dump] [-nonewline] ] ] <host> <port> <name> <id>
   -i         : set ping interval time
   -info      : show full status information and exit
   -f         : show full status information on each ping interval
   -noalias   : ignore $SGE_ROOT/SGE_CELL/common/host_aliases file
   -ssl       : use SSL framework
   -tcp       : use TCP framework
   -dump      : dump communication traffic (see "communication traffic output options" for additional information)
                   (provides the same output like -dump_tag MSG)
   -dump_tag  : dump communication traffic (see "communication traffic output options" for additional information)
                   tag=ALL <debug level> - show all
                   tag=APP <debug level> - show application messages
                   tag=MSG               - show commlib protocol messages
                   <debug level>         - ERROR, WARNING, INFO, DEBUG or DPRINTF
   -nonewline : dump output will not have a linebreak within a message
   -help      : show this info
   host       : host name of running component
   port       : port number of running component
   name       : name of running component (e.g.: "qmaster" or "execd")
   id         : id of running component (e.g.: 1 for daemons)

example:
   qping -info clustermaster 5000 qmaster 1

communication traffic output options:
   The environment variable SGE_QPING_OUTPUT_FORMAT can be used to hide columns and
   to set default column width. For hostname output the parameter hn is used.
   SGE_QPING_OUTPUT_FORMAT="h:1 h:4 w:1:20"
   will hide the columns 1 and 4 and set the width of column 1 to 20 characters.
       h:X   -> hide column X
       s:X   -> show column X
       w:X:Y -> set width of column X to Y
       hn:X  -> set hostname output parameter X. X values are "long" or "short"

   available columns are:

   nr active name      description
   == ====== ====      ===========
   01 yes    time      time of debug output creation
   02 yes    local     endpoint service name where debug client is connected
   03 yes    d.        message direction
   04 yes    remote    name of participating communication endpoint
   05 yes    format    message data format
   06 yes    ack type  message acknowledge type
   07 yes    msg tag   message tag information
   08 yes    msg id    message id
   09 yes    msg rid   message response id
   10 yes    msg len   message length
   11 yes    msg time  time when message was sent/received
   12  no    msg dump  message content dump (xml/bin/cull)
   13  no    info      additional information
   14 yes    msg ltime commlib linger time
   15 yes    con count nr. of connections
   
</entry>



<entry [Wed Jan 31 20:08:04 EST 2007] STALLED JOBS ALL SHOWED gems OVERLOADED ON qstat -j jobno:>



queue instance "all.q@gems.rsmas.miami.edu" dropped because it is overloaded: np_load_avg=2.275146 (no load adjustment) >= 1.75


:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       3.68     darwin        ad
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   1/2       0.01     darwin        
  14311 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   1/2       0.02     darwin        
  14316 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.09     darwin        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   0/2       0.06     darwin        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.04     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   1/2       0.06     darwin        
  14312 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.08     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   1/2       0.03     darwin        
  14315 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   1/2       0.09     darwin        
  14309 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   1/2       0.00     darwin        
  14310 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   1/2       0.01     darwin        
  14314 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        

Wed Jan 31 20:08:06 EST 2007
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       3.68     darwin        ad
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   1/2       0.01     darwin        
  14311 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   1/2       0.02     darwin        
  14316 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.09     darwin        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   0/2       0.06     darwin        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.04     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   1/2       0.06     darwin        
  14312 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.08     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   1/2       0.03     darwin        
  14315 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   1/2       0.09     darwin        
  14309 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   1/2       0.00     darwin        
  14310 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   1/2       0.01     darwin        
  14314 0.55500 iprscan-20 www          r     01/31/2007 19:11:38     1      


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       2.63     darwin        d
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   1/2       0.14     darwin        
  13916 0.55500 iprscan-20 www          r     01/31/2007 13:49:08     1        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   1/2       0.01     darwin        
  13516 0.55500 iprscan-20 www          r     01/31/2007 12:55:38     1        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   1/2       0.08     darwin        
  13518 0.55500 iprscan-20 www          r     01/31/2007 12:55:38     1        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.08     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   2/2       0.15     darwin        
  13520 0.55500 iprscan-20 www          r     01/31/2007 12:55:38     1        
  13874 0.55500 iprscan-20 www          r     01/31/2007 13:45:53     1        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   2/2       0.12     darwin        
  13258 0.55500 iprscan-20 www          r     01/30/2007 19:18:08     1        
  13519 0.55500 iprscan-20 www          r     01/31/2007 12:55:38     1        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   1/2       0.09     darwin        
  13517 0.55500 iprscan-20 www          r     01/31/2007 12:55:38     1        
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   1/2       0.06     darwin        
  13521 0.55500 iprscan-20 www          r     01/31/2007 12:55:38     1        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   1/2       0.08     darwin        
  13514 0.55500 iprscan-20 www          r     01/31/2007 12:55:38     1        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   2/2       0.10     darwin        
  13515 0.55500 iprscan-20 www          r     01/31/2007 12:55:38     1        
  13914 0.55500 iprscan-20 www          r     01/31/2007 13:49:08     1        
 
qstat -j 13914


JOB 1
/common/iprscan/bin/iprscan: /common/iprscan/tmp/20070131/iprscan-20070131-13462999/iprscan-20070131-13462999.dchk exited with status 32768
[Util::contents] Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.5.all.4'

cd /common/iprscan/tmp/20070131/iprscan-20070131-13462999
em *djob
>>>13915<<<
qacct -j 13915

JOB 2
/common/iprscan/bin/iprscan: /common/iprscan/tmp/20070131/iprscan-20070131-13453625/iprscan-20070131-13453625.dchk exited with status 32768
[Util::contents] Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.3.all.0'
cd /common/iprscan/tmp/20070131/iprscan-20070131-13453625
>>>13874<<<
qacct -j 13874

JOB 3

/common/iprscan/bin/iprscan: /common/iprscan/bin/iprscan: /common/iprscan/tmp/20070131/iprscan-20070131-12553179/iprscan-20070131-12553179.dchk exited with status 32768
[Util::contents] Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.2.all.7'
em *job
>>>13520<<<

cd /common/iprscan/tmp/20070131/iprscan-20070131-12553179
em *djob
>>>13520<<<

gems:/common/iprscan/tmp/20070131/iprscan-20070131-12553179 local$ qacct -j 13520
error: job id 13520 not found
gems:/common/iprscan/tmp/20070131/iprscan-20070131-12553179 local$ qstat -j 13520
==============================================================
job_number:                 13520
exec_file:                  job_scripts/13520
submission_time:            Wed Jan 31 12:55:32 2007
owner:                      www
uid:                        70
group:                      www
gid:                        70
sge_o_home:                 /Users/www
sge_o_path:                 /Users/local/apps/cytoscape/PLUGINS:/usr/lib/perl5/site_perl:/usr/share/apps/ActiveTcl/bin:/usr/local/mysql/bin:/usr/local/bin:/usr/share/apps/komodo:/sw/bin:/Users/local/FUNNYBASE/bin:/common/sge/bin/darwin:/bin:/sbin:/usr/bin:/usr/sbin:/common/mpich-1.2.7/ch_p4/bin:/common/bin:/common/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin:/usr/local/biotools/bin:/Applications/bioinf/phylip/exe:/Applications/bioinf/t_coffee:/usr/local/biotools/bin
sge_o_shell:                /bin/bash
sge_o_workdir:              /Users/local/FUNNYBASE/bin/pipeline4
sge_o_host:                 gems
account:                    sge
stderr_path_list:           /dev/null
mail_list:                  www@gems.rsmas.miami.edu
notify:                     FALSE
job_name:                   iprscan-20070131-12553179
stdout_path_list:           /dev/null
jobshare:                   0
env_list:                   PERL5LIB=/RemotePerl
script_file:                /common/iprscan/tmp/20070131/iprscan-20070131-12553179/iprscan-20070131-12553179.dcmd
usage    1:                 cpu=00:00:00, mem=0.00000 GBs, io=0.00000, vmem=N/A, maxvmem=N/A


scheduling info:            queue instance "all.q@gems.rsmas.miami.edu" dropped because it is overloaded: np_load_avg=5.398926 (= 5.398926 + 0.50 * 0.000000 with nproc=1) >= 1.75
                            queue instance "all.q@node011.cluster.private" dropped because it is full
...<<<


queue instance "all.q@gems.rsmas.miami.edu" dropped because it is overloaded: np_load_avg=5.398926 (= 5.398926 + 0.50 * 0.000000 with nproc=1) >= 1.75


</entry>



<entry [Wed Jan 31 12:59:30 EST 2007] NODE IN 'E' ERROR STATE>



qstat -explain E
>>>
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       5.67     darwin        ad
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.00     darwin        E
        queue all.q marked QERROR as result of job 11411's failure at host node001.cluster.private
        queue all.q marked QERROR as result of job 11411's failure at host node001.cluster.private
...<<<

REMOVED ERROR STATE

qmod -cq all.q@node001
>>>
www@gems.rsmas.miami.edu changed state of "all.q@node001.cluster.private" (no error)
<<<


</entry>



<entry [Wed Jan 31 12:30:41 EST 2007] STALLED JOB>



qstat -j 13258

==============================================================
job_number:                 13258
exec_file:                  job_scripts/13258
submission_time:            Tue Jan 30 19:18:02 2007
owner:                      www
uid:                        70
group:                      www
gid:                        70
sge_o_home:                 /Users/www
sge_o_path:                 /Users/local/apps/cytoscape/PLUGINS:/usr/lib/perl5/site_perl:/usr/share/apps/ActiveTcl/bin:/usr/local/mysql/bin:/usr/local/bin:/usr/share/apps/komodo:/sw/bin:/Users/local/FUNNYBASE/bin:/common/sge/bin/darwin:/bin:/sbin:/usr/bin:/usr/sbin:/common/mpich-1.2.7/ch_p4/bin:/common/bin:/common/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin:/usr/local/biotools/bin:/Applications/bioinf/phylip/exe:/Applications/bioinf/t_coffee:/usr/local/biotools/bin
sge_o_shell:                /bin/bash
sge_o_workdir:              /Users/local/FUNNYBASE/bin/unigene
sge_o_host:                 gems
account:                    sge
stderr_path_list:           /dev/null
mail_list:                  www@gems.rsmas.miami.edu
notify:                     FALSE
job_name:                   iprscan-20070130-19180295
stdout_path_list:           /dev/null
jobshare:                   0
env_list:                   PERL5LIB=/RemotePerl
script_file:                /common/iprscan/tmp/20070130/iprscan-20070130-19180295/iprscan-20070130-19180295.dcmd
usage    1:                 cpu=00:00:00, mem=0.00000 GBs, io=0.00000, vmem=N/A, maxvmem=N/A
scheduling info:            queue instance "all.q@node001.cluster.private" dropped because it is temporarily not available
                            queue instance "all.q@gems.rsmas.miami.edu" dropped because it is disabled


BUT NO ERROR MESSAGE IN IPRSCAN TMP DIRECTORY:

cd /common/iprscan/tmp/20070130/iprscan-20070130-19180295
ll
drwxrwxrwx   13 www  admin   442B Jan 30 19:18 .
drwxrwxrwx   74 www  admin     2K Jan 30 19:19 ..
drwxrwxrwx   81 www  admin     2K Jan 31 12:31 chunk_1
drwxrwxrwx   81 www  admin     2K Jan 31 12:31 chunk_2
drwxrwxrwx   81 www  admin     2K Jan 31 12:31 chunk_3
-rwxr-xr-x    1 www  admin     1K Jan 30 19:18 iprscan-20070130-19180295.dcmd
-rw-rw-rw-    1 www  admin     5B Jan 30 19:18 iprscan-20070130-19180295.djob
-rwxr-xr-x    1 www  admin   338B Jan 30 19:18 iprscan-20070130-19180295.dsub
-rw-r--r--    1 www  admin     0B Jan 30 19:18 iprscan-20070130-19180295.errors
-rw-rw-rw-    1 www  admin     4K Jan 30 19:18 iprscan-20070130-19180295.input
-rw-r--r--    1 www  admin    32K Jan 30 19:18 iprscan-20070130-19180295.input.inx
-rw-rw-rw-    1 www  admin     2K Jan 30 19:18 iprscan-20070130-19180295.params
-rw-rw-rw-    1 www  admin     2K Jan 30 19:18 iprscan-20070130-19180295.seqs


em iprscan-20070130-19180295.dcmd



</entry>



<entry [Tue Jan 30 18:03:00 EST 2007] STALLED JOBS>



DELETED THESE JOBS:

11656  qacct: failed       100 : assumedly after job
11657  qacct: failed       100 : assumedly after job
11658  qacct: failed       100 : assumedly after job
11659  qacct: failed       100 : assumedly after job
11660   qacct: failed       100 : assumedly after job
11661   qacct: failed       100 : assumedly after job
11662   qacct: failed       100 : assumedly after job
11663   qacct: failed       100 : assumedly after job
11664   qacct: failed       100 : assumedly after job


WHICH GAVE THE FOLLOWING OUTPUT ON ANOTHER TERMINAL:

  11661 0.55500 iprscan-20 www          dr    01/30/2007 20:09:05     1        

/common/iprscan/bin/iprscan: failed to check sge6 job iprscan-20070130-20085445:
  11659 0.55500 iprscan-20 www          dr    01/30/2007 20:09:05     1        

Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.2.all.6'
/common/iprscan/bin/iprscan: failed to check sge6 job iprscan-20070130-20085349:
  11657 0.55500 iprscan-20 www          dr    01/30/2007 20:09:05     1        

/common/iprscan/bin/iprscan: failed to check sge6 job iprscan-20070130-20085461:
  11662 0.55500 iprscan-20 www          dr    01/30/2007 20:09:05     1        

Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.2.all.0'
Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.2.all.7'
Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.2.all.2'

... WHICH WAS THE REASON FOR THE FAILURE, I.E., NOT IPRSCAN


</entry>



<entry [Mon Jan 29 17:49:19 EST 2007] STALLED IPRSCAN JOBS>



iprseq -j 10841
orthologues.fasta.16:>AK078768|S10856217|unigene_mouse
orthologues.fasta.16.all.4:>AK078768|S10856217|unigene_mouse
orthologues.fasta.2:>AK078768|S10856217|unigene_mouse
orthologues.fasta.2.all.6:>AK078768|S10856217|unigene_mouse

iprseq -j 10843
orthologues.fasta.2:>XM_854814|S26561951|unigene_dog
orthologues.fasta.2.all.1:>XM_854814|S26561951|unigene_dog

iprseq -j 10844
orthologues.fasta.2:>AB209430|S24302974|unigene_human
orthologues.fasta.2.all.0:>AB209430|S24302974|unigene_human

iprseq -j 10845
orthologues.fasta.2:>AK157386|S26697678|unigene_mouse
orthologues.fasta.2.all.5:>AK157386|S26697678|unigene_mouse

iprseq -j 10846
orthologues.fasta.2:>AK053889|S10838657|unigene_mouse
orthologues.fasta.2.all.3:>AK053889|S10838657|unigene_mouse

iprseq -j 10848
orthologues.fasta.2:>BC070912|S20119155|unigene_rat
orthologues.fasta.2.all.7:>BC070912|S20119155|unigene_rat

iprseq -j 10849
orthologues.fasta.2:>AK144007|S26696167|unigene_mouse
orthologues.fasta.2.all.4:>AK144007|S26696167|unigene_mouse
orthologues.fasta.20:>AK144007|S26696167|unigene_mouse
orthologues.fasta.20.all.3:>AK144007|S26696167|unigene_mouse
orthologues.fasta.6:>AK144007|S26696167|unigene_mouse

iprseq -j 11102
orthologues.fasta.3:>XM_531963|S22754813|unigene_dog
orthologues.fasta.3.all.1:>XM_531963|S22754813|unigene_dog

iprseq -j 11296
orthologues.fasta.2:>AK144007|S26696167|unigene_mouse
orthologues.fasta.2.all.4:>AK144007|S26696167|unigene_mouse
orthologues.fasta.20:>AK144007|S26696167|unigene_mouse
orthologues.fasta.20.all.3:>AK144007|S26696167|unigene_mouse
orthologues.fasta.6:>AK144007|S26696167|unigene_mouse

iprseq -j 11297
orthologues.fasta.2:>AK157386|S26697678|unigene_mouse
orthologues.fasta.2.all.5:>AK157386|S26697678|unigene_mouse

iprseq -j 11298
orthologues.fasta.16:>AK078768|S10856217|unigene_mouse
orthologues.fasta.16.all.4:>AK078768|S10856217|unigene_mouse
orthologues.fasta.2:>AK078768|S10856217|unigene_mouse
orthologues.fasta.2.all.6:>AK078768|S10856217|unigene_mouse

iprseq -j 11299

orthologues.fasta.2:>XM_854814|S26561951|unigene_dog
orthologues.fasta.2.all.1:>XM_854814|S26561951|unigene_dog

iprseq -j 11300
orthologues.fasta.2:>AK053889|S10838657|unigene_mouse
orthologues.fasta.2.all.3:>AK053889|S10838657|unigene_mouse

iprseq -j 11302
orthologues.fasta.2:>BC070912|S20119155|unigene_rat
orthologues.fasta.2.all.7:>BC070912|S20119155|unigene_rat

iprseq -j 11303
orthologues.fasta.2:>AB209430|S24302974|unigene_human
orthologues.fasta.2.all.0:>AB209430|S24302974|unigene_human

iprseq -j 11304
orthologues.fasta.2:>BF393947|S14045691|unigene_rat
orthologues.fasta.2.all.8:>BF393947|S14045691|unigene_rat


</entry>



<entry [Sun Jan 28 18:08:04 EST 2007] STALLED IPRSCAN JOBS>



:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       1.93     darwin        d
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   1/2       0.08     darwin        
   9860 0.55500 iprscan-20 www          r     01/28/2007 15:06:20     1        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   1/2       0.08     darwin        
   9700 0.55500 iprscan-20 www          r     01/28/2007 14:51:35     1        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   2/2       0.24     darwin        
   9866 0.55500 iprscan-20 www          r     01/28/2007 15:08:05     1        
   9867 0.55500 iprscan-20 www          r     01/28/2007 15:08:20     1        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   1/2       0.09     darwin        
   9868 0.55500 iprscan-20 www          r     01/28/2007 15:08:35     1        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.04     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   2/2       0.20     darwin        
   9703 0.55500 iprscan-20 www          r     01/28/2007 14:51:35     1        
   9864 0.55500 iprscan-20 www          r     01/28/2007 15:06:35     1        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   1/2       0.01     darwin        
   9701 0.55500 iprscan-20 www          r     01/28/2007 14:51:35     1        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   1/2       0.01     darwin        
   9702 0.55500 iprscan-20 www          r     01/28/2007 14:51:35     1        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   2/2       0.08     darwin        
   9861 0.55500 iprscan-20 www          r     01/28/2007 15:06:20     1        
   9863 0.55500 iprscan-20 www          r     01/28/2007 15:06:20     1        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.01     darwin        

ALL PROBLEMS WITH ALL 4 SEQUENCES OF orthologues.fasta.1 (SEQS 0, 1, 2 AND 3)

iprseq -j 9860
orthologues.fasta.1:>AB017016|S1367513|unigene_human
orthologues.fasta.1.all.0:>AB017016|S1367513|unigene_human
orthologues.fasta.1.hmmpfam.0:>AB017016|S1367513|unigene_human

iprseq -j 9700
orthologues.fasta.1:>AB017016|S1367513|unigene_human
orthologues.fasta.1.all.0:>AB017016|S1367513|unigene_human
orthologues.fasta.1.hmmpfam.0:>AB017016|S1367513|unigene_human

iprseq -j 9703
orthologues.fasta.1:>BC054803|S16188919|unigene_mouse
orthologues.fasta.1.all.2:>BC054803|S16188919|unigene_mouse
orthologues.fasta.1.hmmpfam.2:>BC054803|S16188919|unigene_mouse

iprseq -j 9864
orthologues.fasta.1:>AB017016|S1367513|unigene_human
orthologues.fasta.1.all.0:>AB017016|S1367513|unigene_human
orthologues.fasta.1.hmmpfam.0:>AB017016|S1367513|unigene_human

iprseq -j 9701
orthologues.fasta.1:>XM_545196|S22746698|unigene_dog
orthologues.fasta.1.all.1:>XM_545196|S22746698|unigene_dog
orthologues.fasta.1.hmmpfam.1:>XM_545196|S22746698|unigene_dog

iprseq -j 9702
orthologues.fasta.1:>XM_341746|S16957406|unigene_rat
orthologues.fasta.1.all.3:>XM_341746|S16957406|unigene_rat
orthologues.fasta.1.hmmpfam.3:>XM_341746|S16957406|unigene_rat

iprseq -j 9861
orthologues.fasta.1:>XM_545196|S22746698|unigene_dog
orthologues.fasta.1.all.1:>XM_545196|S22746698|unigene_dog
orthologues.fasta.1.hmmpfam.1:>XM_545196|S22746698|unigene_dog

iprseq -j 9863
orthologues.fasta.1:>BC054803|S16188919|unigene_mouse
orthologues.fasta.1.all.2:>BC054803|S16188919|unigene_mouse
orthologues.fasta.1.hmmpfam.2:>BC054803|S16188919|unigene_mouse

</entry>



<entry [Sat Jan 27 14:08:44 EST 2007] STOPPED JOBS (STALLED ON SGE) AFTER RUNNING App.pm -ENABLED iprscan4.pl>



gems:~/FUNNYBASE/pipeline/orthologues/fasta local$ qstat -f
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       0.36     darwin        d
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   1/2       0.06     darwin        
   8292 0.55500 iprscan-20 www          r     01/27/2007 01:42:35     1        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   0/2       0.03     darwin        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   1/2       0.06     darwin        
   8294 0.55500 iprscan-20 www          r     01/27/2007 01:42:35     1        
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.05     darwin        
gems:~/FUNNYBASE/pipeline/orthologues/fasta local$ qstat -j 8294


FOUND THE FILES IN

/Users/local/FUNNYBASE/pipeline/orthologues/fasta

CORRESPONDING TO THE STALLED JOBS USING /Users/local/FUNNYBASE/bin/utils/iprseq.pl (AKA /usr/bin/iprseq):

iprseq -j 8292
>>>...
To find which 'orthologues.fasta' file the sequence belongs to do:

cd /Users/local/FUNNYBASE/pipeline/orthologues/fasta
grep "AK154972|S26688604|unigene_mouse" *

gems:/Users/local/FUNNYBASE/bin/utils root# cd /Users/local/FUNNYBASE/pipeline/orthologues/fasta
gems:/Users/local/FUNNYBASE/pipeline/orthologues/fasta root# grep "AK154972|S26688604|unigene_mouse" *
orthologues.fasta.16:>AK154972|S26688604|unigene_mouse
orthologues.fasta.16.all.2:>AK154972|S26688604|unigene_mouse
<<<

iprseq -j 8294
>>>...
orthologues.fasta.16:>AK078768|S10856217|unigene_mouse
orthologues.fasta.16.all.4:>AK078768|S10856217|unigene_mouse
orthologues.fasta.2:>AK078768|S10856217|unigene_mouse
<<<

NB: SAME SEQUENCES IN orthologues.fasta.2 !!

ALSO, THE ERROR MESSAGES FROM RUNNING

/Users/local/FUNNYBASE/bin/pipeline4/testApp.pl > testApp.log

SHOWED THAT orthologues.fasta.16.3 DIDN'T RUN BECAUSE THERE WERE NO TRANSLATED SEQUENCES ABOVE THE DEFAULT 50bp CUTOFF.

SO TRIED RE-RUNNING App.pm -ENABLED iprscan4.pl BUT WITH -nocrc AND -trnlen 30 OPTIONS ADDED TO iprscan4.pl:


CURRENT COMPLETENESS STATUS OF ORTHOLOGUES USING unigene/orthologuesiprscancomplete.all.pl

file	complete
1       0
2       0
3       0
4       0
5       0
6       0
7       0
8       0
9       0
10      0
11      0
12      1
13      1
14      1
15      0
16      1
17      1
18      0
19      1
20      0

SO RAN 10, 11, 14 (BY ACCIDENT), 15, 18 AND 20 (AT JUST BEFORE Sat Jan 27 16:59:06 EST 2007)

</entry>



<entry [Thu Jan 25 18:10:10 EST 2007] SOLVED iprscan2.pl PROBLEM>



USED -nocrc ARGUMENT FOR iprscan

/common/iprscan/lib/Index/Index.pm, sub getIndex WAS FINDING NO go ID FOR INTERPRO HIT IPR014039.

THIS MAY HAVE BEEN RELATED TO THE FACT THAT /common/iprscan/data/interpro.xml WAS REINDEXED ON JAN 1st.

ANNOTATED ERROR OUTPUT:

=head2 write_raw

 Description:  Creates raw file for entries retieved and parsed from match.xml

 Arguments:    $r_prot, reference to hash table containing protein name as key
               $r_ipr, reference to hash table containing ipr entries, protein name as key
               $r_matches, reference to hash table containing matches entries as key
               $seq_id, ID of the sequence returned this iprmatches entry.
               $fh, a reference to a file handle to write in.
               $ipr, iprlookup asked?
               $go, go terms asked?

 Returns:      1, '' on success.
               0, msg on failure

/common/iprscan/bin/iprscan_wrapper.pl sub writeRaw
	CALLING package: Dispatcher::Tool::InterProScan
	CALLING file: /common/iprscan/lib/Dispatcher/Tool/InterProScan.pm
	CALLING line: 3871
	CALLING sub: (eval)

	res: 1
	go_entry:
	go: 1
	r_prot: reference to hash table containing protein name as key
$VAR1 = {
          'Q9CX33' => {
                        'length' => '193',
                        'name' => 'Q9CX33_MOUSE',
                        'id' => 'Q9CX33',
                        'crc64' => '012E631189156F5B'
                      }
        };
	r_ipr: HASH(0x1989cf0)
$VAR1 = {
          'Q9CX33' => {
                        'SSF54713' => {
                                        'name' => 'Translation elongation factor EFTs/EF1B, dimerisation',
                                        'id' => 'IPR014039'
                                      },
                        'PS01126' => {
                                       'name' => 'Translation elongation factor EFTs/EF1B',
                                       'id' => 'IPR001816'
                                     },
                        'PS01127' => {
                                       'name' => 'Translation elongation factor EFTs/EF1B',
                                       'id' => 'IPR001816'
                                     },
                        'PTHR11741' => {
                                         'name' => 'Translation elongation factor EFTs/EF1B',
                                         'id' => 'IPR001816'
                                       },
                        'PF00627' => {
                                       'name' => 'Ubiquitin-associated/Translation elongation factor EF1B, N-terminal',
                                       'id' => 'IPR000449'
                                     },
                        'SSF46934' => {
                                        'name' => 'UBA-like',
                                        'id' => 'IPR009060'
                                      },
                        'PF00889' => {
                                       'name' => 'Translation elongation factor EFTs/EF1B, dimerisation',
                                       'id' => 'IPR014039'
                                     }
                      }
        };
	r_matches: HASH(0x1989d08)
$VAR1 = {
          'Q9CX33' => {
                        'SSF54713' => {
                                        'lcn' => [
                                                   {
                                                     'evalue' => '3.4E-11',
                                                     'end' => '146',
                                                     'start' => '101'
                                                   }
                                                 ],
                                        'status' => 'T',
                                        'name' => 'EF_TS',
                                        'iprbool' => 1,
                                        'id' => 'SSF54713',
                                        'evd' => 'HMMPfam',
                                        'dbname' => 'superfamily'
                                      },
                        'G3DSA:1.10.8.30' => {
                                               'lcn' => [
                                                          {
                                                            'evalue' => '6.30000254573025E-7',
                                                            'end' => '95',
                                                            'start' => '42'
                                                          }
                                                        ],
                                               'status' => 'T',
                                               'name' => 'G3DSA:1.10.8.30',
                                               'iprbool' => 0,
                                               'id' => 'G3DSA:1.10.8.30',
                                               'evd' => 'HMMPfam',
                                               'dbname' => 'Gene3D'
                                             },
                        'PS01127' => {
                                       'lcn' => [
                                                  {
                                                    'evalue' => '8.0E-5',
                                                    'end' => '129',
                                                    'start' => '119'
                                                  }
                                                ],
                                       'status' => 'T',
                                       'name' => 'EF_TS_2',
                                       'iprbool' => 1,
                                       'id' => 'PS01127',
                                       'evd' => 'AddProsite',
                                       'dbname' => 'ProfileScan'
                                     },
                        'PF00627' => {
                                       'lcn' => [
                                                  {
                                                    'evalue' => '4.60000044330866E-10',
                                                    'end' => '84',
                                                    'start' => '44'
                                                  }
                                                ],
                                       'status' => 'T',
                                       'name' => 'UBA',
                                       'iprbool' => 1,
                                       'id' => 'PF00627',
                                       'evd' => 'HMMPfam',
                                       'dbname' => 'HMMPfam'
                                     },
                        'SSF46934' => {
                                        'lcn' => [
                                                   {
                                                     'evalue' => '1.77E-10',
                                                     'end' => '99',
                                                     'start' => '45'
                                                   }
                                                 ],
                                        'status' => 'T',
                                        'name' => 'UBA_like',
                                        'iprbool' => 1,
                                        'id' => 'SSF46934',
                                        'evd' => 'HMMPfam',
                                        'dbname' => 'superfamily'
                                      },
                        'PF00889' => {
                                       'lcn' => [
                                                  {
                                                    'evalue' => '1.2999992446818301E-28',
                                                    'end' => '193',
                                                    'start' => '101'
                                                  }
                                                ],
                                       'status' => 'T',
                                       'name' => 'EF_TS',
                                       'iprbool' => 1,
                                       'id' => 'PF00889',
                                       'evd' => 'HMMPfam',
                                       'dbname' => 'HMMPfam'
                                     },
                        'PS01126' => {
                                       'lcn' => [
                                                  {
                                                    'evalue' => '8.0E-5',
                                                    'end' => '67',
                                                    'start' => '52'
                                                  }
                                                ],
                                       'status' => 'T',
                                       'name' => 'EF_TS_1',
                                       'iprbool' => 1,
                                       'id' => 'PS01126',
                                       'evd' => 'AddProsite',
                                       'dbname' => 'ProfileScan'
                                     },
                        'G3DSA:3.30.479.20' => {
                                                 'lcn' => [
                                                            {
                                                              'evalue' => '9.40001006865062E-10',
                                                              'end' => '192',
                                                              'start' => '101'
                                                            }
                                                          ],
                                                 'status' => 'T',
                                                 'name' => 'G3DSA:3.30.479.20',
                                                 'iprbool' => 0,
                                                 'id' => 'G3DSA:3.30.479.20',
                                                 'evd' => 'HMMPfam',
                                                 'dbname' => 'Gene3D'
                                               },
                        'PTHR11741' => {
                                         'lcn' => [
                                                    {
                                                      'evalue' => '1.50000965748784E-73',
                                                      'end' => '193',
                                                      'start' => '47'
                                                    }
                                                  ],
                                         'status' => 'T',
                                         'name' => 'Transl_elong_EFTs/EF1B',
                                         'iprbool' => 1,
                                         'id' => 'PTHR11741',
                                         'evd' => 'HMMPfam',
                                         'dbname' => 'HMMPanther'
                                       }
                      }
        };
	seq_id: unigene_mouse_3_ORF1
	ipr: 1
	($res, $go_entry) = $index->getEntry($$r_ipr{$protid}{$dbid}{id});
	($res, $go_entry) = $index->getEntry($$r_ipr{Q9CX33}{SSF54713}{id})
	($res, $go_entry) = $index->getEntry(IPR014039)
    
	calling getEntry

/common/iprscan/bin/iprscan_wrapper.plsub getEntry
	word: IPR014039
	posid: 
	insensitive: 
	recdel: 
	calling getIndex: ($res, $posid) = $self->getIndex(IPR014039, ); 

/common/iprscan/bin/iprscan_wrapper.plsub getIndex
	word: IPR014039
	insensitive: 
	file: 
	index file: /common/iprscan/data/interpro.xml.inx
	tie(my %index, 'DB_File', /common/iprscan/data/interpro.xml.inx, O_RDONLY, 0, $DB_BTREE)
    DBTREE:
    returning 1, $posid
end of getIndex

	RETURNED FROM getEntry (1, )
	RETURNED FROM getEntry (1, )
	return(0, "write_raw: $go_entry") unless $res
	return(0, "write_raw: ") unless 1


</entry>



<entry [Wed Jan 24 17:12:55 EST 2007] DEBUGGING iprscan2.pl>



cd /Users/local/FUNNYBASE/bin/unigene

./iprscan2.pl -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.14 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.14
>>>...

   <protein id="unigene_dog_3_ORF3" length="81" crc64="97C7E927F4E1F3E8" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="3" end="14" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>

</interpro_matches>
';
No. Interpros: 8
TSV file printed:

/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.14.all.1.tsv

Queryfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.14.all.2
Outfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.14.all.2
TSV file: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.14.all.2.tsv
No output file present... doing iprscan
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.14.all.2 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.14.all.2 -iprlookup -goterms
SUBMITTED iprscan-20070124-16315222
/common/iprscan/bin/iprscan: /common/iprscan/tmp/20070124/iprscan-20070124-16315222/iprscan-20070124-16315222.xml unavailable : No such file or directory

iprscan submission failed: checkSequences: Cannot get raw entry from iprmatches: getRawEntryFromIprMatches: query iprmatches failed: Can't use string ("") as an ARRAY ref while "strict refs" in use at /common/iprscan/lib/Index/IprMatches.pm line 421.

Opening outfile '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.14.all.2'
Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.14.all.2'
<<<

WHERE LINE 421 OF IprMatches.pm IS:

em /common/iprscan/lib/Index/IprMatches.pm

    $go_entry = $go_entry->[0]; #get_entry returns an array ref. A database member acc/id should be unique on all the interpro database.


AND THE *.errors FILE HAS THIS:

gems:/common/iprscan/tmp/20070124/iprscan-20070124-16315222 local$ ll
total 64
drwxrwxrwx   11 www  admin  374B Jan 24 16:32 .
drwxrwxrwx   34 www  admin    1K Jan 24 16:44 ..
drwxrwxrwx    3 www  admin  102B Jan 24 16:32 chunk_1
-rwxr-xr-x    1 www  admin    1K Jan 24 16:31 iprscan-20070124-16315222.dcmd
-rw-rw-rw-    1 www  admin    4B Jan 24 16:31 iprscan-20070124-16315222.djob
-rwxr-xr-x    1 www  admin  338B Jan 24 16:31 iprscan-20070124-16315222.dsub
-rw-r--r--    1 www  admin  254B Jan 24 16:32 iprscan-20070124-16315222.errors
-rw-r--r--    1 www  admin    3B Jan 24 16:32 iprscan-20070124-16315222.exitcode
-rw-rw-rw-    1 www  admin    1K Jan 24 16:31 iprscan-20070124-16315222.input
-rw-rw-rw-    1 www  admin  312B Jan 24 16:31 iprscan-20070124-16315222.params
-rw-rw-rw-    1 www  admin    1K Jan 24 16:31 iprscan-20070124-16315222.seqs
gems:/common/iprscan/tmp/20070124/iprscan-20070124-16315222 local$ em*errors

iprscan submission failed: checkSequences: Cannot get raw entry from iprmatches: getRawEntryFromIprMatches: query iprmatches failed: Can't use string ("") as an ARRAY ref while "strict refs" in use at /common/iprscan/lib/Index/IprMatches.pm line 421.<br>

FOUND THESE LINES IN iprscan_wrapper.pl (LINE 124):
# parse id, length and crc64 for each sequence and creates chunk if needed
($res, $seqs) = $tool->checkSequences($ifile, $checkcrc, $nfile, \@apps, $iprlookup, $goterms); #iprlookup and goterms were 1
handleError($tool, "$tname submission failed: $seqs", $efile) unless $res;

WHERE 
my $tool = new Dispatcher::Tool::InterProScan();

THE ORIGINAL SEQUENCE IS
em /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.14.all.2
>>>
>AK020437|S9072830|unigene_mouse
GGATGTCGCTTCTGCGGTCGCTGCGCTTCTTCCCGGTCGCGTGCACCGGCCGCTCGGCGA
GGGCTGTCCTGCTCCAGCCGTCCCAGCCATGGCTCACGTTTCACGCCGGCCCCTCGCTGT
...<<<

</entry>



<entry [Mon Jan 22 18:42:34 EST 2007] 3-FRAME TRANSLATION (+VE STRAND) ONLY>



ALTER bin/translate.pl

http://www.ebi.ac.uk/interpro/User-FAQ-Scan.html?&format=simple
>>>...
Yes. It will be translated and scanned in all 6 frames without any further assumptions except transcript length cut-off. You can specify the transcript length cut-off and/or codon translation table (based on CodonTable.pm by Heikki Lehvaslaiho). If you wish more sophisticated protein sequence predictions replace or modify bin/translate.pl script.
<<< )


Sixpack takes a nucleic acid sequence and writes out the forward and reverse senses of the sequence with the 3 forward and three reverse translations in a pretty display format.


http://sci.cnb.uam.es/Doc/Courses99/Software/ebi/unix/iprscan/FAQ/v4.0/FAQs.html
If you wish more sophisticated protein sequence predictions replace or modify conf/sixpack.sh script and the command line for the translation in conf/iprscan.conf file (See 'How to replace EMBOSS tools'). 

em conf/sixpack.sh
>>>
#!/bin/sh
# $Id: sixpack.sh,v 1.2 2005/10/26 08:57:16 hunter Exp $

##### NOTE ####
# Should be already set by iprscan_wrapper.pl.
# If not just add the full path to your iprscan installation
#IPRSCAN_HOME=/your/path/here/to/iprscan
###############
IPRSCAN_HOME=/common/iprscan

#Needed for seqret & sixpack emboss applications
EMBOSS_ROOT=$IPRSCAN_HOME/conf
export EMBOSS_ROOT

EMBOSS_ACDROOT=$IPRSCAN_HOME/conf/acd
export EMBOSS_ACDROOT

EMBOSS_DATA=$IPRSCAN_HOME/conf/emboss_data
export EMBOSS_DATA

# "$*" is related to the command line of translatecmd tag in file iprscan.conf.
# we remove the first two arguments to use with iterator.pl and make a tmpfile name out of the infile
# Using "cat $tmpfile" is a really ugly way to get around the fact that you can't just output the
# sequence to stdout with sixpack and that is what iterator needs to function

infile=$1
outfile=$2
tmpfile="$infile.nttmp"

# remove the first 2 arguments as we have now stored them
shift
shift

$IPRSCAN_HOME/bin/iterator.pl -i $infile -o $outfile -c "$IPRSCAN_HOME/bin/binaries/sixpack -filter -auto -stdout -osformat fasta -outfile /dev/null $* -outseq $tmpfile %infile; cat $tmpfile"

# remove the temporary file we created
rm $tmpfile
<<<


CHANGE THE COMMAND:

$IPRSCAN_HOME/bin/iterator.pl -i $infile -o $outfile -c "$IPRSCAN_HOME/bin/binaries/sixpack -filter -auto -stdout -osformat fasta -outfile /dev/null $* -outseq $tmpfile %infile; cat $tmpfile"

BY ADDING THE sixpack ARGUMENT -noreverse:

$IPRSCAN_HOME/bin/iterator.pl -i $infile -o $outfile -c "$IPRSCAN_HOME/bin/binaries/sixpack -noreverse -filter -auto -stdout -osformat fasta -outfile /dev/null $* -outseq $tmpfile %infile; cat $tmpfile"


</entry>



<entry [Wed Jan 10 21:51:02 EST 2007] INTERPROSCAN DETAILS>



1 Blastprodom (a wrapper script on top of Blast package) is used to search against PRODOM families (homologous domains using PSI-BLAST)

2 Gene3d Gene3D is supplementary to the CATH database [grouped by sequence, structural and/or functional similarity (homologous superfamilies, H-Level) or just structural similarity (fold or topology group, the T-level)]. This protein sequence database contains proteins from complete genomes which have been clustered into protein families and annotated with CATH domains, Pfam domains and functional information from KEGG, GO, COG, Affymetrix and STRINGS.

3 mmpanther (hmmpfam from HMMER 2.3.2 package) PANTHER classifies proteins into families and specific functional subfamilies, which are then mapped to ontology terms and pathways.

4 Hmmpir PIR produces the Protein  Sequence Database (PSD) of functionally annotated protein sequences, which grew out of the Atlas of Protein Sequence and Structure (1965-1978) edited by Margaret Dayhoff and has been incorporated into an integrated knowledge base system of value-added databases and analytical tools.

5 Hmmpfam (hmmpfam from HMMER 2.3.2 package) is used to search against the Pfam HMM (Profile hidden Markov models) database

6 Hmmsmart (hmmpfam from HMMER 2.3.2 package) is used to search against the Smart HMM (Profile hidden Markov models) database

7 Hmmtigr (hmmpfam from HMMER 2.3.2 package) is used to search against TIGRFAMs collection of HMMs (Profile hidden Markov models)

8 Fprintscan (FingerPRINTScan) is used to search against the PRINTS collection of protein signatures

9 Scanregexp (ScanProsite) is used to search against the PROSITE patterns collection and verify the  matches by statistically significant CONFIRM patterns

10 Profilescan (ProfileScan (pfscan & ps_scan.pl) from Pftools 2.2) package is used  to search against the PROSITE profiles collection

11 Superfamily A structural classification of proteins database for the investigation of sequences and structures.

12 Seg is used for identifying and masking segments of low compositional complexity in amino acid sequences

13 Signalp predicts the presence and location of signal peptide cleavage sites, using eukaryotic HMM (under commercial license : contact software@cbs.dtu.dk)

14 Tmhmm is used for prediction of transmembrane helices in proteins using a Hidden Markov Model (under commercial license : contact software@cbs.dtu.dk)


NB: InterProScan can use taxonomy-based filtering of results which uses the InterPro taxonomy listed for each InterPro



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Jan  8 12:50:54 EST 2007

Hi Chris,

In addition to the problem below, I now have repeated freezes of the head node, which can only be fixed by restarting manually. It happens when I run large numbers of individual jobs concurrently.

It may be a problem related to nfs as I also cannot mount the data directory from gems on my local machine:

dlc-genomics:~ young$ sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/Volumes/gemshd4/common /common
mount_nfs: realpath /private/var/autofs/common: Host is down

When I run array jobs, the jobs appear to get 'stuck' (see below for previous problem).

I've left the user/pswds the same so you're free to access the system to take a look.

Cheers,

Stuart.


</entry>



<entry [Last login: Sat Jan  6 15:27:59 on ttyp3] Welcome to Darwin!>


You have new mail.
dlc-genomics:~ young$ qstat -f
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       3.26     darwin        d
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   2/2       0.11     darwin        
   4350 0.55500 iprscan-20 www          r     01/06/2007 02:38:18     1        
   4352 0.55500 iprscan-20 www          r     01/06/2007 02:39:02     1        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   2/2       0.01     darwin        
   4349 0.55500 iprscan-20 www          r     01/06/2007 02:38:18     1        
   4353 0.55500 iprscan-20 www          r     01/06/2007 02:39:10     1        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   2/2       0.06     darwin        
   4327 0.55500 iprscan-20 www          r     01/06/2007 02:32:02     1        
   4328 0.55500 iprscan-20 www          r     01/06/2007 02:32:02     1        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   2/2       0.07     darwin        
   4341 0.55500 iprscan-20 www          r     01/06/2007 02:32:34     1        
   4342 0.55500 iprscan-20 www          r     01/06/2007 02:32:34     1        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   2/2       0.14     darwin        
   4332 0.55500 iprscan-20 www          r     01/06/2007 02:32:10     1        
   4344 0.55500 iprscan-20 www          r     01/06/2007 02:32:50     1        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   2/2       0.17     darwin        
   4343 0.55500 iprscan-20 www          r     01/06/2007 02:32:42     1        
   4348 0.55500 iprscan-20 www          r     01/06/2007 02:34:58     1        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   2/2       0.12     darwin        
   4326 0.55500 iprscan-20 www          r     01/06/2007 02:32:02     1        
   4351 0.55500 iprscan-20 www          r     01/06/2007 02:38:22     1        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   2/2       0.05     darwin        
   4337 0.55500 iprscan-20 www          r     01/06/2007 02:32:14     1        
   4345 0.55500 iprscan-20 www          r     01/06/2007 02:32:50     1        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   2/2       0.09     darwin        
   4340 0.55500 iprscan-20 www          r     01/06/2007 02:32:30     1        
   4347 0.55500 iprscan-20 www          r     01/06/2007 02:34:50     1        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   2/2       0.10     darwin        
   4329 0.55500 iprscan-20 www          r     01/06/2007 02:32:06     1        
   4330 0.55500 iprscan-20 www          r     01/06/2007 02:32:06     1        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   0/2       0.02     darwin        d
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   2/2       0.12     darwin        
   4338 0.55500 iprscan-20 www          r     01/06/2007 02:32:22     1        
   4346 0.55500 iprscan-20 www          r     01/06/2007 02:32:58     1        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   2/2       0.07     darwin        
   4331 0.55500 iprscan-20 www          r     01/06/2007 02:32:10     1        
   4336 0.55500 iprscan-20 www          r     01/06/2007 02:32:14     1        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   2/2       0.18     darwin        
   4335 0.55500 iprscan-20 www          r     01/06/2007 02:32:14     1        
   4339 0.55500 iprscan-20 www          r     01/06/2007 02:32:26     1        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.02     darwin        d

############################################################################
 - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS
############################################################################
   4354 0.55500 iprscan-20 www          qw    01/06/2007 02:32:04     1        
   4355 0.55500 iprscan-20 www          qw    01/06/2007 02:32:05     1        
   4356 0.55500 iprscan-20 www          qw    01/06/2007 02:32:05     1        
   4357 0.55500 iprscan-20 www          qw    01/06/2007 02:32:05     1        
   4358 0.55500 iprscan-20 www          qw    01/06/2007 02:32:06     1        
   4359 0.55500 iprscan-20 www          qw    01/06/2007 02:32:06     1        
   4360 0.55500 iprscan-20 www          qw    01/06/2007 02:32:06     1        
   4361 0.55500 iprscan-20 www          qw    01/06/2007 02:32:06     1        
   4362 0.55500 iprscan-20 www          qw    01/06/2007 02:32:06     1        
   4363 0.55500 iprscan-20 www          qw    01/06/2007 02:32:06     1        
   4364 0.55500 iprscan-20 www          qw    01/06/2007 02:32:08     1        
   4365 0.55500 iprscan-20 www          qw    01/06/2007 02:32:08     1        
   4366 0.55500 iprscan-20 www          qw    01/06/2007 02:32:08     1        
   4367 0.55500 iprscan-20 www          qw    01/06/2007 02:32:08     1        
   4368 0.55500 iprscan-20 www          qw    01/06/2007 02:32:12     1        
   4369 0.55500 iprscan-20 www          qw    01/06/2007 02:32:12     1        
   4370 0.55500 iprscan-20 www          qw    01/06/2007 02:32:12     1        
   4371 0.55500 iprscan-20 www          qw    01/06/2007 02:32:12     1        
   4372 0.55500 iprscan-20 www          qw    01/06/2007 02:32:12     1        
   4373 0.55500 iprscan-20 www          qw    01/06/2007 02:32:13     1        
   4374 0.55500 iprscan-20 www          qw    01/06/2007 02:32:13     1        
   4375 0.55500 iprscan-20 www          qw    01/06/2007 02:32:13     1        
   4376 0.55500 iprscan-20 www          qw    01/06/2007 02:32:14     1        
   4377 0.55500 iprscan-20 www          qw    01/06/2007 02:32:14     1        
   4378 0.55500 iprscan-20 www          qw    01/06/2007 02:32:16     1        
   4379 0.55500 iprscan-20 www          qw    01/06/2007 02:32:16     1        
   4380 0.55500 iprscan-20 www          qw    01/06/2007 02:32:16     1        
   4381 0.55500 iprscan-20 www          qw    01/06/2007 02:32:16     1        
   4382 0.55500 iprscan-20 www          qw    01/06/2007 02:32:16     1        
   4383 0.55500 iprscan-20 www          qw    01/06/2007 02:32:16     1        
   4384 0.55500 iprscan-20 www          qw    01/06/2007 02:32:16     1        
   4385 0.55500 iprscan-20 www          qw    01/06/2007 02:32:16     1        
   4386 0.55500 iprscan-20 www          qw    01/06/2007 02:32:17     1        
   4387 0.55500 iprscan-20 www          qw    01/06/2007 02:32:17     1        
   4388 0.55500 iprscan-20 www          qw    01/06/2007 02:32:17     1        
   4389 0.55500 iprscan-20 www          qw    01/06/2007 02:32:17     1        
   4390 0.55500 iprscan-20 www          qw    01/06/2007 02:32:19     1        
   4391 0.55500 iprscan-20 www          qw    01/06/2007 02:32:24     1        
   4392 0.55500 iprscan-20 www          qw    01/06/2007 02:32:24     1        
   4393 0.55500 iprscan-20 www          qw    01/06/2007 02:32:25     1        
   4394 0.55500 iprscan-20 www          qw    01/06/2007 02:32:27     1        
   4395 0.55500 iprscan-20 www          qw    01/06/2007 02:32:27     1        
   4396 0.55500 iprscan-20 www          qw    01/06/2007 02:32:29     1        
   4397 0.55500 iprscan-20 www          qw    01/06/2007 02:32:29     1        
   4398 0.55500 iprscan-20 www          qw    01/06/2007 02:32:29     1        
   4399 0.55500 iprscan-20 www          qw    01/06/2007 02:32:30     1        
   4400 0.55500 iprscan-20 www          qw    01/06/2007 02:32:30     1        
   4401 0.55500 iprscan-20 www          qw    01/06/2007 02:32:30     1        
   4402 0.55500 iprscan-20 www          qw    01/06/2007 02:32:32     1        
   4403 0.55500 iprscan-20 www          qw    01/06/2007 02:32:32     1        
   4404 0.55500 iprscan-20 www          qw    01/06/2007 02:32:33     1        
   4405 0.55500 iprscan-20 www          qw    01/06/2007 02:32:33     1        
   4406 0.55500 iprscan-20 www          qw    01/06/2007 02:32:36     1        
   4407 0.55500 iprscan-20 www          qw    01/06/2007 02:32:37     1        
   4408 0.55500 iprscan-20 www          qw    01/06/2007 02:32:37     1        
   4409 0.55500 iprscan-20 www          qw    01/06/2007 02:32:37     1        
   4410 0.55500 iprscan-20 www          qw    01/06/2007 02:32:37     1        
   4411 0.55500 iprscan-20 www          qw    01/06/2007 02:32:37     1        
   4412 0.55500 iprscan-20 www          qw    01/06/2007 02:32:37     1        
   4413 0.55500 iprscan-20 www          qw    01/06/2007 02:32:39     1        
   4414 0.55500 iprscan-20 www          qw    01/06/2007 02:32:44     1        
   4415 0.55500 iprscan-20 www          qw    01/06/2007 02:32:44     1        
   4416 0.55500 iprscan-20 www          qw    01/06/2007 02:32:46     1        
   4417 0.55500 iprscan-20 www          qw    01/06/2007 02:32:46     1        
   4418 0.55500 iprscan-20 www          qw    01/06/2007 02:32:47     1        
   4419 0.55500 iprscan-20 www          qw    01/06/2007 02:32:48     1        
   4420 0.55500 iprscan-20 www          qw    01/06/2007 02:32:52     1        
   4421 0.55500 iprscan-20 www          qw    01/06/2007 02:32:52     1        
   4422 0.55500 iprscan-20 www          qw    01/06/2007 02:32:53     1        
   4423 0.55500 iprscan-20 www          qw    01/06/2007 02:32:53     1        
   4424 0.55500 iprscan-20 www          qw    01/06/2007 02:32:54     1        
   4425 0.55500 iprscan-20 www          qw    01/06/2007 02:32:54     1        
   4426 0.55500 iprscan-20 www          qw    01/06/2007 02:32:54     1        
   4427 0.55500 iprscan-20 www          qw    01/06/2007 02:32:54     1        
   4428 0.55500 iprscan-20 www          qw    01/06/2007 02:32:59     1        
   4429 0.55500 iprscan-20 www          qw    01/06/2007 02:32:59     1        
   4430 0.55500 iprscan-20 www          qw    01/06/2007 02:33:01     1        
   4431 0.55500 iprscan-20 www          qw    01/06/2007 02:33:01     1        
   4432 0.55500 iprscan-20 www          qw    01/06/2007 02:33:01     1        
   4433 0.55500 iprscan-20 www          qw    01/06/2007 02:33:01     1        
   4434 0.55500 iprscan-20 www          qw    01/06/2007 02:33:02     1        
   4435 0.55500 iprscan-20 www          qw    01/06/2007 02:33:02     1        
   4436 0.55500 iprscan-20 www          qw    01/06/2007 02:33:03     1        
   4437 0.55500 iprscan-20 www          qw    01/06/2007 02:33:06     1        
   4438 0.55500 iprscan-20 www          qw    01/06/2007 02:33:12     1        
   4439 0.55500 iprscan-20 www          qw    01/06/2007 02:33:13     1        
   4440 0.55500 iprscan-20 www          qw    01/06/2007 02:33:15     1        
   4441 0.55500 iprscan-20 www          qw    01/06/2007 02:33:16     1        
   4442 0.55500 iprscan-20 www          qw    01/06/2007 02:33:23     1        
   4443 0.55500 iprscan-20 www          qw    01/06/2007 02:33:23     1        
   4444 0.55500 iprscan-20 www          qw    01/06/2007 02:33:24     1        
   4445 0.55500 iprscan-20 www          qw    01/06/2007 02:33:26     1        
   4446 0.55500 iprscan-20 www          qw    01/06/2007 02:33:30     1        
   4447 0.55500 iprscan-20 www          qw    01/06/2007 02:33:35     1        
   4448 0.55500 iprscan-20 www          qw    01/06/2007 02:33:40     1        
   4449 0.55500 iprscan-20 www          qw    01/06/2007 02:33:43     1        
   4450 0.55500 iprscan-20 www          qw    01/06/2007 02:33:45     1        
   4451 0.55500 iprscan-20 www          qw    01/06/2007 02:33:59     1        
   4452 0.55500 iprscan-20 www          qw    01/06/2007 02:34:00     1        
   4453 0.55500 iprscan-20 www          qw    01/06/2007 02:34:02     1        
   4454 0.55500 iprscan-20 www          qw    01/06/2007 02:34:02     1        
   4455 0.55500 iprscan-20 www          qw    01/06/2007 02:34:06     1        
   4456 0.55500 iprscan-20 www          qw    01/06/2007 02:34:52     1        
   4457 0.55500 iprscan-20 www          qw    01/06/2007 02:34:52     1        
   4458 0.55500 iprscan-20 www          qw    01/06/2007 02:34:52     1        
   4459 0.55500 iprscan-20 www          qw    01/06/2007 02:35:02     1        
   4460 0.55500 iprscan-20 www          qw    01/06/2007 02:35:02     1        
   4461 0.55500 iprscan-20 www          qw    01/06/2007 02:35:02     1        
   4462 0.55500 iprscan-20 www          qw    01/06/2007 02:35:02     1        
   4463 0.55500 iprscan-20 www          qw    01/06/2007 02:35:02     1        
   4464 0.55500 iprscan-20 www          qw    01/06/2007 02:35:03     1        
   4465 0.55500 iprscan-20 www          qw    01/06/2007 02:35:03     1        
   4466 0.55500 iprscan-20 www          qw    01/06/2007 02:35:03     1        
   4467 0.55500 iprscan-20 www          qw    01/06/2007 02:36:01     1        
   4468 0.55500 iprscan-20 www          qw    01/06/2007 02:36:01     1        
   4469 0.55500 iprscan-20 www          qw    01/06/2007 02:36:04     1        
   4470 0.55500 iprscan-20 www          qw    01/06/2007 02:36:06     1        
   4471 0.55500 iprscan-20 www          qw    01/06/2007 02:36:07     1        
   4472 0.55500 iprscan-20 www          qw    01/06/2007 02:36:09     1        
   4473 0.55500 iprscan-20 www          qw    01/06/2007 02:37:15     1        
   4474 0.55500 iprscan-20 www          qw    01/06/2007 02:37:57     1        
   4475 0.55500 iprscan-20 www          qw    01/06/2007 02:37:57     1        
   4476 0.55500 iprscan-20 www          qw    01/06/2007 02:38:00     1        
   4477 0.55500 iprscan-20 www          qw    01/06/2007 02:38:21     1        
   4478 0.55500 iprscan-20 www          qw    01/06/2007 02:38:21     1        
   4479 0.55500 iprscan-20 www          qw    01/06/2007 02:38:21     1        
   4480 0.55500 iprscan-20 www          qw    01/06/2007 02:38:21     1        
   4481 0.55500 iprscan-20 www          qw    01/06/2007 02:38:24     1        
   4482 0.55500 iprscan-20 www          qw    01/06/2007 02:38:24     1        
   4483 0.55500 iprscan-20 www          qw    01/06/2007 02:38:41     1        
   4484 0.55500 iprscan-20 www          qw    01/06/2007 02:38:52     1        
   4485 0.55500 iprscan-20 www          qw    01/06/2007 02:38:52     1        
   4486 0.55500 iprscan-20 www          qw    01/06/2007 02:39:04     1        
   4487 0.55500 iprscan-20 www          qw    01/06/2007 02:39:05     1        
   4488 0.55500 iprscan-20 www          qw    01/06/2007 02:39:05     1        
   4489 0.55500 iprscan-20 www          qw    01/06/2007 02:39:12     1        
   4490 0.55500 iprscan-20 www          qw    01/06/2007 02:39:12     1        
   4491 0.55500 iprscan-20 www          qw    01/06/2007 02:39:20     1        
   4492 0.55500 iprscan-20 www          qw    01/06/2007 02:39:21     1        
   4493 0.55500 iprscan-20 www          qw    01/06/2007 02:39:33     1        
   4494 0.55500 iprscan-20 www          qw    01/06/2007 02:39:36     1        
   4495 0.55500 iprscan-20 www          qw    01/06/2007 02:39:40     1        
   4496 0.55500 iprscan-20 www          qw    01/06/2007 02:39:41     1        
   4497 0.55500 iprscan-20 www          qw    01/06/2007 02:39:41     1        
   4498 0.55500 iprscan-20 www          qw    01/06/2007 02:39:42     1        
   4499 0.55500 iprscan-20 www          qw    01/06/2007 02:39:45     1        
   4500 0.55500 iprscan-20 www          qw    01/06/2007 02:39:47     1        
   4501 0.55500 iprscan-20 www          qw    01/06/2007 02:39:48     1        
   4502 0.55500 iprscan-20 www          qw    01/06/2007 02:39:53     1        
   4503 0.55500 iprscan-20 www          qw    01/06/2007 02:39:53     1        
   4504 0.55500 iprscan-20 www          qw    01/06/2007 02:42:11     1        
   4505 0.55500 iprscan-20 www          qw    01/06/2007 02:42:11     1        
   4506 0.55500 iprscan-20 www          qw    01/06/2007 02:42:20     1        
   4507 0.55500 iprscan-20 www          qw    01/06/2007 02:43:29     1        
   4508 0.55500 iprscan-20 www          qw    01/06/2007 02:43:29     1        
   4509 0.55500 iprscan-20 www          qw    01/06/2007 02:44:24     1        
   4510 0.55500 iprscan-20 www          qw    01/06/2007 02:45:31     1        
   4511 0.55500 iprscan-20 www          qw    01/06/2007 02:45:37     1        
   4512 0.55500 iprscan-20 www          qw    01/06/2007 02:45:44     1        
   4513 0.55500 iprscan-20 www          qw    01/06/2007 02:45:47     1        
   4514 0.55500 iprscan-20 www          qw    01/06/2007 02:46:56     1        
   4515 0.55500 iprscan-20 www          qw    01/06/2007 02:48:54     1        
   4516 0.55500 iprscan-20 www          qw    01/06/2007 02:53:41     1        
   4517 0.55500 iprscan-20 www          qw    01/06/2007 03:02:56     1        
   4518 0.55500 iprscan-20 www          qw    01/06/2007 03:08:31     1        
   4519 0.55500 iprscan-20 www          qw    01/06/2007 03:11:30     1        
   4520 0.55500 iprscan-20 www          qw    01/06/2007 03:11:33     1        
   4521 0.55500 iprscan-20 www          qw    01/06/2007 03:11:37     1        
   4522 0.55500 iprscan-20 www          qw    01/06/2007 03:12:46     1

</entry>



<entry [Sat Jan  6 15:35:05 PST 2007] CHANGE THE MAXIMUM NUMBER OF PENDING JOBS>



qconf -mconf max_jobs 200
qconf max_jobs 200

man sge_conf
>>>...
 sge_conf  defines  the  global and local Grid Engine configurations and
       can be shown/modified by qconf(1) using the -sconf/-mconf options. Only
       root or the cluster administrator may modify sge_conf.

       At  its  initial start-up, sge_qmaster(8) checks to see if a valid Grid
       Engine configuration is available at a well known location in the  Grid
       Engine  internal  directory hierarchy.  If so, it loads that configura-
       tion information and proceeds.  If not, sge_qmaster(8) writes a generic
       configuration  containing  default  values  to that same location.  The
       Grid Engine execution daemons sge_execd(8) upon start-up retrieve their
       configuration from sge_qmaster(8).

       The  actual configuration for both sge_qmaster(8) and sge_execd(8) is a
       superposition of a so called global configuration and a local  configu-
       ration being pertinent for the host on which a master or execution dae-
       mon resides.  If a local configuration is available, its entries  over-
       write  the corresponding entries of the global configuration. Note: The
       local configuration does not have to contain  all  valid  configuration
       entries,  but  only  those which need to be modified against the global
       entries.
...<<<

man qconf
>>>...
       -mconf [host,...|global]      <modify configuration>
              The configuration for the specified host is retrieved, an editor
              is executed (either vi(1) or the editor  indicated  by  $EDITOR)
              and  the changed configuration is registered with sge_qmaster(8)
              upon exit of the editor.  If the optional host argument is omit-
              ted  or if the special host name "global" is specified, the cell
              global configuration is modified.  The  format of the host  con-
              figuration is described in sge_conf(5).
              Requires root or manager privilege.
       -msconf                       <modify scheduler configuration>
              The  current  scheduler  configuration  (see  sched_conf(5))  is
              retrieved,  an  editor  is  executed (either vi(1) or the editor
              indicated by $EDITOR) and the changed  configuration  is  regis-
              tered  with  sge_qmaster(8)  upon  exit of the editor.  Requires
              root or manager privilege.
...
max_jobs
     The number of  active  (not  finished)  jobs  simultaneously
     allowed  in Grid Engine.  is controlled by this parameter. A
     value greater than 0 defines the limit. The default value  0
     means  "unlimited".  If  the max_jobs limit is exceeded by a
     job submission then the submission command exits  with  exit
     status 25 and an appropriate error message.

     Changing max_jobs will take immediate effect.

     This value is a global configuration parameter only. It can-
     not  be  overwritten  by the execution host local configura-
     tion.



...<<<

</entry>



<entry [Fri Jan  5 16:30:20 PST 2007] USE sge_commd TO SHOW MORE ERROR INFO>



em /common/sge/default/spool/node001/messages

AND OTHER NODE MESSAGES:

sge_commd -ll loginfo

- log_err       All error events being recognized are logged. 
- log_warning   All error events being recognized and all detected signs of potentially erroneous behavior are logged. 
- log_info      All error events being recognized, all detected signs of potentially erroneous behavior and a variety of informative messages are logged. 


</entry>



<entry [Thu Jan  4 22:42:53 PST 2007] BATCH RUN OF hmmpanther GAVE THIS ERROR IN THE orthologues.iprscan.sh.o1989.9 FILE:>



gems:~/FUNNYBASE/bin/unigene local$ em *1989.9

>>>...
[Thu Jan  4 17:52:24 2007] iprscan: Can't load '/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle' for module X\
ML::Parser::Expat: dlopen(/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle, 1): Library not loaded: /usr/local\
/lib/libexpat.1.dylib
[Thu Jan  4 17:52:24 2007] iprscan:   Referenced from: /RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
[Thu Jan  4 17:52:24 2007] iprscan:   Reason: image not found at /System/Library/Perl/5.8.6/darwin-thread-multi-2level/DynaLoader.pm line 230.
[Thu Jan  4 17:52:24 2007] iprscan:  at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14
[Thu Jan  4 17:52:24 2007] iprscan: Compilation failed in require at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14.
[Thu Jan  4 17:52:24 2007] iprscan: BEGIN failed--compilation aborted at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 18.
[Thu Jan  4 17:52:24 2007] iprscan: Compilation failed in require at /common/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
[Thu Jan  4 17:52:24 2007] iprscan: BEGIN failed--compilation aborted at /common/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
[Thu Jan  4 17:52:24 2007] iprscan: Compilation failed in require at /common/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
[Thu Jan  4 17:52:24 2007] iprscan: BEGIN failed--compilation aborted at /common/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
[Thu Jan  4 17:52:24 2007] iprscan: Compilation failed in require at /common/iprscan/bin/iprscan line 56.
[Thu Jan  4 17:52:24 2007] iprscan: BEGIN failed--compilation aborted at /common/iprscan/bin/iprscan line 56.
Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.9.hmmpanther.0'
Opening outfile '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.9.hmmpanther.0'
<<<

NB: OTHER JOB IDS DID NOT FINISH, E.G.:

gems:~/FUNNYBASE/bin/unigene local$ em *1989.9
>>>
Queryfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.5.hmmpanther.0
Outfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.5.hmmpanther.0
TSV file: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.5.hmmpanther.0.tsv
No output file present... doing iprscan
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.5.hmmpanther.0 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.5.hmmpanther.0 -iprlookup -goterms -appl hmmpanther
SUBMITTED iprscan-19700628-22072942
<<<

FOUND libexpat.1.5.0.dylib IN /Volumes/gemshd4/common/lib/lib:

ls /Volumes/gemshd4/common/lib/lib
libexpat.1.5.0.dylib    libexpat.a              libexpat.la
libexpat.1.dylib        libexpat.dylib

THOUGH IT WAS ALSO PRESENT IN THESE DIRECTORIES:
/common.bak/lib/lib
/common.bak/lib
/opt/apache2/lib
/Users.bak/local/FUNNYBASE/NOTES/plmods/XML-Parser/expat-2.0.0/.libs
/usr/local/lib/lib
/Volumes/gemshd4/common/lib/lib
/Volumes/gemshd4/Users/local/FUNNYBASE/NOTES/plmods/XML-Parser/expat-2.0.0/.libs


/Volumes/gemshd4/common/lib

SEARCH OF 'DYLD_LIBRARY_PATH expat dylib perl':

SET ENVIRONMENT VARIABLE ON COMMAND LINE:

DYLD_LIBRARY_PATH=/opt/expat/lib:$DYLD_LIBRARY_PATH
export DYLD_LIBRARY_PATH

(Or maybe you need to set DYLD_FALLBACK_LIBRARY_PATH)

OR WITH SCRIPT:
=============================
!/bin/sh

CURDIR=`dirname $0`
if [ "$CURDIR" != "`pwd`" ]; then
    cd $CURDIR
fi
export DYLD_LIBRARY_PATH="`pwd`/Libraries:${DYLD_LIBRARY_PATH}";
=============================

SO TRIED SETTING IT USING THE %ENV VARIABLE IN PERL:

sudo em testenv.pl
=============================
#!/usr/bin/perl -w

my $variable = "/Volumes/gemshd4/common/lib";
$ENV{DYLD_LIBRARY_PATH} = $variable;
print `echo \$DYLD_LIBRARY_PATH`;
=============================

WHICH WORKED OKAY:

gems:~/FUNNYBASE/bin/unigene local$ ./testenv.pl 
/Volumes/gemshd4/common/lib
gems:~/FUNNYBASE/bin/unigene local$ 

SO ADDED THESE LINES TO /Library/Perl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm ON LINE 15:

my $variable = "/Volumes/gemshd4/common/lib";
$ENV{DYLD_LIBRARY_PATH} = $variable;

(BEFORE require XML::Parser::Expat;)

FIRST DELETED ALL JOBS FROM QUEUE (AS USER www):
qdel  -u "*" 

THEN TRIED AGAIN TO RUN hmmpanther WITH orthologuesiprscan.pl:

./orthologuesiprscan.pl -d orthologues -t 1 -p hmmpanther -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out

BUT IT GOT STUCK:

gems:/Users/local/FUNNYBASE/bin/unigene www$ cat *2050.1
>>>...
Queryfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1.hmmpanther.0
Outfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.hmmpanther.0
TSV file: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.hmmpanther.0.tsv
No output file present... doing iprscan
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1.hmmpanther.0 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.hmmpanther.0 -iprlookup -goterms -appl hmmpanther
SUBMITTED iprscan-20070104-20260537
<<<

SO TRIED RUNNING ortholguesiprscan.pl WITH coils:

    ./orthologuesiprscan.pl -d orthologues -t 1 -p coils -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out

    
WHICH GAVE THE SAME UNINFORMATIVE OUTPUT, SO TRIED VARIOUS IPRSCAN APPS WITH iprscan2.pl:

 ./iprscan2.pl -p <APPLICATION> -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1
 
            Hits Run time 
fprintscan  no  00:02:49
profilescan no  00:13:12
blastprodom no  00:00:56    Could be cached
hmmsmart    no  
hmmpanther  yes 00:01:53    Could be cached
hmmpfam     yes 00:33:09    Why so long? 
hmmtigr     no
scanregexp  no
coils       no
seg         yes   
hmmpir      no  00:07:05
superfamily yes 00:33:31
gene3d      no  00:08:52

Program Speed(s) (v4.3)
======================
HMMPfam	70
HMMPanther	12
HMMPIR	22
blastprodom	18
coils	7
gene3d	28
HMMSmart	8
HMMTigr	23
FPRINTScan	12
scanregexp	8
profilescan	17
superfamily	151
seg	8
signalp	7
tmhmm	7

RUNNING blastprodom GIVES THIS ERROR:

>>>...
Queryfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1.blastprodom.1
Outfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.blastprodom.1
TSV file: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.blastprodom.1.tsv
No output file present... doing iprscan
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1.blastprodom.1 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.blastprodom.1 -iprlookup -goterms -appl blastprodom
SUBMITTED iprscan-20070105-01173721
/common/iprscan/bin/iprscan: /common/iprscan/tmp/20070105/iprscan-20070105-01173721/iprscan-20070105-01173721.xml unavailable : No such file or directory
supervise: doRawResults: error running system command /common/iprscan/bin/converter.pl -format xml -input /common/iprscan/tmp/20070105/iprscan-20070105-01173721/chunk_2/merged.raw -jobid iprscan-20070105-01173721: <br><br><br>To help us solve the problem, please copy and paste this error<br>and email it to the administrator <a href="mailto:nobody@localhost.com?subject=Error in InterProScan job iprscan-20070105-01173721">nobody@localhost.com</a>.<br><br>Sorry about the inconvenience and thank you for using InterProScan.<br><br>Opening outfile '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.blastprodom.1'
Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.blastprodom.1'
<<<

SO I DISABLED node012 AND node016 (WHICH WERE 'DIFFERENT' FROM THE OTHERS):

qmod -d all.q@node012
qmod -d all.q@node016

AND WAS ABLE TO RUN blastprodom WITH iprscan2.pl.

THEN TRIED RUNNING hmmpanther WITH orthologuesiprscan.pl AGAIN TO SEE IF DISABLING node012 AND node016 SOLVED THE PROBLEM WITH orthologuesiprscan.pl:

    ./orthologuesiprscan.pl -d orthologues -t 2 -p hmmpanther -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out

BUT IT GOT STUCK AGAIN:

gems:~/FUNNYBASE/bin/unigene local$ em *2351.2

>>>...
No output file present... doing iprscan
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.2.hmmpanther.0 -o /Users/l\
ocal/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.2.hmmpanther.0 -iprlookup -goterms -appl hmmpanther
SUBMITTED iprscan-20070104-22410112
<<<

TRIED TO RUN superfamily WITH orthologuesiprscan.pl BUT IT GOT STUCK:

gems:/Users/local/FUNNYBASE/bin/unigene www$ em *2359.1
>>>...
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1.superfamily.0 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.superfamily.0 -iprlookup -goterms -appl superfamily
SUBMITTED iprscan-20070105-07091403
<<<

RAN superfamily WITH iprscan2.pl:

 ./iprscan2.pl -p superfamily -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1


SO TRIED RUNNING funnybasearrayblast.pl, WHICH GAVE THIS ERROR:

gems:~/FUNNYBASE/pipeline/funnybase9/blast local$ em funnybase9-refseq-dog.sh.o2395.1

>>>
usage: join [-a fileno | -v fileno ] [-e string] [-1 field] [-2 field]
            [-o list] [-t char] file1 file2
[NULL_Caption] WARNING: 0.1: Unable to open refseq-dog.pin
[NULL_Caption] WARNING: 0.1: Unable to open refseq-dog.pin
[NULL_Caption] FATAL ERROR: 0.1: Database /common/data/refseq-dog was not found or does not exist
<<<

SO THE ERROR HAS TO DO WITH ARRAY JOBS NOT FINDING THE /common/data DIRECTORY?


</entry>



<entry [Thu Jan  4 16:14:23 PST 2007] ERROR RUNNING hmmpanther WITH orthologuesiprscan.pl>



cd /Users/local/FUNNYBASE/bin/unigene
em *1987.9

usage: join [-a fileno | -v fileno ] [-e string] [-1 field] [-2 field]
            [-o list] [-t char] file1 file2
Fields: db,id,name,interproid,interproname,interprotype,childlist,foundinlist,golist,targetsource,targetid,targetname,hitscoredec,hitscoreexp,hitstatus,hitnumber,query\
start,querystop,divtag,svg
Queryfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.9.hmmppanther.0
Outfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.9.hmmppanther.0
TSV file: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.9.hmmppanther.0.tsv
No output file present... doing iprscan
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.9.hmmppanther.0 -o /Users/local/FUNNYBASE/pipeline/o\
rthologues/fasta/orthologues.out.9.hmmppanther.0 -iprlookup -goterms -appl hmmppanther
[Thu Jan  4 16:08:23 2007] iprscan: Can't locate XML/Parser.pm in @INC (@INC contains: /common/iprscan/lib /System/Library/Perl/5.8.6/darwin-thread-multi-2level /Syste\
m/Library/Perl/5.8.6 /Library/Perl/5.8.6/darwin-thread-multi-2level /Library/Perl/5.8.6 /Library/Perl /Network/Library/Perl/5.8.6/darwin-thread-multi-2level /Network/L\
ibrary/Perl/5.8.6 /Network/Library/Perl /System/Library/Perl/Extras/5.8.6/darwin-thread-multi-2level /System/Library/Perl/Extras/5.8.6 /Library/Perl/5.8.1 .) at /commo\
n/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 34.
[Thu Jan  4 16:08:23 2007] iprscan: BEGIN failed--compilation aborted at /common/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 34.
[Thu Jan  4 16:08:23 2007] iprscan: Compilation failed in require at /common/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
[Thu Jan  4 16:08:23 2007] iprscan: BEGIN failed--compilation aborted at /common/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
[Thu Jan  4 16:08:23 2007] iprscan: Compilation failed in require at /common/iprscan/bin/iprscan line 56.
[Thu Jan  4 16:08:23 2007] iprscan: BEGIN failed--compilation aborted at /common/iprscan/bin/iprscan line 56.
Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.9.hmmppanther.0'
Opening outfile '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.9.hmmppanther.0'


FOUND THAT REMOTE PERL IS NOT INCLUDED IN @INC ON node015:

node015:/RemotePerl/5.8.6 vanwye$ perl -V
>>>...
  Compiled at Aug 22 2005 06:12:36
  %ENV:
    PERL5LIB="/RemotePerl"
  @INC:
    /RemotePerl/5.8.6/darwin-thread-multi-2level
    /RemotePerl/5.8.6
    /RemotePerl
    /System/Library/Perl/5.8.6/darwin-thread-multi-2level
    /System/Library/Perl/5.8.6
    /Library/Perl/5.8.6/darwin-thread-multi-2level
    /Library/Perl/5.8.6
    /Library/Perl
    /Network/Library/Perl/5.8.6/darwin-thread-multi-2level
    /Network/Library/Perl/5.8.6
    /Network/Library/Perl
    /System/Library/Perl/Extras/5.8.6/darwin-thread-multi-2level
    /System/Library/Perl/Extras/5.8.6
    /Library/Perl/5.8.1
    .
<<<


BUT XML/Parser.pm IS ALSO NOT FOUND ON gems IN /Library/Perl/5.8.6/darwin-thread-multi-2level :

/Library/Perl/5.8.6/darwin-thread-multi-2level local$ ls
Apache      BSD         Bundle      DBD         DBI.pm      GD          MD5.pm      Mysql.pm    Params      Roadmap.pod Win32       auto
Attribute   Bio         Crypt       DBI         Digest      GD.pm       Mysql       Net         PerlIO      System      XML         qd.pl
gems:/Library/Perl/5.8.6/darwin-thread-multi-2level local$ cd XML/
gems:/Library/Perl/5.8.6/darwin-thread-multi-2level/XML local$ ls
Parser    Parser.pm Quote.pm
gems:/Library/Perl/5.8.6/darwin-thread-multi-2level/XML local$ pwd
/Library/Perl/5.8.6/darwin-thread-multi-2level/XML
gems:/Library/Perl/5.8.6/darwin-thread-multi-2level/XML local$ 


AND XML/Parser.pm IS NOT FOUND ON gems IN /RemotePerl (WHICH MAPS TO /Library/Perl ON gems), ALTHOUGH THE DIRECTORY Parser IS FOUND:

node015:/RemotePerl/5.8.6/darwin-thread-multi-2level vanwye$  ls
Apache      BSD         Bundle      DBD         DBI.pm      GD          MD5.pm      Mysql.pm    Params      Roadmap.pod Win32       auto
Attribute   Bio         Crypt       DBI         Digest      GD.pm       Mysql       Net         PerlIO      System      XML         qd.pl
node015:/RemotePerl/5.8.6/darwin-thread-multi-2level vanwye$ ls XML/
Parser    Parser.pm Quote.pm


SO ADDED 'use lib "/RemotePerl";' ABOVE LINE 34 IN

/common/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 34.

AND THE ERROR WAS FIXED!


</entry>



<entry [Thu Jan  4 14:35:35 PST 2007] CREATED sequenceview.html/sequenceview.cgi TO VIEW START AND STOP CODONS IN ANY>


NUCLEOTIDE SEQUENCE:

START:
AUG

STOP:
UAG is amber
UGA is opal (umber)
UAA is ochre

Codon table:

UUU (Phe/F)Phenylalanine
UUC (Phe/F)Phenylalanine
UUA (Leu/L)Leucine
UUG (Leu/L)Leucine
UCU (Ser/S)Serine
UCC (Ser/S)Serine
UCA (Ser/S)Serine
UCG (Ser/S)Serine
UAU (Tyr/Y)Tyrosine
UAC (Tyr/Y)Tyrosine
		UAA Ochre (Stop)
		UAG Amber (Stop)
UGU (Cys/C)Cysteine
UGC (Cys/C)Cysteine
		UGA Opal (Stop)
UGG (Trp/W)Tryptophan
CUU (Leu/L)Leucine
CUC (Leu/L)Leucine
CUA (Leu/L)Leucine
CUG (Leu/L)Leucine
CCU (Pro/P)Proline
CCC (Pro/P)Proline
CCA (Pro/P)Proline
CCG (Pro/P)Proline
CAU (His/H)Histidine
CAC (His/H)Histidine
CAA (Gln/Q)Glutamine
CAG (Gln/Q)Glutamine
CGU (Arg/R)Arginine
CGC (Arg/R)Arginine
CGA (Arg/R)Arginine
CGG (Arg/R)Arginine
AUU (Ile/I)Isoleucine
AUC (Ile/I)Isoleucine
AUA (Ile/I)Isoleucine
		AUG (Met/M)Methionine, Start
ACU (Thr/T)Threonine
ACC (Thr/T)Threonine
ACA (Thr/T)Threonine
ACG (Thr/T)Threonine
AAU (Asn/N)Asparagine
AAC (Asn/N)Asparagine
AAA (Lys/K)Lysine
AAG (Lys/K)Lysine
AGU (Ser/S)Serine
AGC (Ser/S)Serine
AGA (Arg/R)Arginine
AGG (Arg/R)Arginine
GUU (Val/V)Valine
GUC (Val/V)Valine
GUA (Val/V)Valine
GUG (Val/V)Valine
GCU (Ala/A)Alanine
GCC (Ala/A)Alanine
GCA (Ala/A)Alanine
GCG (Ala/A)Alanine
GAU (Asp/D)Aspartic acid
GAC (Asp/D)Aspartic acid
GAA (Glu/E)Glutamic acid
GAG (Glu/E)Glutamic acid
GGU (Gly/G)Glycine
GGC (Gly/G)Glycine
GGA (Gly/G)Glycine
GGG (Gly/G)Glycine

Reverse codon table:

This table shows the 20 standard amino acids used in proteins, and the codons that code for each amino acid.

Ala 	A 	GCU, GCC, GCA, GCG
Leu 	L 	UUA, UUG, CUU, CUC, CUA, CUG
Arg 	R 	CGU, CGC, CGA, CGG, AGA, AGG
Lys 	K 	AAA, AAG
Asn 	N 	AAU, AAC
Met 	M 	AUG
Asp 	D 	GAU, GAC
Phe 	F 	UUU, UUC
Cys 	C 	UGU, UGC
Pro 	P 	CCU, CCC, CCA, CCG
Gln 	Q 	CAA, CAG
Ser 	S 	UCU, UCC, UCA, UCG, AGU,AGC
Glu 	E 	GAA, GAG
Thr 	T 	ACU, ACC, ACA, ACG
Gly 	G 	GGU, GGC, GGA, GGG
Trp 	W 	UGG
His 	H 	CAU, CAC
Tyr 	Y 	UAU, UAC
Ile 	I 	AUU, AUC, AUA
Val 	V 	GUU, GUC, GUA, GUG
Start 	AUG
Stop 	UAG, UGA, UAA


</entry>



<entry [Thu Jan  4 13:40:15 PST 2007] IPRSCAN APPS, DATABASE TYPES AND RUN TIMES>



fprintscan
profilescan
blastprodom
hmmsmart
hmmpanther
hmmpfam
hmmtigr
scanregexp
coils
seg
tmhmm
signalp
hmmpir
superfamily
gene3d

my %apps = (
'fprintscan'  => 'PRINTS',
'profilescan' => 'PROFILE',
'pfscan'      => 'PROFILE',
'blastprodom' => 'PRODOM',
'hmmsmart'=> 'SMART',
'hmmpanther'  => 'PANTHER',
'hmmpfam' => 'PFAM',
'hmmtigr' => 'TIGRFAMs',
'scanregexp'  => 'PROSITE',
'coils'   => 'COIL',
'seg' => 'SEG',
'tmhmm'   => 'TMHMM',
'signalp' => 'SIGNALP',
'hmmpir'  => 'PIR',
'superfamily' => 'SUPERFAMILY',
'gene3d'  => 'GENE3D'
);

my $dbtype = {
'fprintscan'  => 'matrix',
'profilescan' => 'strings',
'pfscan'      => 'strings',
'blastprodom' => 'sequences',
'hmmsmart'    => 'model',
'hmmpanther'  => 'model',
'hmmpfam'     => 'model',
'hmmtigr'     => 'model',
'scanregexp'  => 'strings',
'coils'       => 'matrix',
'seg'         => 'NA',
'tmhmm'       => 'model',
'signalp'     => 'model',
'hmmpir'      => 'model',
'superfamily' => 'model',
'gene3d'      => 'model'
};

* Benchmarking information: Crude benchmarking was done for InterProScan running on P50750|CDK9_HUMAN (327aa). This will be repeated for each release so that users know what to expect as far as performance goes. Machine specifications: HP Compaq with 2x Pentium 4 CPUs (3.2GHz); 512Mb RAM. InterProScan was run on 1 CPU; each program was run separately.
      Program Name	Speed in v4.3 (s)
      HMMPfam	70
      HMMPanther	12
      HMMPIR	22
      blastprodom	18
      coils	7
      gene3d	28
      HMMSmart	8
      HMMTigr	23
      FPRINTScan	12
      scanregexp	8
      profilescan	17
      superfamily	151
      seg	8
      signalp	7
      tmhmm	7
      --------------	-----------------
      total	6m38s

</entry>



<entry [Thu Jan  4 13:36:09 PST 2007] COMPLETED RUN OF orthologuesiprscan2engine.pl TO DO ALL hmmpfam FOR ORTHOLOGUES:>



>>>...
SUBMITTED iprscan-20070104-00345994
SUBMITTED iprscan-20070104-00400704
./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.13 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.13
SUBMITTED iprscan-20070104-00451504
SUBMITTED iprscan-20070104-00500557
SUBMITTED iprscan-20070104-00550654
SUBMITTED iprscan-20070104-01000446
./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.14 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.14
SUBMITTED iprscan-20070104-01050075
SUBMITTED iprscan-20070104-01095915
SUBMITTED iprscan-20070104-01150173
/common/iprscan/bin/iprscan: /common/iprscan/tmp/20070104/iprscan-20070104-01150173/iprscan-20070104-01150173.xml unavailable : No such file or directory
iprscan submission failed: checkSequences: Cannot get raw entry from iprmatches: getRawEntryFromIprMatches: query iprmatches failed: Can't use string ("") as an ARRAY ref while "strict refs" in use at /common/iprscan/lib/Index/IprMatches.pm line 421.<br>Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.14.hmmpfam.2'
./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.15 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.15
SUBMITTED iprscan-20070104-01150992
SUBMITTED iprscan-20070104-01300047
SUBMITTED iprscan-20070104-01345789
SUBMITTED iprscan-20070104-01385055
SUBMITTED iprscan-20070104-01432335
SUBMITTED iprscan-20070104-01482262
SUBMITTED iprscan-20070104-01582234
SUBMITTED iprscan-20070104-02032925
SUBMITTED iprscan-20070104-02050432
SUBMITTED iprscan-20070104-02143538
SUBMITTED iprscan-20070104-02200318
SUBMITTED iprscan-20070104-02295285
SUBMITTED iprscan-20070104-02392366
./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.16 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.16
SUBMITTED iprscan-20070104-02484850
SUBMITTED iprscan-20070104-02580912
SUBMITTED iprscan-20070104-03020772
/common/iprscan/bin/iprscan: checkParams: unable to create input files: formatSequences: translated sequence did not contain open reading frames above the chosen threshold (50 bases). Please try again with lower open reading frame size
Can't open file '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.16.hmmpfam.3'
./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.17 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.17
SUBMITTED iprscan-20070104-03071472
SUBMITTED iprscan-20070104-03122494
SUBMITTED iprscan-20070104-03172845
SUBMITTED iprscan-20070104-03223017
SUBMITTED iprscan-20070104-03273349
./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.18 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.18
SUBMITTED iprscan-20070104-03324443
SUBMITTED iprscan-20070104-03380868
SUBMITTED iprscan-20070104-03432870
SUBMITTED iprscan-20070104-03440445
SUBMITTED iprscan-20070104-03485179
./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.19 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.19
SUBMITTED iprscan-20070104-03541185
SUBMITTED iprscan-20070104-03591862
SUBMITTED iprscan-20070104-04042366
SUBMITTED iprscan-20070104-04091270
SUBMITTED iprscan-20070104-04141455
SUBMITTED iprscan-20070104-04185782
SUBMITTED iprscan-20070104-04241404
./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.20 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.20
SUBMITTED iprscan-20070104-04292130
SUBMITTED iprscan-20070104-04342218
SUBMITTED iprscan-20070104-04392206
SUBMITTED iprscan-20070104-04441700
SUBMITTED iprscan-20070104-04490357
SUBMITTED iprscan-20070104-04534732
SUBMITTED iprscan-20070104-04584227
SUBMITTED iprscan-20070104-05033773
SUBMITTED iprscan-20070104-05083032
gems:/Users/local/FUNNYBASE/bin/unigene www$
<<<


</entry>



<entry [Wed Jan  3 19:39:16 PST 2007] =====================================================>


How can I plug in SignalP / TMHMM predictions?
=====================================================


The InterProScan package provides all required scripts/parsers
for the methods but you have to contact the authors to get the
programs and data since they are not publicly available.

Intallation:
SignalP : 1) Save the signalpshell script (well configured)into iprscan/bin/binaries 
	     or made a soft link to your installation. 
	  2) You should not have to do anything with data as they are in your
	     signalp-vxx package by default and signalp shell script is pointing on.
	  3) Open iprscan/conf/signalp.conf file and check if all the path etc are ok.
	     You can use different versions of SignalP: v1.X, v2.X or newer.
	     To do so, open iprscan/conf/signalp.conf and search for signalp.version tag.
	     Then, change value (1-> v1.X, 2-> v2.X or newer).

	  NOTE: SignalP version 2.0 and newer have limitations in the number of submitted sequences
		to 4000.
		If you don't want any restrictions you can try to hack the code editing signalp
		shell script (suggestion). Search for:

		# Maximal number of sequences (command line and WWW):
		# Leave it empty of you don't want any limitations for the max number of input sequence (huge analysis).
		# Default (4000)
		#MAXSEQ=4000
		MAXSEQ=
		MAXWWWSEQ=4000

		#We check if the $MAXSEQ is set. If not it means we don't want any limitations in the number
		#of input sequences. 
		if [ "$MAXSEQ" != "" ] 
		then
			if [ $NSEQ -gt $MAXSEQ ]
			then
				echo signalp: too many sequences, the limit is $MAXSEQ
				exit 1
			elif [ "$WWW" -a \( "$NSEQ" -gt "$MAXWWWSEQ" \) ]
			then
				cat $SIGNALP/doc/wwwtoomany.html | sed 's/_NUM_/'$MAXWWWSEQ'/'
				exit
			fi	
		fi


TMHMM :   1) Save decodeanhmm binary in iprscan/bin/binaries or make a soft link to your
	     installation.
	  2) Save TMHMM2.X.X.model in your iprscan/data directory.
	  3) Uncomment the applications in the header of CONFIG.pl and run it.

BOTH) Edit configuration file tags (signalp.conf) to reflect your system. (queue : local/lsf42/pbs54/sge)
      and host.exec for local implementation or queue.name and/or resource for queueing systems.
      Have a look at other applications configuration file already set.

See: README for the relevant URLs and references.





+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Jan  3 13:12:12 PST 2007

WORKING!!! WITH iprscan2.pl AND NEW IPRSCAN AT /common/iprscan/bin/iprscan:

cd /Users/local/FUNNYBASE/bin/unigene
./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1

>>>
Using default conf file: conf/default.conf
Inputfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1
Outputfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1 -iprlookup -goterms -email syoung@rsmas.miami.edu
SUBMITTED iprscan-20070103-13012992
<interpro_matches>

   <protein id="unigene_dog_1_ORF1" length="339" crc64="F5896B0411A54636" >
        <interpro id="IPR008907" name="P25-alpha" type="Family">
          <match id="PF05517" name="p25-alpha" dbname="PFAM">
            <location start="171" end="335" score="1.1e-117" status="T" evidence="HMMPfam" />
          </match>
        </interpro>
   </protein>
   <protein id="unigene_human_1_ORF1" length="230" crc64="410C0DC6E2D031AE" >
        <interpro id="IPR008907" name="P25-alpha" type="Family">
          <match id="PF05517" name="p25-alpha" dbname="PFAM">
            <location start="62" end="226" score="4.3e-121" status="T" evidence="HMMPfam" />
          </match>
        </interpro>
   </protein>
   <protein id="unigene_mouse_1_ORF2" length="228" crc64="8DAE96AF1B7E2C22" >
        <interpro id="IPR008907" name="P25-alpha" type="Family">
          <match id="PF05517" name="p25-alpha" dbname="PFAM">
            <location start="60" end="224" score="4.5e-116" status="T" evidence="HMMPfam" />
          </match>
        </interpro>
   </protein>
   <protein id="unigene_rat_3_ORF1" length="247" crc64="97095D684EC116EC" >
        <interpro id="IPR008907" name="P25-alpha" type="Family">
          <match id="PF05517" name="p25-alpha" dbname="PFAM">
            <location start="79" end="243" score="4.5e-116" status="T" evidence="HMMPfam" />
          </match>
        </interpro>
   </protein>

</interpro_matches>

No. query records :4

Run time: 00:07:03
Completed ./iprscan2.pl
1:08PM, 3 January 2007
****************************************
<<<


CHECK OUTPUT AT:

cd /Users/local/FUNNYBASE/pipeline/orthologues/fasta
ll *out*tsv


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Jan  3 09:02:03 PST 2007

From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Wednesday, January 3, 2007 1:18 PM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] iprscan okay but some nodes down + users
	
Stuart,

Nodes 12 and 16 are "disabled" because they're different than the  
others.  I noticed that password free ssh as root didn't work for  
those machines.  Rather than figure out the exact manner in which  
they differ, I disabled them from running jobs.  If you just wanted  
to enable them, you would do it like this (in a root shell):

qmod -e all.q@node016

I disabled them with this:

qmod -d all.q@node012

All this "disabling" does is prevents new jobs from being scheduled  
there.   I also did this to the head node, since it should have  
plenty of work to do just serving files and scheduling.

The long term answer to this is probably to install the OS image from  
one of the other nodes on those two machines, wiping out whatever  
difference is there.

I'll send a link to the FAQ when I have it online.

-Chris Dwan
  The Bioteam



</entry>



<entry [Tue Jan  2 19:49:03 PST 2007] AFTER REINSTALL, TESTED RUNNING orthologuesiprscan.pl WHICH CALLS iprscan2.pl:>



su
su - www
cd /Users/local/FUNNYBASE/bin/unigene/
./orthologuesiprscan.pl -t 20 -p hmmpfam -i orthologues.fasta -o orthologues.out

>>>...
Using default conf file: conf/default.conf
Inputfile: orthologues.fasta.9
Outputfile: orthologues.fasta.9
/common/apps/iprscan/bin/iprscan -cli -seqtype n -i orthologues.fasta.9 -iprlookup -goterms -email syoung@rsmas.miami.edu
Error: Unable to read sequence '/common/apps/iprscan/tmp/20070102/iprscan-20070102-17042819/iprscan-20070102-17042819.seqs'
Died: seqret terminated: Bad value for '-sequence' with -auto defined
/common/apps/iprscan/bin/iprscan: checkParams: unable to create input files: formatSequences:
/common/apps/iprscan/conf/seqret.sh  /common/apps/iprscan/tmp/20070102/iprscan-20070102-17042819/iprscan-20070102-17042819.seqs > /common/apps/iprscan/tmp/20070102/iprscan-20070102-17042819/i\
prscan-20070102-17042819.input.formatted: exit status 1
...<<<


qacct -j 1149
>>>...
==============================================================
qname        all.q               
hostname     node008.cluster.private
group        www                 
owner        www                 
project      NONE                
department   defaultdepartment   
jobname      orthologues.iprscan.sh
jobnumber    1149                
taskid       9                   
account      sge                 
priority     0                   
qsub_time    Tue Jan  2 19:47:40 2007
start_time   Tue Jan  2 17:04:26 2007
end_time     Tue Jan  2 17:04:28 2007
...<<<


em /common/apps/iprscan/tmp/20070102/iprscan-20070102-17042819/

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Jan  2 19:43:55 PST 2007

PROBLEM RUNNING orthologuesiprscan.pl WHICH CALLS iprscan2.pl.

cd /Users/local/FUNNYBASE/bin/unigene
em orthologues.iprscan.sh.o1147.9
>>>...
Using default conf file: conf/default.conf
Inputfile: orthologues.fasta.9
Outputfile: orthologues.fasta.9
/common/apps/iprscan/bin/iprscan : /Volumes/gemshd4/common/apps/iprscan is not a directory!
...<<<

SO I REINSTALLED iprscan USING /common/apps/iprscan AS THE BASE DIRECTORY:

gems:/common/apps/iprscan local$ sudo ./Config.pl 
Password:

####################################################################################################
#                Welcome to the InterProScan v4.2 installation script.
#
#  Tips:
#     - If you are happy with the suggestion between [], just press <enter>
#     - Whatever is shown between '/' and '/' is considered a valid input pattern
####################################################################################################


    Reconfigure everything? (first time install) [n] /(y|n)/? : y

! Tip:
!
! InterProScan needs to know where it is installed and where perl is installed
!
    Do you want to set paths to perl and the installation directory? [y] /y|n/? : y

! Tip:
!
! If your servers are using a shared file system, such as NFS, InterProScan is able to utilise this
! and perform distributed computing across multiple servers
! You will need set up the full UNIX path to the installation directory.
! You also need to ensure that the InterProScan installation is on a SHARED DISK.
!
    Please enter the full path for the InterProScan installation [/Volumes/gemshd4/common/apps/iprscan] /.+/? : /common/apps/iprscan
    Do you want to set another Perl command in place of [/usr/bin/perl]? [n] /y|n/? : n
>>Changing Perl path in scripts ... >>DONE

>>Checking if your Perl installation has the modules required by InterProScan...
>>All modules needed are already installed.


! Tip:
!
! To cope with bulk jobs and to enable efficient parallelization, InterProScan splits input files
! with FASTA formatted sequences into smaller parts (chunks). Default size is "10" sequences per chunk
!
    Do you want to setup chunk size [y] /(y|n)/? : n


! Tip:
!
! You can configure InterProScan to limit the number of input sequences (protein & nucleotide),
! set a maximum length for nucleotide sequences and a minimum length for protein sequences.
! You can also configure the minimum length for a translated orf and use the codon table value
! to translate the dna/rna sequence into six frames (http://www.ebi.ac.uk/cgi-bin/mutations/trtables.cgi).
!
    Do you want to configure this? [y] /(y|n)/? : n


! Tip:
!
! InterProScan currently encorporates searching against 13 databases. You have the choice whether or not to
! set up a particular search tool against a database and how exactly that application will be configured to run
! in this next section. You will configure the queue systems (if any) you will use, first
!
    Do you want to setup applications (if you don't, no applications will be included in InterProScan by default)? [y] /(y|n)/? : 

! Tip:
!
! InterProScan is able to execute the applications via a UNIX QUEUING system which has an INTERACTIVE mode (such as LSF).
! Currently, we mainly support LSF (which is used at EBI) however, we also provide configuration files for other systems
! which are listed in the documentation
!
    Do you wish to use a queue system? [n] /(y|n)/? : n
>>Checking your local architecture for InterProScan compatibility

! Tip:
! Blastprodom (a wrapper script on top of Blast package) is used to search against PRODOM families
    Do you want to use blastprodom ? [y] /(y|n)/? : y
!
! PLEASE NOTE:
! InterProScan can be launched through a cluster machine using RSH. To be able to utilise this
! feature, all your machines must be visible to each other (see the .rhost file) and be mounted on
! a shared file system, such as NFS
! (If you are installing InterProScan for use on only one machine please ignore the above message)
!
    
Please enter the execution host name of blastprodom [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Coils is used to predict coiled-coil segments with the algorithm of Lupas et al.
    Do you want to use coils ? [y] /(y|n)/? : y
    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Gene3d Gene3D is supplementary to the CATH database. This protein sequence database contains proteins from complete genomes which have been clustered into protein families and annotated with CATH domains, Pfam domains and functional information from KEGG, GO, COG, Affymetrix and STRINGS.
    Do you want to use gene3d ? [y] /(y|n)/? : y
    
Please enter the execution host name of gene3d [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmpanther (hmmpfam from HMMER 2.3.2 package) PANTHER classifies proteins into families and specific functional subfamilies, which are then mapped to ontology terms and pathways.
    Do you want to use hmmpanther ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmpanther [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmpir PIR produces the Protein  Sequence Database (PSD) of functionally annotated protein sequences, which grew out of the Atlas of Protein Sequence and Structure (1965-1978) edited by Margaret Dayhoff and has been incorporated into an integrated knowledge base system of value-added databases and analytical tools.
    Do you want to use hmmpir ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmpir [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmpfam (hmmpfam from HMMER 2.3.2 package) is used to search against the Pfam HMM (Profile hidden Markov models) database
    Do you want to use hmmpfam ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmpfam [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmsmart (hmmpfam from HMMER 2.3.2 package) is used to search against the Smart HMM (Profile hidden Markov models) database
    Do you want to use hmmsmart ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmsmart [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmtigr (hmmpfam from HMMER 2.3.2 package) is used to search against TIGRFAMs collection of HMMs (Profile hidden Markov models)
    Do you want to use hmmtigr ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmtigr [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Fprintscan (FingerPRINTScan) is used to search against the PRINTS collection of protein signatures
    Do you want to use fprintscan ? [y] /(y|n)/? : 
    
Please enter the execution host name of fprintscan [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Scanregexp (ScanProsite) is used to search against the PROSITE patterns collection and verify the  matches by statistically significant CONFIRM patterns
    Do you want to use scanregexp ? [y] /(y|n)/? : 
    
Please enter the execution host name of scanregexp [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Profilescan (ProfileScan (pfscan & ps_scan.pl) from Pftools 2.2) package is used  to search against the PROSITE profiles collection
    Do you want to use profilescan ? [y] /(y|n)/? : 
    
Please enter the execution host name of profilescan [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Superfamily A structural classification of proteins database for the investigation of sequences and structures.
    Do you want to use superfamily ? [y] /(y|n)/? : 
    
Please enter the execution host name of superfamily [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Seg is used for identifying and masking segments of low compositional complexity in amino acid sequences
    Do you want to use seg ? [y] /(y|n)/? : 
    
Please enter the execution host name of seg [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Signalp predicts the presence and location of signal peptide cleavage sites, using eukaryotic HMM (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use signalp ? [n] /(y|n)/? : y
    
Please enter the execution host name of signalp [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Tmhmm is used for prediction of transmembrane helices in proteins using a Hidden Markov Model (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use tmhmm ? [n] /(y|n)/? : y
    
Please enter the execution host name of tmhmm [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
!
! InterProScan can use taxonomy-based filtering of results which uses the InterPro taxonomy listed for each InterPro
! entry to hide hits not relevant to that taxon.
!
    Do you want to make it available for users? [y] /y|n/? : 

! Tip:
!
! You can specify an email address for the administrator. By default it will be you (root)
! Administrators receive an email when a problem occurs in InterProScan.
!
    Do you want to set an administrator email address? [y] /y|n/? : 
    Please enter the email address of the administrator: [] /[\w\.\-]+\@[\w\.\-]+/? : syoung@rsmas.miami.edu
>>Writing to the configuration files ... 
>>Processing file : /common/apps/iprscan/conf/blastprodom.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/coils.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/fprintscan.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/gene3d.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpanther.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpfam.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpir.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmsmart.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmtigr.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/profilescan.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/scanregexp.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/seg.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/signalp.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/superfamily.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/tmhmm.conf ... >>DONE
>>Setting File /common/apps/iprscan/conf/iprscan.conf ... >>DONE

! Tip:
!
! InterProScan can be launched through a web interface. It can also be launched through secure HTTP (https)
!
    Do you want to run InterProScan using a web interface? [n] /y|n/? : y
    Do you want to run InterProScan through a secure HTTP server? [n] /y|n/? : n
    Enter the name of your server. You can specify a specific port to listen to. (e.g. "foo.bar.com:4000") [foobar.com] /[\w\.\_\-\:\d]+/? : gems.rsmas.miami.edu
!
! Please add the following lines to your web server configuration (httpd.conf):
#--------------------------------------
 Alias /doc/ "/Volumes/gemshd4/common/apps/iprscan/doc/html/"
 Alias /images/ "/Volumes/gemshd4/common/apps/iprscan/images/"
 Alias /tmp/ "/Volumes/gemshd4/common/apps/iprscan/tmp/"

 ScriptAlias /iprscan/ "/Volumes/gemshd4/common/apps/iprscan/bin/"

 <Directory "/Volumes/gemshd4/common/apps/iprscan/images">
   Options Indexes MultiViews
   AllowOverride None
   Order allow,deny
   Allow from all
 </Directory>

 <Directory "/Volumes/gemshd4/common/apps/iprscan/tmp/">
   Options FollowSymLinks Includes SymLinksIfOwnerMatch
    IndexIgnore */.??* *~ *# */HEADER* */README* */RCS
    AllowOverride AuthConfig Limit FileInfo

    Order deny,allow
    Deny from all
    Allow from .rsmas.miami.edu
 </Directory>
#--------------------------------------
! Be sure that 'apache' or 'httpd' virtual user (uid/gid) have right to read/write into your /Volumes/gemshd4/common/apps/iprscan iprscan directory.
! Otherwise, if you are running a standalone http(s) web server, be sure also the user running it have to enough
! rights to read/wrtie /Volumes/gemshd4/common/apps/iprscan iprscan directory
! If you encoutered problem with your web server running iprscan, read the FAQ or contact your system administrator.

! After restarting your web server try http://gems.rsmas.miami.edu/iprscan/iprscan
!
>>Indexing datafile ... 
No specific file(s) given, indexing all files:

Pfam
prints.pval
prodom.ipr
Pfam-C
smart.desc
sf_hmm
sf.seq
Pfam-A.seed
superfamily.hmm
smart.thresholds
smart.HMMs
match.xml
interpro.xml
TIGRFAMs_HMM.LIB
Gene3D.hmm


No specific file(s) given, converting all files:

Pfam
sf_hmm
superfamily.hmm
smart.HMMs
sf_hmm_sub
TIGRFAMs_HMM.LIB
Gene3D.hmm

WARNING: Some problems occured when trying to index the data files.
 Please fix the error below and index your datafiles using the /common/apps/iprscan/bin/index_data.pl script.
 For help, type './index_data.pl -h' 
errormsg: ERROR: File [/common/apps/iprscan/data/sf.seq] not found : No such file or directory at /common/apps/iprscan/bin/index_data.pl line 217.
>>DONE

! Tip:
!
!  We would appreciate it if you would register your installation of InterProScan.  This will allow us to notify you
! of bug fixes and new releases of the software. We will not use your email address for any other purpose.
!
    Would you like to register InterProScan? [y] /(y|n)/? : n

>>Installation is completed.  Please test your installation by running on the command line:

        /common/apps/iprscan/bin/iprscan -cli -i /common/apps/iprscan/test.seq -o /common/apps/iprscan/test.out -format raw -goterms -ipr

and comparing it with the merged.raw file that is supplied.
For help and options on the commandline, run /common/apps/iprscan/bin/iprscan -cli -h




+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Jan  2 15:06:10 PST 2007

MSG: inquiry-support@bioteam.net
Re: [bioteam.net #13817] iprscan okay but some nodes down + users

Hi Chris,

Thanks for getting it up and running. Could you send me the FAQ you wrote up to cover the iprscan changes?

I tried running just hmmpfam on 'gems' and it worked fine:

su
su - www
/common/iprscan/bin/iprscan -cli -appl hmmpfam -i /common/iprscan/test.seq -o /common/iprscan/test.out -format raw -goterms -iprlookup

I also ran hmmpfam okay on some execution nodes (node002, node015, node016). On node016 I found that the job was submitted to sge okay (the job ran on node008) but node016 is 'disabled':

qstat -j 1141
>>>...
scheduling info:            queue instance "all.q@node012.cluster.private" dropped because it is disabled
                            queue instance "all.q@gems.rsmas.miami.edu" dropped because it is disabled
                            queue instance "all.q@node016.cluster.private" dropped because it is disabled
<<<

I tried restarting SGE using SystemStarter but this failed:

node016:~ vanwye$ sudo SystemStarter -v -d start SGE
Password:
SystemStarter[2888]: Found item: AutoFS
SystemStarter[2888]: Requires: Evaluating Portmap
SystemStarter[2888]: Uses: Evaluating Disks
SystemStarter[2888]: Uses: Keeping Disks
SystemStarter[2888]: Found item: GANGLIA
SystemStarter[2888]: Requires: Evaluating Network
SystemStarter[2888]: Uses: Evaluating Disks
SystemStarter[2888]: Uses: Keeping Disks
SystemStarter[2888]: Uses: Evaluating AutoFS
SystemStarter[2888]: Uses: Keeping AutoFS
SystemStarter[2888]: Found item: NodeConfig
SystemStarter[2888]: Requires: Evaluating AutoFS
SystemStarter[2888]: Requires: Keeping AutoFS
SystemStarter[2888]: Found item: SGE
SystemStarter[2888]: Requires: Evaluating Disks
SystemStarter[2888]: Requires: Keeping Disks
SystemStarter[2888]: Requires: Evaluating Resolver
SystemStarter[2888]: Found item: Apache
SystemStarter[2888]: Uses: Evaluating Disks
SystemStarter[2888]: Uses: Keeping Disks
SystemStarter[2888]: Uses: Evaluating NFS
SystemStarter[2888]: Uses: Keeping NFS
SystemStarter[2888]: Found item: AppleShare
SystemStarter[2888]: Requires: Evaluating Disks
SystemStarter[2888]: Requires: Keeping Disks
SystemStarter[2888]: Found item: AppServices
SystemStarter[2888]: Found item: AuthServer
SystemStarter[2888]: Found item: CrashReporter
SystemStarter[2888]: Found item: Disks
SystemStarter[2888]: Requires: Evaluating SecurityServer
SystemStarter[2888]: Uses: Evaluating System Tuning
SystemStarter[2888]: Found item: FibreChannel
SystemStarter[2888]: Requires: Evaluating SecurityServer
SystemStarter[2888]: Found item: HeadlessStartup
SystemStarter[2888]: Requires: Evaluating NetInfo
SystemStarter[2888]: Uses: Evaluating NetInfo
SystemStarter[2888]: Found item: IFCStart
SystemStarter[2888]: Found item: IPAliases
SystemStarter[2888]: Requires: Evaluating Network
SystemStarter[2888]: Found item: IPFailover
SystemStarter[2888]: Requires: Evaluating Network
SystemStarter[2888]: Found item: IPFilter
SystemStarter[2888]: Requires: Evaluating NetworkExtensions
SystemStarter[2888]: Requires: Evaluating SerialNumberSupport
SystemStarter[2888]: Requires: Keeping SerialNumberSupport
SystemStarter[2888]: Found item: IPServices
SystemStarter[2888]: Uses: Evaluating mDNSResponder
SystemStarter[2888]: Uses: Evaluating Portmap
SystemStarter[2888]: Found item: Mailman
SystemStarter[2888]: Requires: Evaluating Network
SystemStarter[2888]: Uses: Evaluating Network Time
SystemStarter[2888]: Uses: Keeping Network Time
SystemStarter[2888]: Uses: Evaluating NFS
SystemStarter[2888]: Uses: Keeping NFS
SystemStarter[2888]: Found item: Metadata
SystemStarter[2888]: Requires: Evaluating Core Services
SystemStarter[2888]: Requires: Keeping Core Services
SystemStarter[2888]: Requires: Evaluating Disks
SystemStarter[2888]: Requires: Keeping Disks
SystemStarter[2888]: Found item: MySQL
SystemStarter[2888]: Requires: Evaluating Resolver
SystemStarter[2888]: Requires: Evaluating Disks
SystemStarter[2888]: Requires: Keeping Disks
SystemStarter[2888]: Found item: NAT
SystemStarter[2888]: Requires: Evaluating Network
SystemStarter[2888]: Uses: Evaluating IPFilter
SystemStarter[2888]: Uses: Keeping IPFilter
SystemStarter[2888]: Found item: NetworkTime
SystemStarter[2888]: Found item: NFS
SystemStarter[2888]: Uses: Evaluating Disks
SystemStarter[2888]: Uses: Keeping Disks
SystemStarter[2888]: Found item: NIS
SystemStarter[2888]: Found item: PrintingServices
SystemStarter[2888]: Requires: Evaluating Resolver
SystemStarter[2888]: Uses: Evaluating Network Time
SystemStarter[2888]: Uses: Keeping Network Time
SystemStarter[2888]: Found item: QuickTimeStreamingServer
SystemStarter[2888]: Requires: Evaluating Resolver
SystemStarter[2888]: Found item: RemoteDesktopAgent
SystemStarter[2888]: Requires: Evaluating Disks
SystemStarter[2888]: Requires: Keeping Disks
SystemStarter[2888]: Requires: Evaluating Resolver
SystemStarter[2888]: Found item: Samba
SystemStarter[2888]: Requires: Evaluating Disks
SystemStarter[2888]: Requires: Keeping Disks
SystemStarter[2888]: Requires: Evaluating DirectoryServices
SystemStarter[2888]: Found item: SerialNumberSupport
SystemStarter[2888]: Found item: SerialTerminalSupport
SystemStarter[2888]: Uses: Evaluating SystemLog
SystemStarter[2888]: Uses: Keeping SystemLog
SystemStarter[2888]: Found item: SNMP
SystemStarter[2888]: Requires: Evaluating Resolver
SystemStarter[2888]: Found item: SoftwareUpdateServer
SystemStarter[2888]: Requires: Evaluating Network
SystemStarter[2888]: Uses: Evaluating Network
SystemStarter[2888]: Checking Sun Grid Engine Services
SystemStarter[2888]: Antecedents: <CFArray 0x3018d0 [0xa07ba150]>{type = fixed-mutable, count = 1, capacity = 2, values = (
        0 : <CFString 0x301bf0 [0xa07ba150]>{contents = "Disks"}
)}
SystemStarter[2888]:    Failed requirement/uses: Disks
SystemStarter[2888]: Checking local disks
SystemStarter[2888]: Antecedents: <CFArray 0x3021b0 [0xa07ba150]>{type = fixed-mutable, count = 0, capacity = 1, values = (
)}
SystemStarter[2888]: Soft dependancies: <CFArray 0x302550 [0xa07ba150]>{type = fixed-mutable, count = 0, capacity = 1, values = (
)}
SystemStarter[2888]: Best pick so far, based on failed dependancies (2147483647->0)
SystemStarter[2888]: Running command (2889): /System/Library/StartupItems/Disks/Disks start
SystemStarter[2888]: Checking Sun Grid Engine Services
SystemStarter[2888]: Antecedents: <CFArray 0x3018d0 [0xa07ba150]>{type = fixed-mutable, count = 1, capacity = 2, values = (
        0 : <CFString 0x301bf0 [0xa07ba150]>{contents = "Disks"}
)}
SystemStarter[2888]:    Failed requirement/uses: Disks
Checking disks
SystemStarter[2888]: Finished local disks (2889)
SystemStarter[2888]: Checking Sun Grid Engine Services
SystemStarter[2888]: Antecedents: <CFArray 0x3018d0 [0xa07ba150]>{type = fixed-mutable, count = 1, capacity = 2, values = (
        0 : <CFString 0x301bf0 [0xa07ba150]>{contents = "Disks"}
)}
SystemStarter[2888]: No soft dependancies
SystemStarter[2888]: Best pick so far, based on failed dependancies (2147483647->0)
SystemStarter[2888]: Running command (2891): /Library/StartupItems/SGE/SGE start
SystemStarter[2888]: IPC message = <CFDictionary 0x3018f0 [0xa07ba150]>{type = mutable, count = 3, capacity = 17, pairs = (
        5 : <CFString 0x3046b0 [0xa07ba150]>{contents = "ProcessID"} = <CFNumber 0x301b70 [0xa07ba150]>{value = +2891, type = kCFNumberSInt64Type}
        11 : <CFString 0x301620 [0xa07ba150]>{contents = "ConsoleMessage"} = <CFString 0x3025b0 [0xa07ba150]>{contents = "Starting Sun Grid Engine"}
        18 : <CFString 0x303fd0 [0xa07ba150]>{contents = "Message"} = <CFString 0x301620 [0xa07ba150]>{contents = "ConsoleMessage"}
)}
SystemStarter[2888]: Starting Sun Grid Engine
SystemStarter[2888]: Sun Grid Engine Services (2891) did not complete successfully
SystemStarter[2888]: none left
SystemStarter[2888]: The following StartupItems failed to properly start:
SystemStarter[2888]: /Library/StartupItems/SGE
SystemStarter[2888]:  - execution of Startup script failed
node016:~ vanwye$ ps aux | grep sge
sge       2963   0.0  0.1    28504    808  ??  S     3:50PM   0:00.01 /common/sge/bin/darwin/sge_execd
vanwye    2965   0.0  0.0    18048    244  p0  R+    3:50PM   0:00.00 grep sge

SO I REBOOTED gems TO REFRESH THE qstat QUEUE but got the same error:

qstat -f

queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       0.16     darwin        d
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.05     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   0/2       0.02     darwin        d
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.03     darwin        d

Curiously, it was nodes 012 and 016 that were still available in the queue when the other nodes dropped off abouty a week ago (until I rebooted all the other nodes and the head node, which brought them back into the queue).

Even after rebooting node016 and running SystemStarter I get a similar error:

node016:~ vanwye$ sudo SystemStarter -v -d start SGE
Password:
SystemStarter[1895]: Found item: AutoFS
SystemStarter[1895]: Requires: Evaluating Portmap
SystemStarter[1895]: Uses: Evaluating Disks
SystemStarter[1895]: Uses: Keeping Disks
SystemStarter[1895]: Found item: GANGLIA
SystemStarter[1895]: Requires: Evaluating Network
SystemStarter[1895]: Uses: Evaluating Disks
SystemStarter[1895]: Uses: Keeping Disks
SystemStarter[1895]: Uses: Evaluating AutoFS
SystemStarter[1895]: Uses: Keeping AutoFS
SystemStarter[1895]: Found item: NodeConfig
SystemStarter[1895]: Requires: Evaluating AutoFS
SystemStarter[1895]: Requires: Keeping AutoFS
SystemStarter[1895]: Found item: SGE
SystemStarter[1895]: Requires: Evaluating Disks
SystemStarter[1895]: Requires: Keeping Disks
SystemStarter[1895]: Requires: Evaluating Resolver
SystemStarter[1895]: Found item: Apache
SystemStarter[1895]: Uses: Evaluating Disks
SystemStarter[1895]: Uses: Keeping Disks
SystemStarter[1895]: Uses: Evaluating NFS
SystemStarter[1895]: Uses: Keeping NFS
SystemStarter[1895]: Found item: AppleShare
SystemStarter[1895]: Requires: Evaluating Disks
SystemStarter[1895]: Requires: Keeping Disks
SystemStarter[1895]: Found item: AppServices
SystemStarter[1895]: Found item: AuthServer
SystemStarter[1895]: Found item: CrashReporter
SystemStarter[1895]: Found item: Disks
SystemStarter[1895]: Requires: Evaluating SecurityServer
SystemStarter[1895]: Uses: Evaluating System Tuning
SystemStarter[1895]: Found item: FibreChannel
SystemStarter[1895]: Requires: Evaluating SecurityServer
SystemStarter[1895]: Found item: HeadlessStartup
SystemStarter[1895]: Requires: Evaluating NetInfo
SystemStarter[1895]: Uses: Evaluating NetInfo
SystemStarter[1895]: Found item: IFCStart
SystemStarter[1895]: Found item: IPAliases
SystemStarter[1895]: Requires: Evaluating Network
SystemStarter[1895]: Found item: IPFailover
SystemStarter[1895]: Requires: Evaluating Network
SystemStarter[1895]: Found item: IPFilter
SystemStarter[1895]: Requires: Evaluating NetworkExtensions
SystemStarter[1895]: Requires: Evaluating SerialNumberSupport
SystemStarter[1895]: Requires: Keeping SerialNumberSupport
SystemStarter[1895]: Found item: IPServices
SystemStarter[1895]: Uses: Evaluating mDNSResponder
SystemStarter[1895]: Uses: Evaluating Portmap
SystemStarter[1895]: Found item: Mailman
SystemStarter[1895]: Requires: Evaluating Network
SystemStarter[1895]: Uses: Evaluating Network Time
SystemStarter[1895]: Uses: Keeping Network Time
SystemStarter[1895]: Uses: Evaluating NFS
SystemStarter[1895]: Uses: Keeping NFS
SystemStarter[1895]: Found item: Metadata
SystemStarter[1895]: Requires: Evaluating Core Services
SystemStarter[1895]: Requires: Keeping Core Services
SystemStarter[1895]: Requires: Evaluating Disks
SystemStarter[1895]: Requires: Keeping Disks
SystemStarter[1895]: Found item: MySQL
SystemStarter[1895]: Requires: Evaluating Resolver
SystemStarter[1895]: Requires: Evaluating Disks
SystemStarter[1895]: Requires: Keeping Disks
SystemStarter[1895]: Found item: NAT
SystemStarter[1895]: Requires: Evaluating Network
SystemStarter[1895]: Uses: Evaluating IPFilter
SystemStarter[1895]: Uses: Keeping IPFilter
SystemStarter[1895]: Found item: NetworkTime
SystemStarter[1895]: Found item: NFS
SystemStarter[1895]: Uses: Evaluating Disks
SystemStarter[1895]: Uses: Keeping Disks
SystemStarter[1895]: Found item: NIS
SystemStarter[1895]: Found item: PrintingServices
SystemStarter[1895]: Requires: Evaluating Resolver
SystemStarter[1895]: Uses: Evaluating Network Time
SystemStarter[1895]: Uses: Keeping Network Time
SystemStarter[1895]: Found item: QuickTimeStreamingServer
SystemStarter[1895]: Requires: Evaluating Resolver
SystemStarter[1895]: Found item: RemoteDesktopAgent
SystemStarter[1895]: Requires: Evaluating Disks
SystemStarter[1895]: Requires: Keeping Disks
SystemStarter[1895]: Requires: Evaluating Resolver
SystemStarter[1895]: Found item: Samba
SystemStarter[1895]: Requires: Evaluating Disks
SystemStarter[1895]: Requires: Keeping Disks
SystemStarter[1895]: Requires: Evaluating DirectoryServices
SystemStarter[1895]: Found item: SerialNumberSupport
SystemStarter[1895]: Found item: SerialTerminalSupport
SystemStarter[1895]: Uses: Evaluating SystemLog
SystemStarter[1895]: Uses: Keeping SystemLog
SystemStarter[1895]: Found item: SNMP
SystemStarter[1895]: Requires: Evaluating Resolver
SystemStarter[1895]: Found item: SoftwareUpdateServer
SystemStarter[1895]: Requires: Evaluating Network
SystemStarter[1895]: Uses: Evaluating Network
SystemStarter[1895]: Checking Sun Grid Engine Services
SystemStarter[1895]: Antecedents: <CFArray 0x3018d0 [0xa07ba150]>{type = fixed-mutable, count = 1, capacity = 2, values = (
        0 : <CFString 0x301bf0 [0xa07ba150]>{contents = "Disks"}
)}
SystemStarter[1895]:    Failed requirement/uses: Disks
SystemStarter[1895]: Checking local disks
SystemStarter[1895]: Antecedents: <CFArray 0x3021b0 [0xa07ba150]>{type = fixed-mutable, count = 0, capacity = 1, values = (
)}
SystemStarter[1895]: Soft dependancies: <CFArray 0x302550 [0xa07ba150]>{type = fixed-mutable, count = 0, capacity = 1, values = (
)}
SystemStarter[1895]: Best pick so far, based on failed dependancies (2147483647->0)
SystemStarter[1895]: Running command (1896): /System/Library/StartupItems/Disks/Disks start
SystemStarter[1895]: Checking Sun Grid Engine Services
SystemStarter[1895]: Antecedents: <CFArray 0x3018d0 [0xa07ba150]>{type = fixed-mutable, count = 1, capacity = 2, values = (
        0 : <CFString 0x301bf0 [0xa07ba150]>{contents = "Disks"}
)}
SystemStarter[1895]:    Failed requirement/uses: Disks
Checking disks
SystemStarter[1895]: Finished local disks (1896)
SystemStarter[1895]: Checking Sun Grid Engine Services
SystemStarter[1895]: Antecedents: <CFArray 0x3018d0 [0xa07ba150]>{type = fixed-mutable, count = 1, capacity = 2, values = (
        0 : <CFString 0x301bf0 [0xa07ba150]>{contents = "Disks"}
)}
SystemStarter[1895]: No soft dependancies
SystemStarter[1895]: Best pick so far, based on failed dependancies (2147483647->0)
SystemStarter[1895]: Running command (1898): /Library/StartupItems/SGE/SGE start
SystemStarter[1895]: IPC message = <CFDictionary 0x3018f0 [0xa07ba150]>{type = mutable, count = 3, capacity = 17, pairs = (
        5 : <CFString 0x3046b0 [0xa07ba150]>{contents = "ProcessID"} = <CFNumber 0x301b70 [0xa07ba150]>{value = +1898, type = kCFNumberSInt64Type}
        11 : <CFString 0x301620 [0xa07ba150]>{contents = "ConsoleMessage"} = <CFString 0x3025b0 [0xa07ba150]>{contents = "Starting Sun Grid Engine"}
        18 : <CFString 0x303fd0 [0xa07ba150]>{contents = "Message"} = <CFString 0x301620 [0xa07ba150]>{contents = "ConsoleMessage"}
)}
SystemStarter[1895]: Starting Sun Grid Engine
SystemStarter[1895]: Waiting for Sun Grid Engine Services
SystemStarter[1895]: Waiting for Sun Grid Engine Services
SystemStarter[1895]: Waiting for Sun Grid Engine Services
SystemStarter[1895]: Waiting for Sun Grid Engine Services
SystemStarter[1895]: Waiting for Sun Grid Engine Services
SystemStarter[1895]: Waiting for Sun Grid Engine Services
SystemStarter[1895]: Waiting for Sun Grid Engine Services
SystemStarter[1895]: Waiting for Sun Grid Engine Services
SystemStarter[1895]: Sun Grid Engine Services (1898) did not complete successfully
SystemStarter[1895]: none left
SystemStarter[1895]: The following StartupItems failed to properly start:
SystemStarter[1895]: /Library/StartupItems/SGE
SystemStarter[1895]:  - execution of Startup script failed

Aside from the problem of these two execution nodes being disabled, there is still the issue of the user settings on the head node and the execution nodes. Do you have any ideas about either of these problems?

Cheers,

Stuart.



>From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
>Reply-To: inquiry-support@bioteam.net
>To: youngstuart@hotmail.com
>Subject: Re: [bioteam.net #13817] install submit host + interproscan
>Date: Tue, 2 Jan 2007 12:54:14 -0500
>
>
>Stuart,
>
>I think I finally got the last complication out of interproscan.  I'm
>writing up the changes that were necessary to get this working on our
>FAQ system, so that others can have a quicker time of it.  I've run a
>couple of tests, and they "work for me" as the user "www".
>
>Could you please check it out on your own and see whether you get
>coherent results?
>
>-Chris Dwan
>   The BioTeam
>
>On Jan 1, 2007, at 11:58 PM, stuart young via RT wrote:
>
> >
> > <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >
> > Hi Chris,
> >
> > Thanks for working on it over the weekend. Did you manage to get at
> > that
> > environment variable?
> >
> > Stuart.
> >
> > _________________________________________________________________
> > Don't just search. Find. Check out the new MSN Search!
> > http://search.msn.click-url.com/go/onm00200636ave/direct/01/
> >
>
>

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Jan  2 15:03:54 PST 2007

From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Tuesday, January 2, 2007 5:54 PM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] install submit host + interproscan
	
Stuart,

I think I finally got the last complication out of interproscan.  I'm  
writing up the changes that were necessary to get this working on our  
FAQ system, so that others can have a quicker time of it.  I've run a  
couple of tests, and they "work for me" as the user "www".

Could you please check it out on your own and see whether you get  
coherent results?

-Chris Dwan
  The BioTeam

On Jan 1, 2007, at 11:58 PM, stuart young via RT wrote:

>
> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
>
> Hi Chris,
>
> Thanks for working on it over the weekend. Did you manage to get at  
> that
> environment variable?
>
> Stuart.
>
> _________________________________________________________________
> Don't just search. Find. Check out the new MSN Search!
> http://search.msn.click-url.com/go/onm00200636ave/direct/01/
>


From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Tuesday, January 2, 2007 5:07 PM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] install submit host + interproscan
	
Stuart,

So far, "no."  The problem at the moment is that all the pieces of  
the jobs are completing, but the final "supervisor" job is failing to  
notice that they're done.  It runs forever.

I'm digging around in "ipr_qstat.pl" and similar scripts to find out  
why this isn't working.

I assume that you're the one starting the other iprscan jobs that I'm  
seeing on the system this morning.

-Chris Dwan
  The BioTeam

On Jan 1, 2007, at 11:58 PM, stuart young via RT wrote:

>
> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
>
> Hi Chris,
>
> Thanks for working on it over the weekend. Did you manage to get at  
> that
> environment variable?
>
> Stuart.
>
> _________________________________________________________________
> Don't just search. Find. Check out the new MSN Search!
> http://search.msn.click-url.com/go/onm00200636ave/direct/01/
>

RECAP OF CHANGES TO RUN iprscan ON sge:

Michael Gang michael.gang at evogene.com
Mon Sep 4 06:09:01 EDT 2006

    * Next message: [Bioclusters] running interpro at all
    * Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]

Dear all,


Well, it looks like we've got Interpro running on the cluster with the 
Sun Grid Engine queuing system.


Steps needed to make this work (technical details):

1 - Allow job submission from all nodes (!)
2 - Update qstat_ipr.pl in the iprscan/bin directory (the patch from 
interpro should work, basically-you need to use the -f flag for qstat).
3 - Create a symbolic link to iprscan/conf/sge6.env under 
iprscan/conf/sge.env
3 - Change the following line in iprscan/sge6/conf

asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e 
/dev/null [%env IPRSCAN_HOME]/conf/sge6-test.sh [%toolcmd]

To this:
asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e 
/dev/null [%toolcmd]

Best regards,
Michael Gang
Evogene Bioinformatics

P.S. I would like to specially thank Mr. Powers for his great help

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

CHRIS USED THIS COMMAND:

su
su - www
top -u

gems:~ www$ /common/iprscan/bin/iprscan -cli -i /common/iprscan/test.seq -o /common/iprscan/test.out -format raw -goterms -iprlookup


From :	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To :    inquiry-support@bioteam.net
Sent :	 Tuesday, January 2, 2007 5:03 AM
To :	 youngstuart@hotmail.com
Subject :	Re: [bioteam.net #13817] install submit host + interproscan
	
	
I've got a set of jobs running now.  The trick was to substitute in a  
full path in /common/iprscan/conf/sge6.conf

I don't know if the jobs will succeed, but at the very least, they're  
still running.

-Chris Dwan
  The BioTeam


CHRIS CHANGED /common/iprscan/conf/sge6.conf FROM THIS:

>>>
default.template=[%env IPRSCAN_HOME]/conf/sgeenv.sh
default.tool.template=[%env IPRSCAN_HOME]/conf/sgetool.sh
default.workdir=[%env IPRSCAN_HOME]/tmp
debug=0
status=[%env IPRSCAN_HOME]/bin/qstat_ipr.pl
kill=qdel
optqueue=[%if %queue.name  ? -q %queue.name]

asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e /dev/null [%toolcmd]
syncsub=qsub -N [%jobid] [%optqueue] -o /dev/null -e /dev/null -I [%toolcmd]

jobid.regexp=Your job (\d+)
state.notfound.regexp=Following jobs do not exist
state.pend.regexp=\w+\s+(s|h|T|t|w|S|qw)\s+\d+
state.run.regexp=\w+\s+(r)\s+\d+
state.done.regexp=\w+\s+(DONE)\s+\d+
state.fail.regexp=\w+\s+(Eqw)\s+\d+
<<<

TO THIS:

>>>
default.template=[%env IPRSCAN_HOME]/conf/sge6env.sh
default.tool.template=[%env IPRSCAN_HOME]/conf/sge6tool.sh
default.workdir=[%env IPRSCAN_HOME]/tmp
debug=0
status=[%env IPRSCAN_HOME]/bin/qstat_ipr.pl
kill=qdel
optqueue=[%if %queue.name  ? -q %queue.name]

# CMD
# Per http://bioinformatics.org/pipermail/bioclusters/2006-September/003084.html
# asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e /dev/null [%env IPRSCAN_HOME]/conf/sge6-test.sh [%toolcmd]
asyncsub=/common/sge/bin/darwin/qsub -N [%jobid] [%optqueue] -shell y -b y -o /dev/null -e /dev/null -v PERL5LIB=/RemotePerl [%toolcmd]
syncsub=/common/sge/bin/darwin/qsub -N [%jobid] [%optqueue] -o /dev/null -e /dev/null -v PERL5LIB=/RemotePerl -I [%toolcmd]

jobid.regexp=Your job (\d+)
state.notfound.regexp=Following jobs do not exist
state.pend.regexp=\w+\s+(s|h|T|t|w|S|qw)\s+\d+
state.run.regexp=\w+\s+(r)\s+\d+
state.done.regexp=\w+\s+(DONE)\s+\d+
state.fail.regexp=\w+\s+(Eqw)\s+\d+
<<<


++++++++++++++++++++++++++++++++++++++++++++++++++++++++
MSG: inquiry-support@bioteam.net

Hi Chris,

I indexed match_complete.xml and ran the new iprscan:

sudo perl /common/iprscan/bin/index_data.pl -f match_complete.xml -inx -iforce -v

WARNING: No data path specified, defaulting to /common/iprscan/data/!
Creating Tool Object...Done
Parsing Configuration file...Done

========================================================
        Indexing for match_complete.xml
========================================================
Creating Index::IprMatches module ... DONE

... File /common/iprscan/data//match_complete.xml being reindexed ...
*****
WARNING !! : Index file /common/iprscan/data//match_complete.xml.inx not here. Index file will be created during indexing.
*****
Building index from file /common/iprscan/data//match_complete.xml, may take long time ... DONE

Creating Index file for datafile /common/iprscan/data//match_complete.xml ... DONE


*********************************************************
- Indexing for match_complete.xml DONE
*********************************************************
gems:/common/iprscan/data local$ sudo su
Password:
gems:/common/iprscan/data root# su - www
gems:~ www$ /common/iprscan/bin/iprscan -cli -i test.seq -iprlookup -goterms
SUBMITTED iprscan-20061229-18552349

FAILED
        Iprscan job iprscan-20061229-18552349 failed and exit with status code 65280
/common/iprscan/bin/iprscan: /common/iprscan/tmp/20061229/iprscan-20061229-18552349/iprscan-20061229-18552349.xml unavailable : No such file or directory
Can't load '/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle' for module XML::Parser::Expat: dlopen(/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle, 1): Library not loaded: /usr/local/lib/libexpat.1.dylib
  Referenced from: /RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
  Reason: image not found at /System/Library/Perl/5.8.6/darwin-thread-multi-2level/DynaLoader.pm line 230.
 at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14
Compilation failed in require at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14.
BEGIN failed--compilation aborted at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 18.
Compilation failed in require at /common/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 34.
BEGIN failed--compilation aborted at /common/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 34.
Compilation failed in require at /common/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
BEGIN failed--compilation aborted at /common/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
Compilation failed in require at /common/iprscan/bin/iprscan_wrapper.pl line 50.
BEGIN failed--compilation aborted at /common/iprscan/bin/iprscan_wrapper.pl line 50.
gems:~ www$ 


THOUGH IT RUNS AND THEN FAILS TO FIND OUTPUT WHEN I USE THE OLD iprscan:

cd /common/apps/iprscan
/common/apps/iprscan/bin/iprscan -cli -i test.seq -iprlookup -goterms

>>>...
queue->check(iprscan, iprscan-20061229-19051361)
Res: 2
Job ID: iprscan-20061229-19051361
(Queue) STDOUT:

/Volumes/gemshd4/common/apps/iprscan/tmp/20061229/iprscan-20061229-19051361/iprscan-20061229-19051361.exitcode
8 /Volumes/gemshd4/common/apps/iprscan/tmp/20061229/iprscan-20061229-19051361/iprscan-20061229-19051361.exitcode


(Queue) STDERR:



queue->check(iprscan, iprscan-20061229-19051361)
Res: 3
Job ID: iprscan-20061229-19051361

FAILED
        Iprscan job iprscan-20061229-19051361 failed and exit with status code 256
/common/apps/iprscan/bin/iprscan: /Volumes/gemshd4/common/apps/iprscan/tmp/20061229/iprscan-20061229-19051361/iprscan-20061229-19051361.xml unavailable : No such file or directory
supervise: doRawResults: failed to create XML output : No such file or directory<br><br>To help us solve the problem, please copy and paste this error<br>and email it to the administrator <a href="mailto:syoung@rsmas.miami.edu?subject=Error in InterProScan job iprscan-20061229-19051361">syoung@rsmas.miami.edu</a>.<br><br>Sorry about the inconvenience and thank you for using InterProScan.<br><br>gems:/common/apps/iprscan www$ gems:/common/apps/iprscan www$
<<<


Hi Chris,

I indexed match_complete.xml and ran the new iprscan:

sudo perl /common/iprscan/bin/index_data.pl -f match_complete.xml -inx -iforce -v

WARNING: No data path specified, defaulting to /common/iprscan/data/!
Creating Tool Object...Done
Parsing Configuration file...Done

========================================================
        Indexing for match_complete.xml
========================================================
Creating Index::IprMatches module ... DONE

... File /common/iprscan/data//match_complete.xml being reindexed ...
*****
WARNING !! : Index file /common/iprscan/data//match_complete.xml.inx not here. Index file will be created during indexing.
*****
Building index from file /common/iprscan/data//match_complete.xml, may take long time ... DONE

Creating Index file for datafile /common/iprscan/data//match_complete.xml ... DONE


*********************************************************
- Indexing for match_complete.xml DONE
*********************************************************
gems:/common/iprscan/data local$ sudo su
Password:
gems:/common/iprscan/data root# su - www
gems:~ www$ /common/iprscan/bin/iprscan -cli -i test.seq -iprlookup -goterms
SUBMITTED iprscan-20061229-18552349

FAILED
        Iprscan job iprscan-20061229-18552349 failed and exit with status code 65280
/common/iprscan/bin/iprscan: /common/iprscan/tmp/20061229/iprscan-20061229-18552349/iprscan-20061229-18552349.xml unavailable : No such file or directory
Can't load '/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle' for module XML::Parser::Expat: dlopen(/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle, 1): Library not loaded: /usr/local/lib/libexpat.1.dylib
  Referenced from: /RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
  Reason: image not found at /System/Library/Perl/5.8.6/darwin-thread-multi-2level/DynaLoader.pm line 230.
 at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14
Compilation failed in require at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14.
BEGIN failed--compilation aborted at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 18.
Compilation failed in require at /common/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 34.
BEGIN failed--compilation aborted at /common/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 34.
Compilation failed in require at /common/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
BEGIN failed--compilation aborted at /common/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
Compilation failed in require at /common/iprscan/bin/iprscan_wrapper.pl line 50.
BEGIN failed--compilation aborted at /common/iprscan/bin/iprscan_wrapper.pl line 50.
gems:~ www$ 


THOUGH IT RUNS AND THEN FAILS TO FIND OUTPUT WHEN I USE THE OLD iprscan:

cd /common/apps/iprscan
/common/apps/iprscan/bin/iprscan -cli -i test.seq -iprlookup -goterms

>>>...
queue->check(iprscan, iprscan-20061229-19051361)
Res: 2
Job ID: iprscan-20061229-19051361
(Queue) STDOUT:

/Volumes/gemshd4/common/apps/iprscan/tmp/20061229/iprscan-20061229-19051361/iprscan-20061229-19051361.exitcode
8 /Volumes/gemshd4/common/apps/iprscan/tmp/20061229/iprscan-20061229-19051361/iprscan-20061229-19051361.exitcode


(Queue) STDERR:



queue->check(iprscan, iprscan-20061229-19051361)
Res: 3
Job ID: iprscan-20061229-19051361

FAILED
        Iprscan job iprscan-20061229-19051361 failed and exit with status code 256
/common/apps/iprscan/bin/iprscan: /Volumes/gemshd4/common/apps/iprscan/tmp/20061229/iprscan-20061229-19051361/iprscan-20061229-19051361.xml unavailable : No such file or directory
supervise: doRawResults: failed to create XML output : No such file or directory<br><br>To help us solve the problem, please copy and paste this error<br>and email it to the administrator <a href="mailto:syoung@rsmas.miami.edu?subject=Error in InterProScan job iprscan-20061229-19051361">syoung@rsmas.miami.edu</a>.<br><br>Sorry about the inconvenience and thank you for using InterProScan.<br><br>gems:/common/apps/iprscan www$ gems:/common/apps/iprscan www$
<<<


If you get a chance to take another look over the weekend, please let me know what you get.

Cheers,

Stuart.



+++++++++++++++++</entry>



<entry [INDEX THE NEW IPRSCAN DATA FILES] (E.G., IF GET ERROR 'FATAL: Failed to open HMM database /Users/young/FUNNYBASE/bin/iprscan/data/Pfam.bin'>


IN IPRSCAN OUTPUT.)

perl /Users/young/FUNNYBASE/bin/iprscan/bin/index_data.pl -f Pfam inx iforce -bin -v

# DIDN'T DO THIS AS THE FILE IS 4G:
# sudo cp /common/iprscan/data/match_complete.xml /common/iprscan/data/match_complete.xml.bkp1

sudo perl /common/iprscan/bin/index_data.pl -f match_complete.xml -inx -iforce -v


( NB: Core InterPro entries are publicly released in two ASCII (text) flat files, written in XML: 
*  match.xml - contains all UniProtKB protein sequence matches 
*  match_complete.xml - contains all UniProtKB sequence matches and  those that have not yet been integrated into InterPro. )

+++++++++++++++++</entry>



<entry [Fri Dec 29 18:16:21 EST 2006] CHRIS DWAN REINSTALL OF iprscan>



Stuart,

I just finished unpacking the data files into /common/iprscan/data,  
but I have to get on the road now, and I won't be back online  
consistently until Tuesday.  I'll try to log in over the weekend and  
figure out what's up, but I can't promise that I'll have a chance to  
do that.

Right now, I'm testing the system like this:

sudo su
su - www

This gets me into a shell as user "www".

Then I run iprscan like this:

/common/iprscan/bin/iprscan -cli -i test.seq -iprlookup -goterms

At that point, I get this error:

iprscan submission failed: checkSequences: Cannot get raw entry from  
iprmatches: getRawEntryFromIprMatches: query iprmatches failed:  
checkIndex: No such indexed file /common/iprscan/data/ 
match_complete.xml.inx at /common/iprscan/lib/Dispatcher/Tool/ 
InterProScan.pm line 3883.<br>

Which indicates that I need to index the data, but I need to look up  
the exact command to do that.  I recall that it involves the command  
"index_data.pl", but not the command line arguments.

Sorry that this is taking so long.  I really think that we're nearly  
there, and I apologize for having to leave before it's completely done.

Talk to you Tuesday, if not earlier.

-Chris Dwan
  The Bioteam




On Dec 29, 2006, at 5:03 PM, stuart young via RT wrote:

>
> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
>
> Hi Chris,
>
> I'm back. Any luck with running iprscan?
>
> Stuart.
>
>
>> From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
>> Reply-To: inquiry-support@bioteam.net
>> To: youngstuart@hotmail.com
>> Subject: Re: [bioteam.net #13817] install submit host + interproscan
>> Date: Fri, 29 Dec 2006 14:12:23 -0500
>>
>>
>> I'm having a heck of a time getting the Interpro datasets
>> downloaded.  I keep getting "corrupted MAC on input" messages.  Is
>> there a trick involved in moving large amounts of data around on this
>> machine's network?
>>
>> -Chris Dwan
>>   The BioTeam
>>
>> On Dec 29, 2006, at 2:06 PM, stuart young via RT wrote:
>>
>>>
>>> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
>>>
>>> Hi Chris,
>>>
>>> Thanks for getting it so far. I had no inkling of the OS issue. Let
>>> me know
>>> once you've had a chance to test iprscan with the new data set.
>>>
>>> Cheers,
>>>
>>> Stuart.
>>>
>>>
>>>
>>>> From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
>>>> Reply-To: inquiry-support@bioteam.net
>>>> To: youngstuart@hotmail.com
>>>> Subject: Re: [bioteam.net #13817] install submit host +  
>>>> interproscan
>>>> Date: Fri, 29 Dec 2006 09:16:16 -0500
>>>>
>>>>
>>>>
>>>> Stuart,
>>>>
>>>> There were a number of tricky bits involved in getting interpro  
>>>> scan
>>>> working on your cluster, but I'm certainly making progress, and you
>>>> should be good to go sometime today.
>>>>
>>>> ##
>>>>
>>>> I did a clean install from the source in /common/iprscan, to make
>>>> sure that you and I wouldn't step on each other as we debugged.
>>>> Please don't touch that install until I've verified that it's  
>>>> working
>>>> right.
>>>>
>>>> ##
>>>>
>>>> I submitted as the user "www", since that one is known to work okay
>>>> with grid engine and all that.  We've agreed to leave the user
>>>> management issues to another day.
>>>>
>>>> ##
>>>>
>>>>   I added a line to /common/sge/default/common/sge_aliases
>>>>
>>>> /Volumes/gemshd4/ * * /
>>>>
>>>> This means that SGE won't "help" us by changing "/common" into "/
>>>> Volumes/gemshd4/common", for example.  This is necessary for any  
>>>> SGE
>>>> jobs run from the /common or /Users directories to work properly.
>>>>
>>>> ##
>>>>
>>>> Updated the iprscan installation per this thread on bioclusters:
>>>>
>>>> http://bioinformatics.org/pipermail/bioclusters/2006-September/
>>>> 003084.html
>>>>
>>>> 3 - Change the following line in iprscan/sge6/conf
>>>>
>>>> asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e
>>>> /dev/null [%env IPRSCAN_HOME]/conf/sge6-test.sh [%toolcmd]
>>>>
>>>> To this:
>>>> asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e
>>>> /dev/null [%toolcmd]
>>>>
>>>> ##
>>>>
>>>> Added an additional tag to that same config file:  "-v PERL5LIB=/
>>>> RemotePerl" to make sure that we would pick up the perl include  
>>>> path
>>>> on the nodes.
>>>>
>>>> ##
>>>>
>>>> I started encountering these errors:
>>>>
>>>> Can't load '/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/
>>>> Parser/Expat/Expat.bundle' for module XML::Parser::Expat: dlopen(/
>>>> RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/
>>>> Expat.bundle, 1): Library not loaded: /usr/local/lib/libexpat. 
>>>> 1.dylib
>>>>
>>>> After messing with the dynamic libraries for a while, I realized  
>>>> that
>>>> jobs went okay on the portal, but not on the compute nodes, so I
>>>> checked the OS versions on the nodes, and they were all over the
>>>> map.  The portal is at 10.4.7, and the nodes were at 10.4.3.  I  
>>>> took
>>>> the liberty of updating the nodes to 10.4.8.
>>>>
>>>> ##
>>>>
>>>> After all of this, I was able to submit jobs and get a  
>>>> comprehensible
>>>> error message:
>>>>
>>>> iprscan submission failed: checkSequences: Cannot get raw entry  
>>>> from
>>>> iprmatches: getRawEntryFromIprMatches: query iprmatches failed:
>>>> checkIndex: No such indexed file /common/iprscan/data/
>>>> match_complete.xml.inx at /common/iprscan/lib/Dispatcher/Tool/
>>>> InterProScan.pm line 3883.<br>
>>>>
>>>> So now, I'm downloading a fresh copy of the interpro datasets.   
>>>> That
>>>> is slated to take an hour or so.  When it completes, I'll index  
>>>> them
>>>> and continue working.
>>>>
>>>> -Chris Dwan
>>>>   The BioTeam
>>>>
>>>
>>> ________________


+++++++++++++++++</entry>



<entry [Thu Dec 28 14:26:31 EST 2006] MSG: inquiry-support@bioteam.net>



Hi Chris,

I hadn't noticed that (which node is it?) but I'd actually really like to get to the bottom of this problem first:

1. I just reinstalled iprscan to run without SGE support.

2. I can run iprscan on the head node (gems).

3. But, when I run on node001, I get the following error:

sudo /common/apps/iprscan/bin/iprscan -cli -i /Users/local/FUNNYBASE/bin/iprscan/funnybase9.seq.3.1.fasta -o /Users/local/FUNNYBASE/bin/iprscan/funnybase9.seq.3.1.xml -format xml -goterms -ipr -verbose -seqtype n -trlen 30 -appl hmmpfam
>>>
$VAR1 = bless( {}, 'Dispatcher::Queue' );
$VAR1 = 'iprscan';
$VAR1 = [];
$VAR1 = undef;
$VAR1 = {
          'stdin' => '/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.params',
          'stderr' => '/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.errors',
          'exitcode' => '/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.exitcode'
        };
$VAR1 = undef;
$VAR1 = 'iprscan-20061228-11313538';
queue->submitWithArgs()
Res: 1
Job ID: iprscan-20061228-11313538
SUBMITTED iprscan-20061228-11313538
(Queue) STDOUT:

/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.exitcode


(Queue) STDERR:

gems.rsmas.miami.edu: Connection refused


here
queue->check(iprscan, iprscan-20061228-11313538)
Res: 0
Job ID: iprscan-20061228-11313538
/common/apps/iprscan/bin/iprscan: failed to check local job iprscan-20061228-11313538:
/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.exitcode
gems.rsmas.miami.edu: Connection refused

<<<

(The extra iprscan output is added for debugging.)

Stuart.


>From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
>Reply-To: inquiry-support@bioteam.net
>To: youngstuart@hotmail.com
>Subject: Re: [bioteam.net #13817] install submit host + interproscan
>Date: Thu, 28 Dec 2006 13:00:32 -0500
>
>
>Stuart,
>
>No luck so far.  You're one task down on the list.  The current one
>is to figure out why one of our core servers keeps powering itself off.
>
>-Chris Dwan
>   The Bioteam


+++++++++++++++++</entry>



<entry [Thu Dec 28 12:57:24 PST 2006] TEST INSTALLATION>



>>Installation is completed.  Please test your installation by running on the command line:

/Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -i /Volumes/gemshd4/common/apps/iprscan/test.seq -o /Volumes/gemshd4/common/apps/iprscan/test.out -format raw -goterms -ipr -verbose

and comparing it with the merged.raw file that is supplied.
For help and options on the commandline, run /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -h

>>>...
/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-12580190/iprscan-20061228-12580190.exitcode
not ready yet

[REPEATED]
...

queue->check(iprscan, iprscan-20061228-12580190)
Res: 2
Job ID: iprscan-20061228-12580190
.(Queue) STDOUT:

/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-12580190/iprscan-20061228-12580190.exitcode
not ready yet


(Queue) STDERR:



queue->check(iprscan, iprscan-20061228-12580190)
Res: 2
Job ID: iprscan-20061228-12580190
.(Queue) STDOUT:

/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-12580190/iprscan-20061228-12580190.exitcode
8 /Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-12580190/iprscan-20061228-12580190.exitcode


(Queue) STDERR:



queue->check(iprscan, iprscan-20061228-12580190)
Res: 3
Job ID: iprscan-20061228-12580190

DONE

=== ********************************************************** ===

Some jobs failed! Please see the report file
/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-12580190/iprscan-20061228-12580190.report
You can try to relaunch failed jobs using ResubmitJobs.pl
script located in your iprscan/bin directory as follow :
./ResubmitJobs.pl -r /Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-12580190/iprscan-20061228-12580190.report

Then old results will be concatenated with new one

=== ********************************************************** ===


<h1>Software error:</h1>
<pre>Could not open /Volumes/gemshd4/common/apps/iprscan/test.out : Permission denied at /Volumes/gemshd4/common/apps/iprscan/bin/iprscan line 349.</pre>
<p>
For help, please send mail to this site's webmaster, giving this error message 
and the time and date of the error.

</p>
[Thu Dec 28 13:00:10 2006] iprscan: Could not open /Volumes/gemshd4/common/apps/iprscan/test.out : Permission denied at /Volumes/gemshd4/common/apps/iprscan/bin/iprscan line 349.
gems:/common/apps/iprscan/bin local$ 
<<<


REPEATED WITH sudo:

sudo /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -i /Volumes/gemshd4/common/apps/iprscan/test.seq -o /Volumes/gemshd4/common/apps/iprscan/test.out -format raw -goterms -ipr -verbose

>>>...
queue->check(iprscan, iprscan-20061228-13015847)
Res: 3
Job ID: iprscan-20061228-13015847

DONE

=== ********************************************************** ===

Some jobs failed! Please see the report file
/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-13015847/iprscan-20061228-13015847.report
You can try to relaunch failed jobs using ResubmitJobs.pl
script located in your iprscan/bin directory as follow :
./ResubmitJobs.pl -r /Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-13015847/iprscan-20061228-13015847.report

Then old results will be concatenated with new one

=== ********************************************************** ===
<<<

REPEATED WITH XML OUTPUT:

sudo /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -i /Volumes/gemshd4/common/apps/iprscan/test.seq -o /Volumes/gemshd4/common/apps/iprscan/test.out.XML -format xml -goterms -ipr -verbose


REPEATED WITH NUCLEOTIDE SEQUENCE, RAW OUTPUT:

(E.G.: iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose)

sudo /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -i /Volumes/gemshd4/common/apps/iprscan/test.nuc.seq -o /Volumes/gemshd4/common/apps/iprscan/test.nuc.out -format raw -goterms -ipr -verbose -seqtype n

REPEATED WITH NUCLEOTIDE SEQUENCE, XML OUTPUT:

sudo /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -i /Volumes/gemshd4/common/apps/iprscan/test.nuc.seq -o /Volumes/gemshd4/common/apps/iprscan/test.nuc.out.xml -format xml -goterms -ipr -verbose -seqtype n

TEST USING ONLY hmmpfam ON FUNNYBASE9 SEQUENCES WITH TRANSCRIPT LENGTH THRESHOLD:

THRESHOLD = 20  (-trlen 20):
sudo /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -i /Users/local/FUNNYBASE/bin/iprscan/funnybase9.seq.1.1.fasta -o /Users/local/FUNNYBASE/bin/iprscan/funnybase9.seq.1.1.trlen20.xml -format xml -goterms -ipr -verbose -seqtype n -trlen 20 -appl hmmpfam

THRESHOLD = 30  (-trlen 30):
sudo /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -i /Users/local/FUNNYBASE/bin/iprscan/funnybase9.seq.1.1.fasta -o /Users/local/FUNNYBASE/bin/iprscan/funnybase9.seq.1.1.trlen30.xml -format xml -goterms -ipr -verbose -seqtype n -trlen 30 -appl hmmpfam

WORKS OKAY!

BUT IPRSCAN DOES NOT WORK ON NODE 001:

sudo ln -s / /Volumes/gemshd4
cd /Users/local/FUNNYBASE/bin/iprscan
sudo /common/apps/iprscan/bin/iprscan -cli -i /Users/local/FUNNYBASE/bin/iprscan/funnybase9.seq.3.1.fasta -o /Users/local/FUNNYBASE/bin/iprscan/funnybase9.seq.3.1.xml -format xml -goterms -ipr -verbose -seqtype n -trlen 30 -appl hmmpfam
>>>
$VAR1 = bless( {}, 'Dispatcher::Queue' );
$VAR1 = 'iprscan';
$VAR1 = [];
$VAR1 = undef;
$VAR1 = {
          'stdin' => '/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.params',
          'stderr' => '/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.errors',
          'exitcode' => '/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.exitcode'
        };
$VAR1 = undef;
$VAR1 = 'iprscan-20061228-11313538';
queue->submitWithArgs()
Res: 1
Job ID: iprscan-20061228-11313538
SUBMITTED iprscan-20061228-11313538
(Queue) STDOUT:

/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.exitcode


(Queue) STDERR:

gems.rsmas.miami.edu: Connection refused


here
queue->check(iprscan, iprscan-20061228-11313538)
Res: 0
Job ID: iprscan-20061228-11313538
/common/apps/iprscan/bin/iprscan: failed to check local job iprscan-20061228-11313538:
/Volumes/gemshd4/common/apps/iprscan/tmp/20061228/iprscan-20061228-11313538/iprscan-20061228-11313538.exitcode
gems.rsmas.miami.edu: Connection refused

<<<


CREATED SCRIPT iprscan.timer.pl WHICH DOES:

1. TIMES IPRSCAN BASED ON DIFFERENT VALUES FOR trlen OPTION
2. GET THE NUMBER OF INPUT SEQUENCES (IN *.input FILE)





+++++++++++++++++</entry>



<entry [Thu Dec 28 12:56:08 PST 2006] REINSTALLED WITHOUT 'USE QUEUE' OPTION>



gems:/common/apps/iprscan local$ sudo ./Config.pl 
Password:

####################################################################################################
#                Welcome to the InterProScan v4.2 installation script.
#
#  Tips:
#     - If you are happy with the suggestion between [], just press <enter>
#     - Whatever is shown between '/' and '/' is considered a valid input pattern
####################################################################################################


    Reconfigure everything? (first time install) [n] /(y|n)/? : y

! Tip:
!
! InterProScan needs to know where it is installed and where perl is installed
!
    Do you want to set paths to perl and the installation directory? [y] /y|n/? : 

! Tip:
!
! If your servers are using a shared file system, such as NFS, InterProScan is able to utilise this
! and perform distributed computing across multiple servers
! You will need set up the full UNIX path to the installation directory.
! You also need to ensure that the InterProScan installation is on a SHARED DISK.
!
    Please enter the full path for the InterProScan installation [/Volumes/gemshd4/common/apps/iprscan] /.+/? : 
    Do you want to set another Perl command in place of [/usr/bin/perl]? [n] /y|n/? : 
>>Changing Perl path in scripts ... >>DONE

>>Checking if your Perl installation has the modules required by InterProScan...
>>All modules needed are already installed.


! Tip:
!
! To cope with bulk jobs and to enable efficient parallelization, InterProScan splits input files
! with FASTA formatted sequences into smaller parts (chunks). Default size is "10" sequences per chunk
!
    Do you want to setup chunk size [y] /(y|n)/? : n


! Tip:
!
! You can configure InterProScan to limit the number of input sequences (protein & nucleotide),
! set a maximum length for nucleotide sequences and a minimum length for protein sequences.
! You can also configure the minimum length for a translated orf and use the codon table value
! to translate the dna/rna sequence into six frames (http://www.ebi.ac.uk/cgi-bin/mutations/trtables.cgi).
!
    Do you want to configure this? [y] /(y|n)/? : n


! Tip:
!
! InterProScan currently encorporates searching against 13 databases. You have the choice whether or not to
! set up a particular search tool against a database and how exactly that application will be configured to run
! in this next section. You will configure the queue systems (if any) you will use, first
!
    Do you want to setup applications (if you don't, no applications will be included in InterProScan by default)? [y] /(y|n)/? : y

! Tip:
!
! InterProScan is able to execute the applications via a UNIX QUEUING system which has an INTERACTIVE mode (such as LSF).
! Currently, we mainly support LSF (which is used at EBI) however, we also provide configuration files for other systems
! which are listed in the documentation
!
    Do you wish to use a queue system? [n] /(y|n)/? : n
>>Checking your local architecture for InterProScan compatibility

! Tip:
! Blastprodom (a wrapper script on top of Blast package) is used to search against PRODOM families
    Do you want to use blastprodom ? [y] /(y|n)/? : y
!
! PLEASE NOTE:
! InterProScan can be launched through a cluster machine using RSH. To be able to utilise this
! feature, all your machines must be visible to each other (see the .rhost file) and be mounted on
! a shared file system, such as NFS
! (If you are installing InterProScan for use on only one machine please ignore the above message)
!
    
Please enter the execution host name of blastprodom [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of blastprodom [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of blastprodom [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of blastprodom [gems.rsmas.miami.edu] /\S+/? : 
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of blastprodom [gems.rsmas.miami.edu] /\S+/? : gems.rsmas.miami.edu
ln: binaries/Darwin: File exists
WARNING: Unable to create symbolic link 'ln -s Darwin binaries': 
Please correct the error or create the symbolic link manually if necessary

    
Please enter the execution host name of blastprodom [gems.rsmas.miami.edu] /\S+/? :     

! Tip:
! Coils is used to predict coiled-coil segments with the algorithm of Lupas et al.
    Do you want to use coils ? [y] /(y|n)/? : y
    
Please enter the execution host name of coils [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Gene3d Gene3D is supplementary to the CATH database. This protein sequence database contains proteins from complete genomes which have been clustered into protein families and annotated with CATH domains, Pfam domains and functional information from KEGG, GO, COG, Affymetrix and STRINGS.
    Do you want to use gene3d ? [y] /(y|n)/? : 
    
Please enter the execution host name of gene3d [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmpanther (hmmpfam from HMMER 2.3.2 package) PANTHER classifies proteins into families and specific functional subfamilies, which are then mapped to ontology terms and pathways.
    Do you want to use hmmpanther ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmpanther [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmpir PIR produces the Protein  Sequence Database (PSD) of functionally annotated protein sequences, which grew out of the Atlas of Protein Sequence and Structure (1965-1978) edited by Margaret Dayhoff and has been incorporated into an integrated knowledge base system of value-added databases and analytical tools.
    Do you want to use hmmpir ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmpir [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmpfam (hmmpfam from HMMER 2.3.2 package) is used to search against the Pfam HMM (Profile hidden Markov models) database
    Do you want to use hmmpfam ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmpfam [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmsmart (hmmpfam from HMMER 2.3.2 package) is used to search against the Smart HMM (Profile hidden Markov models) database
    Do you want to use hmmsmart ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmsmart [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Hmmtigr (hmmpfam from HMMER 2.3.2 package) is used to search against TIGRFAMs collection of HMMs (Profile hidden Markov models)
    Do you want to use hmmtigr ? [y] /(y|n)/? : 
    
Please enter the execution host name of hmmtigr [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Fprintscan (FingerPRINTScan) is used to search against the PRINTS collection of protein signatures
    Do you want to use fprintscan ? [y] /(y|n)/? : 
    
Please enter the execution host name of fprintscan [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Scanregexp (ScanProsite) is used to search against the PROSITE patterns collection and verify the  matches by statistically significant CONFIRM patterns
    Do you want to use scanregexp ? [y] /(y|n)/? : 
    
Please enter the execution host name of scanregexp [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Profilescan (ProfileScan (pfscan & ps_scan.pl) from Pftools 2.2) package is used  to search against the PROSITE profiles collection
    Do you want to use profilescan ? [y] /(y|n)/? : 
    
Please enter the execution host name of profilescan [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Superfamily A structural classification of proteins database for the investigation of sequences and structures.
    Do you want to use superfamily ? [y] /(y|n)/? : 
    
Please enter the execution host name of superfamily [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Seg is used for identifying and masking segments of low compositional complexity in amino acid sequences
    Do you want to use seg ? [y] /(y|n)/? : 
    
Please enter the execution host name of seg [gems.rsmas.miami.edu] /\S+/? : 

! Tip:
! Signalp predicts the presence and location of signal peptide cleavage sites, using eukaryotic HMM (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use signalp ? [n] /(y|n)/? : 

! Tip:
! Tmhmm is used for prediction of transmembrane helices in proteins using a Hidden Markov Model (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use tmhmm ? [n] /(y|n)/? : 

! Tip:
!
! InterProScan can use taxonomy-based filtering of results which uses the InterPro taxonomy listed for each InterPro
! entry to hide hits not relevant to that taxon.
!
    Do you want to make it available for users? [y] /y|n/? : 

! Tip:
!
! You can specify an email address for the administrator. By default it will be you (root)
! Administrators receive an email when a problem occurs in InterProScan.
!
    Do you want to set an administrator email address? [y] /y|n/? : 
    Please enter the email address of the administrator: [] /[\w\.\-]+\@[\w\.\-]+/? : syoung@rsmas.miami.edu
>>Writing to the configuration files ... 
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/blastprodom.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/coils.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/fprintscan.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/gene3d.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmpanther.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmpfam.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmpir.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmsmart.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmtigr.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/profilescan.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/scanregexp.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/seg.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/superfamily.conf ... >>DONE
>>Setting File /Volumes/gemshd4/common/apps/iprscan/conf/iprscan.conf ... >>DONE

! Tip:
!
! InterProScan can be launched through a web interface. It can also be launched through secure HTTP (https)
!
    Do you want to run InterProScan using a web interface? [n] /y|n/? : y
    Do you want to run InterProScan through a secure HTTP server? [n] /y|n/? : n
    Enter the name of your server. You can specify a specific port to listen to. (e.g. "foo.bar.com:4000") [foobar.com] /[\w\.\_\-\:\d]+/? : gems.rsmas.miami.edu
!
! Please add the following lines to your web server configuration (httpd.conf):
#--------------------------------------
 Alias /doc/ "/Volumes/gemshd4/common/apps/iprscan/doc/html/"
 Alias /images/ "/Volumes/gemshd4/common/apps/iprscan/images/"
 Alias /tmp/ "/Volumes/gemshd4/common/apps/iprscan/tmp/"

 ScriptAlias /iprscan/ "/Volumes/gemshd4/common/apps/iprscan/bin/"

 <Directory "/Volumes/gemshd4/common/apps/iprscan/images">
   Options Indexes MultiViews
   AllowOverride None
   Order allow,deny
   Allow from all
 </Directory>

 <Directory "/Volumes/gemshd4/common/apps/iprscan/tmp/">
   Options FollowSymLinks Includes SymLinksIfOwnerMatch
    IndexIgnore */.??* *~ *# */HEADER* */README* */RCS
    AllowOverride AuthConfig Limit FileInfo

    Order deny,allow
    Deny from all
    Allow from .rsmas.miami.edu
 </Directory>
#--------------------------------------
! Be sure that 'apache' or 'httpd' virtual user (uid/gid) have right to read/write into your /Volumes/gemshd4/common/apps/iprscan iprscan directory.
! Otherwise, if you are running a standalone http(s) web server, be sure also the user running it have to enough
! rights to read/wrtie /Volumes/gemshd4/common/apps/iprscan iprscan directory
! If you encoutered problem with your web server running iprscan, read the FAQ or contact your system administrator.

! After restarting your web server try http://gems.rsmas.miami.edu/iprscan/iprscan
!
>>Indexing datafile ... 
No specific file(s) given, indexing all files:

Pfam
prints.pval
prodom.ipr
Pfam-C
smart.desc
sf_hmm
sf.seq
Pfam-A.seed
superfamily.hmm
smart.thresholds
smart.HMMs
match.xml
interpro.xml
TIGRFAMs_HMM.LIB
Gene3D.hmm


No specific file(s) given, converting all files:

Pfam
sf_hmm
superfamily.hmm
smart.HMMs
sf_hmm_sub
TIGRFAMs_HMM.LIB
Gene3D.hmm

WARNING: Some problems occured when trying to index the data files.
 Please fix the error below and index your datafiles using the /Volumes/gemshd4/common/apps/iprscan/bin/index_data.pl script.
 For help, type './index_data.pl -h' 
errormsg: ERROR: File [/Volumes/gemshd4/common/apps/iprscan/data/sf.seq] not found : No such file or directory at /Volumes/gemshd4/common/apps/iprscan/bin/index_data.pl line 217.
>>DONE

! Tip:
!
!  We would appreciate it if you would register your installation of InterProScan.  This will allow us to notify you
! of bug fixes and new releases of the software. We will not use your email address for any other purpose.
!
    Would you like to register InterProScan? [y] /(y|n)/? : n

>>Installation is completed.  Please test your installation by running on the command line:

        /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -i /Volumes/gemshd4/common/apps/iprscan/test.seq -o /Volumes/gemshd4/common/apps/iprscan/test.out -format raw -goterms -ipr

and comparing it with the merged.raw file that is supplied.
For help and options on the commandline, run /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -h

+++++++++++++++++</entry>



<entry [Tue Dec 26 19:57:54 PST 2006] REVISITING RECURRING execvp ERROR>



/common/sge/default/common/settings.sh
echo $SGE_ROOT
cd /Volumes/gemshd4/common/apps/iprscan/bin
# sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose
sudo iprscan -cli -appl hmmpfam -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose


gems:/Volumes/gemshd4/common/apps/iprscan/bin local$ qstat -j 986
job_number:                 986
exec_file:                  job_scripts/986
submission_time:            Tue Dec 26 19:56:03 2006
owner:                      root
uid:                        0
group:                      wheel
gid:                        0
sge_o_home:                 /Users/local
sge_o_log_name:             root
sge_o_path:                 /Users/local/apps/cytoscape/PLUGINS:/usr/lib/perl5/site_perl:/usr/share/apps/ActiveTcl/bin:/usr/local/mysql/bin:/usr/local/bin:/usr/share/apps/komodo:/sw/bin:/Users/local/FUNNYBASE/bin:/common/sge/bin/darwin:/bin:/sbin:/usr/bin:/usr/sbin:/common/mpich-1.2.7/ch_p4/bin:/common/bin:/common/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin:/usr/local/biotools/bin:/Applications/bioinf/phylip/exe:/Applications/bioinf/t_coffee:/usr/local/biotools/bin
sge_o_shell:                /bin/bash
sge_o_workdir:              /Volumes/gemshd4/common/apps/iprscan/bin
sge_o_host:                 gems
account:                    sge
stderr_path_list:           /dev/null
mail_list:                  root@gems.rsmas.miami.edu
notify:                     FALSE
job_name:                   iprscan-20061226-19560362
stdout_path_list:           /dev/null
jobshare:                   0
hard_queue_list:            all.q
env_list:                   
script_file:                /Volumes/gemshd4/common/apps/iprscan/tmp/20061226/iprscan-20061226-19560362/iprscan-20061226-19560362.dcmd
error reason    1:          12/26/2006 17:24:40 [0:25288]: execvp(/Volumes/gemshd4/common/apps/iprscan/tmp/20061226/iprscan-2006
scheduling info:            job is in error state


SO CHECKED IN sge SETTINGS:

em /common/sge/default/common/settings.sh
>>>
SGE_ROOT=/common/sge; export SGE_ROOT

ARCH=`$SGE_ROOT/util/arch`
DEFAULTMANPATH=`$SGE_ROOT/util/arch -m`
MANTYPE=`$SGE_ROOT/util/arch -mt`

SGE_CELL=default; export SGE_CELL
SGE_QMASTER_PORT=701; export SGE_QMASTER_PORT
SGE_EXECD_PORT=702; export SGE_EXECD_PORT

if [ "$MANPATH" = "" ]; then
   MANPATH=$DEFAULTMANPATH
fi
MANPATH=$SGE_ROOT/$MANTYPE:$MANPATH; export MANPATH

PATH=$SGE_ROOT/bin/$ARCH:$PATH; export PATH
shlib_path_name=`$SGE_ROOT/util/arch -lib`
old_value=`eval echo '$'$shlib_path_name`
if [ x$old_value = x ]; then
   eval $shlib_path_name=$SGE_ROOT/lib/$ARCH
else
   eval $shlib_path_name=$SGE_ROOT/lib/$ARCH:$old_value
fi
export $shlib_path_name
unset ARCH DEFAULTMANPATH MANTYPE shlib_path_name
<<<

WHERE (ON genomics)

echo $SGE_ROOT

GIVES

/Users/young/FUNNYBASE/apps/sge

SO CHECKED messages:

cd /common/sge/default/spool
tail -80  node004/messages

>>>...
12/26/2006 17:31:08|execd|node004|E|shepherd of job 988.1 exited with exit status = 27
12/26/2006 17:31:08|execd|node004|E|can't open usage file "active_jobs/988.1/usage" for job 988.1: No such file or directory
12/26/2006 17:31:08|execd|node004|E|12/26/2006 17:31:08 [0:24932]: execvp(/Volumes/gemshd4/common/apps/iprscan/tmp/20061226/iprscan-20061226-20143842/iprscan-20061226-20143842.dcmd, "/Volumes/gemshd4/common/apps/iprscan/tmp/20061226/iprscan-20061226-20143842/iprscan-20061226-20143842.dcmd") failed: No such file or directory
12/26/2006 17:32:08|execd|node004|E|mailer had timeout - killing
12/26/2006 17:32:08|execd|node004|E|mailer exited with exit status = 1
<<<

SEARCHED ONLINE FOR 'sge "can't open usage file"':

>>>From: John Coldrick <jc@axyzfx.com>
Date: Mon, 28 Feb 2005 16:53:47 -0500
Content-Type: text/plain;
  charset="iso-8859-1"
Subject: [GE users] "can't open usage file" Spool Error

	Answering my own question..:P

	It seems that the standard output(which we trap) generated by the task was 
indeed colliding with file permission issues - it appears SGE outputs those 
files with a particular umask - 022 I believe.  Because the user numbers 
aren't mapped perfectly on all systems, it's sometimes is blocked in writing 
and this is the error you get.

	Until I get this resolved, is there a way to source a different umask for 
redirected output from tasks?  I'm guessing that despite the end user umask 
being set to a particular value, SGE ignores this and outputs it with a 
default root umask, despite the ownership of the file belonging to the user.

...<<<


GET LIST OF USERS:

niutil -list . /users

SHOWS sge AND nodody USERS.

POSSIBLE SOLUTION:

SET THE 'rw' PERMISSION IN NETINFO FOR NFS SHARES:

1. ADD "rw" TO 'opts' OPTION IN NetInfo Manager.
2. RESTART NetInfo DATABASES
3. RESTART NFS DAEMON ON NODES:

node002:/ root# sudo SystemStarter stop AutoFS
node002:/ root# sudo SystemStarter start AutoFS
OR:
sudo dsh -a SystemStarter stop AutoFS
sudo dsh -a SystemStarter start AutoFS

TEST WRITE PERMISSIONS WITH COPY IN /common/sge/default/common:
cd /common/sge/default/common
node002:/common/sge/default/common root# cp settings.sh settings.sh.bkp # OK
dlc-genomics:~ young$ sudo cp settings.sh settings.sh.bkp2 # OK

4. RESTART NODES AND MASTER (JUST IN CASE)

5. RUN iprscan AGAIN

BUT ALL NODES EXCEPT 012 AND 016 ARE SHOWING THE 'E' ERROR...

MAY BE PERMISSIONS PROBLEM WITH USER local:

man niload
>>>...
 -r     Load  entries  in  "raw" format, as generated by nidump -r.
...
 
 SYSTEM USAGE
       Most  processes  on  Mac  OS X access the information from the files in
       /etc and from NetInfo indirectly through the  system  library  and  the
       lookupd  daemon.   In some cases the files in /etc are consulted before
       NetInfo, making it unnecessary to copy  information  from  these  files
       into  NetInfo.   The  files  /etc/hosts,  /etc/networks, /etc/services,
       /etc/protocols, and /etc/rpcs are consulted before NetInfo.
       Additionally, the /etc/exports file is the primary source for NFS  file
       share  configuration.  niload no longer supports loading NFS file share
       information into NetInfo.

       See lookupd(8) for more information.

EXAMPLES
       "niload passwd . < /etc/passwd" loads the local /etc/passwd  file  into
       the local NetInfo database.

       "niload -d -r /locations ."  replaces the contents of /locations in the
       local domain with input given in nidump "raw" format.
<<<


man nidump
>>>...
EXAMPLES
       "nidump hosts ." dumps a hosts file from the local NetInfo domain.

       "nidump -r /locations /" dumps the /locations  directory  of  the  root
       domain.

       "nidump  -t -r /name=users/uid=530 trotter/network" dumps the directory
       for the user whose UID is 530.
SEE ALSO
       niload(8),   niutil(8),    netinfo(5),    aliases(5),    bootparams(5),
       bootptab(5),  exports(5),  fstab(5),  group(5),  hosts(5), networks(5),
       passwd(5), printcap(5), protocols(5), rpc(5), services(5)
<<<

BUT
nidump -t -r /name=users/uid=501 

AND OTHER PERMUTATIONS DON'T WORK.
THIS DOES WORK:

niutil -read . /users/local
>>>
hint: 
sharedDir: 
_writers_passwd: local
name: local
home: /Users/local
applemail: <?xml version="1.0" encoding="UTF-8"?>
<dict>
        <key>kAPOPRequired</key>
        <string>APOPNotRequired</string>
        <key>kAltMailStoreLoc</key>
        <string></string>
        <key>kAttributeVersion</key>
        <string>Apple Mail 1.0</string>
        <key>kAutoForwardValue</key>
        <string></string>
        <key>kIMAPLoginState</key>
        <string>IMAPAllowed</string>
        <key>kMailAccountLocation</key>
        <string>gems.rsmas.miami.edu</string>
        <key>kMailAccountState</key>
        <string>Enabled</string>
        <key>kPOP3LoginState</key>
        <string>POP3Allowed</string>
        <key>kUserDiskQuota</key>
        <string>0</string>
</dict>

authentication_authority: ;ShadowHash;
passwd: ********
_writers_hint: local
_writers_picture: local
_shadow_passwd: 
realname: local admin
uid: 501
shell: /bin/bash
generateduid: 10A607E9-0DA9-4730-A800-916F1EE35B63
naprivs: -1073741569
gid: 501
_writers_tim_password: local
_writers_realname: 
picture: /Library/User Pictures/Animals/Butterfly.tif
<<<





ADDITIONAL FOR IPRSCAN
======================
FIX PERMISSIONS IN iprscan AND SUBDIRECTORIES:

gems:/common/apps local$ sudo chown -R  sge:wheel iprscan
Password:




+++++++++++++++++</entry>



<entry [TRIED TO RUN iprscan AFTER REINSTALLING Config.pl] Thu Dec 21 15:39:45 PST 2006>



cd /Volumes/gemshd4/common/apps/iprscan/bin
sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose

SUBMITTED iprscan-20061221-16031363

PENDING   ...
FAILED
/usr/bin/iprscan: /common/apps/iprscan/tmp/20061221/iprscan-20061221-16031363/iprscan-20061221-16031363.xml unavailable : No such file or directory

OMIT -iprlookup OPTION:

sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -goterms -o ../test.nuc.out.xml -verbose

SAME ERROR.

BUT CAN RUN ARRAYJOB ON CLUSTER:

sudo ./funnybasearrayblast.pl -d funnybase9 -t refseq-dog   # OK





+++++++++++++++++</entry>



<entry [REINSTALLED iprscan IN THE /Volumes/gemshd4/common/aps DIRECTORY USING Config.pl:] Thu Dec 21 15:39:45 PST 2006>



(NB: DID NOT FINISH CONFIGURATION OF APACHE - DO LATER...)

cd /Volumes/gemshd4/common/apps/iprscan/bin
sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose
FILE: /Library/Perl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
Module->dl_load_flags: XML::Parser::Expat->dl_load_flags
SUBMITTED iprscan-20061221-15404223

PENDING   ..
FAILED
/usr/bin/iprscan: /common/apps/iprscan/tmp/20061221/iprscan-20061221-15404223/iprscan-20061221-15404223.xml unavailable : No such file or directory

ll /common/apps/iprscan/tmp/20061220/iprscan-20061220-15010067

gems:/Volumes/gemshd4/common/apps/iprscan local$ sudo ./Config.pl 

####################################################################################################
#                Welcome to the InterProScan v4.2 installation script.
#
#  Tips:
#     - If you are happy with the suggestion between [], just press <enter>
#     - Whatever is shown between '/' and '/' is considered a valid input pattern
####################################################################################################


    Reconfigure everything? (first time install) [n] /(y|n)/? : n

! Tip:
!
! InterProScan needs to know where it is installed and where perl is installed
!
    Do you want to set paths to perl and the installation directory? [n] /y|n/? : n

! Tip:
!
! To cope with bulk jobs and to enable efficient parallelization, InterProScan splits input files
! with FASTA formatted sequences into smaller parts (chunks). Default size is "10" sequences per chunk
!
    Do you want to setup chunk size [n] /(y|n)/? : n


! Tip:
!
! You can configure InterProScan to limit the number of input sequences (protein & nucleotide),
! set a maximum length for nucleotide sequences and a minimum length for protein sequences.
! You can also configure the minimum length for a translated orf and use the codon table value
! to translate the dna/rna sequence into six frames (http://www.ebi.ac.uk/cgi-bin/mutations/trtables.cgi).
!
    Do you want to configure this? [n] /(y|n)/? : n


! Tip:
!
! InterProScan currently encorporates searching against 13 databases. You have the choice whether or not to
! set up a particular search tool against a database and how exactly that application will be configured to run
! in this next section. You will configure the queue systems (if any) you will use, first
!
    Do you want to setup applications (if you don't, no applications will be included in InterProScan by default)? [n] /(y|n)/? : y

! Tip:
!
! InterProScan is able to execute the applications via a UNIX QUEUING system which has an INTERACTIVE mode (such as LSF).
! Currently, we mainly support LSF (which is used at EBI) however, we also provide configuration files for other systems
! which are listed in the documentation
!
    Do you wish to use a queue system? [n] /(y|n)/? : y
!
! VERY IMPORTANT NOTICE: 
! Applications from InterProScan will use a specific directory (iprscan/bin/binaries) to find application binaries
! and execute them.  'binaries' is a symbolic link pointing to a directory with a name that corresponds to the
! OS architecture (OSF1|Linux|AIX|SunOS|Darwin|IRIX).  If you have different architectures on any of the nodes
! used by your queueing system, you must make sure that the link they see points to the appropriate architecture
! directory.
! (e.g.) Host A is Linux, Host B is OSF1.  Be sure that, on host A, the binaries link points to a directory called
!       'Linux' and on host B, to 'OSF1'
!

Please choose one queueing system from the list. If you don't want to use this queueing system, just type '<enter>'

    Do you want to use pbs54? [n] /(y|n)/? : n
    Do you want to use lsf42? [n] /(y|n)/? : n
    Do you want to use sge6? [n] /(y|n)/? : y
!
! You can now configure global settings for your queue system for all the applications (such as a ! global cluster name and/or a global queue name)
!
    Do you want to set a global cluster name for all applications? [n] /(y|n)/? : cluster.private
WARNING: The information input is not valid (doesn't match /(y|n)/), please try again.
    Do you want to set a global cluster name for all applications? [n] /(y|n)/? : y
    Please enter the global cluster name you want to use for all applications [] /([^\s\;\,\:]+)/? : cluster.private
    Do you want to set a global queue name for all applications? [n] /(y|n)/? : y
    Please enter the name of the global queue you want to use for all applications [] /([^\s\;\,\:]+)/? : all.q
>>You will run InterProScan with sge6 queueing system using:
>>      -Global cluster name 'cluster.private'.
>>      -Global queue name 'all.q'.
    
Is all this information correct? [n] /y|n/? : y  
>>Checking your local architecture for InterProScan compatibility

! Tip:
! Blastprodom (a wrapper script on top of Blast package) is used to search against PRODOM families
    Do you want to use blastprodom ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Coils is used to predict coiled-coil segments with the algorithm of Lupas et al.
    Do you want to use coils ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Gene3d Gene3D is supplementary to the CATH database. This protein sequence database contains proteins from complete genomes which have been clustered into protein families and annotated with CATH domains, Pfam domains and functional information from KEGG, GO, COG, Affymetrix and STRINGS.
    Do you want to use gene3d ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Hmmpanther (hmmpfam from HMMER 2.3.2 package) PANTHER classifies proteins into families and specific functional subfamilies, which are then mapped to ontology terms and pathways.
    Do you want to use hmmpanther ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Hmmpir PIR produces the Protein  Sequence Database (PSD) of functionally annotated protein sequences, which grew out of the Atlas of Protein Sequence and Structure (1965-1978) edited by Margaret Dayhoff and has been incorporated into an integrated knowledge base system of value-added databases and analytical tools.
    Do you want to use hmmpir ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Hmmpfam (hmmpfam from HMMER 2.3.2 package) is used to search against the Pfam HMM (Profile hidden Markov models) database
    Do you want to use hmmpfam ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Hmmsmart (hmmpfam from HMMER 2.3.2 package) is used to search against the Smart HMM (Profile hidden Markov models) database
    Do you want to use hmmsmart ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Hmmtigr (hmmpfam from HMMER 2.3.2 package) is used to search against TIGRFAMs collection of HMMs (Profile hidden Markov models)
    Do you want to use hmmtigr ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Fprintscan (FingerPRINTScan) is used to search against the PRINTS collection of protein signatures
    Do you want to use fprintscan ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Scanregexp (ScanProsite) is used to search against the PROSITE patterns collection and verify the  matches by statistically significant CONFIRM patterns
    Do you want to use scanregexp ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Profilescan (ProfileScan (pfscan & ps_scan.pl) from Pftools 2.2) package is used  to search against the PROSITE profiles collection
    Do you want to use profilescan ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Superfamily A structural classification of proteins database for the investigation of sequences and structures.
    Do you want to use superfamily ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Seg is used for identifying and masking segments of low compositional complexity in amino acid sequences
    Do you want to use seg ? [y] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Signalp predicts the presence and location of signal peptide cleavage sites, using eukaryotic HMM (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use signalp ? [n] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
! Tmhmm is used for prediction of transmembrane helices in proteins using a Hidden Markov Model (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use tmhmm ? [n] /(y|n)/? : y
ln: binaries/Darwin: File exists
        ERROR: Unable to create symbolic link 'ln -s Darwin binaries': 
        Please correct the error or create the symbolic link manually if necessary


! Tip:
!
! InterProScan can use taxonomy-based filtering of results which uses the InterPro taxonomy listed for each InterPro
! entry to hide hits not relevant to that taxon.
!
    Do you want to make it available for users? [y] /y|n/? : y

! Tip:
!
! You can specify an email address for the administrator. By default it will be you (root)
! Administrators receive an email when a problem occurs in InterProScan.
!
    Do you want to set an administrator email address? [y] /y|n/? : youngstuart@hotmail.com
>>Writing to the configuration files ... 
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/blastprodom.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/coils.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/fprintscan.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/gene3d.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmpanther.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmpfam.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmpir.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmsmart.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/hmmtigr.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/profilescan.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/scanregexp.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/seg.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/signalp.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/superfamily.conf ... >>DONE
>>Processing file : /Volumes/gemshd4/common/apps/iprscan/conf/tmhmm.conf ... >>DONE
>>Setting File /Volumes/gemshd4/common/apps/iprscan/conf/iprscan.conf ... >>DONE

! Tip:
!
! InterProScan can be launched through a web interface. It can also be launched through secure HTTP (https)
!
    Do you want to run InterProScan using a web interface? [n] /y|n/? : y
    Do you want to run InterProScan through a secure HTTP server? [n] /y|n/? : gems.rsmas.miami.edu
WARNING: The information input is not valid (doesn't match /y|n/), please try again.
    Do you want to run InterProScan through a secure HTTP server? [n] /y|n/? : y
    Enter the name of your server. You can specify a specific port to listen to. (e.g. "foo.bar.com:4000") [foobar.com] /[\w\.\_\-\:\d]+/? : gems.rsmas.miami.edu
!
! Please add the following lines to your web server configuration (httpd.conf):
#--------------------------------------
 Alias /doc/ "/Volumes/gemshd4/common/apps/iprscan/doc/html/"
 Alias /images/ "/Volumes/gemshd4/common/apps/iprscan/images/"
 Alias /tmp/ "/Volumes/gemshd4/common/apps/iprscan/tmp/"

 ScriptAlias /iprscan/ "/Volumes/gemshd4/common/apps/iprscan/bin/"

 <Directory "/Volumes/gemshd4/common/apps/iprscan/images">
   Options Indexes MultiViews
   AllowOverride None
   Order allow,deny
   Allow from all
 </Directory>

 <Directory "/Volumes/gemshd4/common/apps/iprscan/tmp/">
   Options FollowSymLinks Includes SymLinksIfOwnerMatch
    IndexIgnore */.??* *~ *# */HEADER* */README* */RCS
    AllowOverride AuthConfig Limit FileInfo

    Order deny,allow
    Deny from all
    Allow from .rsmas.miami.edu
 </Directory>
#--------------------------------------
! Be sure that 'apache' or 'httpd' virtual user (uid/gid) have right to read/write into your /Volumes/gemshd4/common/apps/iprscan iprscan directory.
! Otherwise, if you are running a standalone http(s) web server, be sure also the user running it have to enough
! rights to read/wrtie /Volumes/gemshd4/common/apps/iprscan iprscan directory
! If you encoutered problem with your web server running iprscan, read the FAQ or contact your system administrator.

! After restarting your web server try https://gems.rsmas.miami.edu/iprscan/iprscan
!
>>Indexing datafile ... 
No specific file(s) given, indexing all files:

Pfam
prints.pval
prodom.ipr
Pfam-C
smart.desc
sf_hmm
sf.seq
Pfam-A.seed
superfamily.hmm
smart.thresholds
smart.HMMs
match.xml
interpro.xml
TIGRFAMs_HMM.LIB
Gene3D.hmm


No specific file(s) given, converting all files:

Pfam
sf_hmm
superfamily.hmm
smart.HMMs
sf_hmm_sub
TIGRFAMs_HMM.LIB
Gene3D.hmm

WARNING: Some problems occured when trying to index the data files.
 Please fix the error below and index your datafiles using the /Volumes/gemshd4/common/apps/iprscan/bin/index_data.pl script.
 For help, type './index_data.pl -h' 
errormsg: ERROR: File [/Volumes/gemshd4/common/apps/iprscan/data/sf.seq] not found : No such file or directory at /Volumes/gemshd4/common/apps/iprscan/bin/index_data.pl line 217.
>>DONE

! Tip:
!
!  We would appreciate it if you would register your installation of InterProScan.  This will allow us to notify you
! of bug fixes and new releases of the software. We will not use your email address for any other purpose.
!
    Would you like to register InterProScan? [n] /(y|n)/? : n

>>Installation is completed.  Please test your installation by running on the command line:

        /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -i /Volumes/gemshd4/common/apps/iprscan/test.seq -o /Volumes/gemshd4/common/apps/iprscan/test.out -format raw -goterms -ipr

and comparing it with the merged.raw file that is supplied.
For help and options on the commandline, run /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -hgems:/Volumes/gemshd4/common/apps/iprscan local$ /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli -h
FILE: /Library/Perl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
Module->dl_load_flags: XML::Parser::Expat->dl_load_flags
usage: /Volumes/gemshd4/common/apps/iprscan/bin/iprscan -cli [-email <addr>] [-appl <name> ...] [-nocrc] [-altjobs] [-seqtype p|n] [-trlen <N>] [-trtable <table>]
               [-iprlookup] [-goterms] [-taxo <taxonomy> ...] [-txrule <0,1>] -i <seqfile> [-o <output file>]

  -i <seqfile>      Your sequence file (mandatory).
  -o <output file>  The output file where to write results (optional), default is STDOUT.
  -email <addr>     Submitter email address (required for non-interactive).
  -appl <name>      Application(s) to run (optional), default is all.
  -nocrc            Don't perform CRC64 check, default is on.
  -altjobs          Launch jobs alternatively, chunk after chunk. Default is off.
  -seqtype <type>   Sequence type: n for DNA/RNA, p for protein (default).
  -trlen <n>        Transcript length threshold (20-150).
  -trtable <table>  Codon table number.
  -goterms          Show GO terms if iprlookup option is also given.
  -iprlookup        Switch on the InterPro lookup for results.
  -taxo             Activate the Taxonomy filter for abbreviated taxonomy.(e.g. : -taxo Arthto -taxo Bact)
                    Possible values: Arabidopsis thaliana (AraTh),Archaea (Arch),Arthropoda (Arthro),Bacteria (Bact),Caenorhabditis elegans (CaeEl)
                                     Chordata (Chor),Cyanobacteria (Cyan), Eukaryota (Euka),Fruit Fly (FrFly),Fungi (Fung),Green Plants (GrePl)
                                     Human (Huma),Metazoa (Meta),Mouse (Mous),Nematoda (Nema),Other Eukaryotes (OthEuk),Plastid Group (PlasGrp)
                                     Rice spp (Rice),Saccharomyces cerevisiae (SacCer),Synechosystis PCC 6803 (Synec),Unclassified (Unclass),Virus (Vir)
  -txrule <0,1>   Make decision on the taxonomy: 0 -> AND, 1 -> OR.
  -format <format>  Output results format (raw, txt, html, xml(default), ebixml(EBI header on top of xml), gff)
  -verbose          Print messages during run



+++++++++++++++++</entry>



<entry [MSG: bill@bioteam.net, chris@bioteam.net,inquiry-support@bioteam.net] CC: dlopata@apple.net>



Hi Bill/Chris,

Did you get my earlier messages? I emailed 'inquiry-support@bioteam.net' last Wednesday and emailed 'bill@bioteam.net' and 'chris@bioteam.net' on Friday but I haven't received even a confirmatory reply.

Here's my updated status:

1) NFS + Submit host
I just shifted over the shared directories (/common, /Users, /Groups, /Library/perl) to our new RAID disc and am now getting 'permission denied' from 'mount_nfs' when I try to mount them on my local machine (not the head node). I'll be trying to fix that later today. Any suggestions on where to look?

- Once I have it working, I'll try to set up my local machine as a submit host by doing:
source /common/sge/default/common/settings.sh

- My scripts are in /Users/local on the head node on my local machine. If I mount it as /Users2/local, will it suffice to specify a corresponding mapping inside the 'sge_aliases' file to let the head node find the scripts I'm referring to in '/Users/local' on the head node? 

2) Interproscan
If get the following error when I submit a job to Interproscan:

 sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose
Password:
FILE: /Library/Perl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
Module->dl_load_flags: XML::Parser::Expat->dl_load_flags
SUBMITTED iprscan-20061220-15010067

PENDING   ....
FAILED
/usr/bin/iprscan: /common/apps/iprscan/tmp/20061220/iprscan-20061220-15010067/iprscan-20061220-15010067.xml unavailable : No such file or directory

Here's the output from qstat:
gems:/common/apps/iprscan/bin local$ qstat -f
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       0.13     darwin        
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   0/2       0.04     darwin        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.03     darwin        

############################################################################
 - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS
############################################################################
    957 0.55500 iprscan-20 root         Eqw   12/20/2006 11:35:42     1        
    958 0.55500 iprscan-20 root         Eqw   12/20/2006 11:43:51     1        
    959 0.55500 iprscan-20 root         Eqw   12/20/2006 15:01:00     1        
    934 0.00000 iprscan-20 local        Eqw   12/03/2006 22:33:30     1        
    935 0.00000 iprscan-20 local        Eqw   12/08/2006 22:32:42     1        
    939 0.00000 iprscan-20 local        Eqw   12/13/2006 20:16:36     1        
    940 0.00000 iprscan-20 local        Eqw   12/13/2006 20:21:59     1        
    942 0.00000 iprscan-20 local        Eqw   12/18/2006 09:33:00     1        
    947 0.00000 iprscan-20 local        Eqw   12/18/2006 11:21:10     1        
    953 0.00000 iprscan-20 local2       Eqw   12/19/2006 11:33:55     1        
    954 0.00000 iprscan-20 root         Eqw   12/19/2006 11:34:20     1        
    955 0.00000 iprscan-20 local        Eqw   12/19/2006 11:35:52     1        
    956 0.00000 iprscan-20 root         Eqw   12/19/2006 11:36:09     1        
gems:/common/apps/iprscan/bin local$ qstat -j 159
Following jobs do not exist: 159
gems:/common/apps/iprscan/bin local$ qstat -j 959
job_number:                 959
exec_file:                  job_scripts/959
submission_time:            Wed Dec 20 15:01:00 2006
owner:                      root
uid:                        0
group:                      wheel
gid:                        0
sge_o_home:                 /Users/local
sge_o_log_name:             root
sge_o_path:                 /Users/local/apps/cytoscape/PLUGINS:/usr/lib/perl5/site_perl:/usr/share/apps/ActiveTcl/bin:/usr/local/mysql/bin:/usr/local/bin:/usr/share/apps/komodo:/sw/bin:/Users/local/FUNNYBASE/bin:/common/sge/bin/darwin:/bin:/sbin:/usr/bin:/usr/sbin:/common/mpich-1.2.7/ch_p4/bin:/common/bin:/common/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin:/usr/local/biotools/bin:/Applications/bioinf/phylip/exe:/Applications/bioinf/t_coffee:/usr/local/biotools/bin
sge_o_shell:                /bin/bash
sge_o_workdir:              /common/apps/iprscan/bin
sge_o_host:                 gems
account:                    sge
stderr_path_list:           /dev/null
mail_list:                  root@gems.rsmas.miami.edu
notify:                     FALSE
job_name:                   iprscan-20061220-15010067
stdout_path_list:           /dev/null
jobshare:                   0
hard_queue_list:            all.q
env_list:                   
script_file:                /common/apps/iprscan/tmp/20061220/iprscan-20061220-15010067/iprscan-20061220-15010067.dcmd
error reason    1:          12/20/2006 12:17:32 [0:5364]: execvp(/common/apps/iprscan/tmp/20061220/iprscan-20061220-15010067/ipr
scheduling info:            job is in error state
gems:/common/apps/iprscan/bin local$ 

Thanks,

Stuart.

++++++++++++++++++++++++++++++++++++++++




USER local
==========
cd /common/apps/iprscan/bin
sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose
>>>



CHECK OF qstat:
>>>...
############################################################################
 - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS
############################################################################
    957 0.55500 iprscan-20 root         Eqw   12/20/2006 11:35:42     1        
    958 0.55500 iprscan-20 root         Eqw   12/20/2006 11:43:51     1        
    934 0.00000 iprscan-20 local        Eqw   12/03/2006 22:33:30     1        
    935 0.00000 iprscan-20 local        Eqw   12/08/2006 22:32:42     1        
    939 0.00000 iprscan-20 local        Eqw   12/13/2006 20:16:36     1        
    940 0.00000 iprscan-20 local        Eqw   12/13/2006 20:21:59     1        
    942 0.00000 iprscan-20 local        Eqw   12/18/2006 09:33:00     1        
    947 0.00000 iprscan-20 local        Eqw   12/18/2006 11:21:10     1        
    953 0.00000 iprscan-20 local2       Eqw   12/19/2006 11:33:55     1        
    954 0.00000 iprscan-20 root         Eqw   12/19/2006 11:34:20     1        
    955 0.00000 iprscan-20 local        Eqw   12/19/2006 11:35:52     1        
    956 0.00000 iprscan-20 root         Eqw   12/19/2006 11:36:09     1 
<<<

qstat -j 957
>>>
job_number:                 957
exec_file:                  job_scripts/957
submission_time:            Wed Dec 20 11:35:42 2006
owner:                      root
uid:                        0
group:                      wheel
gid:                        0
sge_o_home:                 /Users/local
sge_o_log_name:             root
sge_o_path:                 /Users/local/apps/cytoscape/PLUGINS:/usr/lib/perl5/site_perl:/usr/share/apps/ActiveTcl/bin:/usr/local/mysql/bin:/usr/local/bin:/usr/share/apps/komodo:/sw/bin:/Users/local/FUNNYBASE/bin:/common/sge/bin/darwin:/bin:/sbin:/usr/bin:/usr/sbin:/common/mpich-1.2.7/ch_p4/bin:/common/bin:/common/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin:/usr/local/biotools/bin:/Applications/bioinf/phylip/exe:/Applications/bioinf/t_coffee:/usr/local/biotools/bin
sge_o_shell:                /bin/bash
sge_o_workdir:              /common/apps/iprscan/bin
sge_o_host:                 gems
account:                    sge
stderr_path_list:           /dev/null
mail_list:                  root@gems.rsmas.miami.edu
notify:                     FALSE
job_name:                   iprscan-20061220-11354069
stdout_path_list:           /dev/null
jobshare:                   0
hard_queue_list:            all.q
env_list:                   
script_file:                /common/apps/iprscan/tmp/20061220/iprscan-20061220-11354069/iprscan-20061220-11354069.dcmd
error reason    1:          12/20/2006 08:53:13 [0:4987]: execvp(/common/apps/iprscan/tmp/20061220/iprscan-20061220-11354069/ipr
scheduling info:            job is in error state
<<<








+</entry>



<entry [Tue Dec 19 11:39:19 PST 2006] TEST IPRSCAN WITH USER local AND local2:>



USER local2
===========
local2$ iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose
FILE: /Library/Perl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
Module->dl_load_flags: XML::Parser::Expat->dl_load_flags
SUBMITTED iprscan-20061219-11335477

PENDING   ....
FAILED
/usr/bin/iprscan: /common/apps/iprscan/tmp/20061219/iprscan-20061219-11335477/iprscan-20061219-11335477.xml unavailable : No such file or directory
gems:/common/apps/iprscan/bin local2$ sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

Password:
FILE: /Library/Perl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
Module->dl_load_flags: XML::Parser::Expat->dl_load_flags
SUBMITTED iprscan-20061219-11342099

PENDING   ...
FAILED
/usr/bin/iprscan: /common/apps/iprscan/tmp/20061219/iprscan-20061219-11342099/iprscan-20061219-11342099.xml unavailable : No such file or directory


Date: Fri, 03 Mar 2006 10:32:14 +0100
From: Gerd Marquardt <marquardt@rrzn.uni-hannover.de>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Subject: [GE users] can't get password entry for user


GOOGLE OF "can't get password entry for user":

Jinal Jhaveri wrote:

> Hi All,
>
> From few days, were are seeing following errors for few jobs.
>
> can't get password entry for user "kfelkins". Either the user does not 
> exist or NIS error!
> error reason  453:          can't get password entry for user 
> "kfelkins". Either the user does not exist or NIS error!
> error reason  454:          can't get password entry for user 
> "kfelkins". Either the user does not exist or NIS error!
>
> and thus leading the node in error state.
>
> The error doesn't happen for a particular user but randomly for 
> various users. I didn't get much information from the execd code 
> except that sge_getpwnam , which in turn calls getpwnam , fails. 
> sge_getpwnam doesn't specifically tell what error did it receive from 
> getpwnam. I saw several emails in the group, but none of those 
> situation apply to us. The user is definitely configured correctly on 
> that node and this error happens only randomly.  Also qconf -suserl 
> does show the name of that user.
>
>
We also have a problem with getpwnam and a statically linked program. 
This program was linked with an older kernel. We are working with LDAP 
not NIS, the program doesn't find the LDAP-passwd-entry. We added the 
user into the local /etc/passwd on every node. Now the program works.

-- 
 Gerd Marquardt
                    
 
ADDED LINE TO /etc/passwd ON gems:

local:*:501:501:local:/Users/local/:/bin/sh


SEARCH "netinfo" "authentication error"

NetInfo niload failing, permission denied

    * Subject: Re: NetInfo niload failing, permission denied
    * From: Marc Majka <email@hidden>
    * Date: Fri, 26 Sep 2003 09:19:53 -0700

niload should be OK if you load everything from one file. The thing to avoid is using niutil for massive updates. The problem is that each call to niutil opens a new TCP socket, which is slow. A better way to add a lot of data to a NetInfo database is to use nicl. See /usr/libexec/create_nidb for an example of a perl script that uses nicl to build the default local database.

em /usr/libexec/create_nidb
>>>
##
# Create a NetInfo database from flat files.
# Usage: create_nidb [tag [masterhostname [root]]
#
# Default tag is local.
# Default master hostname for tag local is localhost.
# Default master hostname for other tags is system hostname.
##

use Sys::Hostname;

my $nipath = "/var/db/netinfo";
my $filepath = "/etc";
my $root = "/";
... <<<



USER local
==========
cd /common/apps/iprscan/bin
sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose
FILE: /Library/Perl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
Module->dl_load_flags: XML::Parser::Expat->dl_load_flags
SUBMITTED iprscan-20061219-11355237

PENDING   ...
FAILED
/usr/bin/iprscan: /common/apps/iprscan/tmp/20061219/iprscan-20061219-11355237/iprscan-20061219-11355237.xml unavailable : No such file or directory
gems:/common/apps/iprscan/bin local$ sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose
Password:
FILE: /Library/Perl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
Module->dl_load_flags: XML::Parser::Expat->dl_load_flags
SUBMITTED iprscan-20061219-11360910

PENDING   .
RUNNING   ..
FAILED
/usr/bin/iprscan: /common/apps/iprscan/tmp/20061219/iprscan-20061219-11360910/iprscan-20061219-11360910.xml unavailable : No such file or directory
gems:/common/apps/iprscan/bin local$ cd /common/apps/iprscan/tmp/20061219/iprscan-20061219-11360910/
gems:/common/apps/iprscan/tmp/20061219/iprscan-20061219-11360910 local$ ls
iprscan-20061219-11360910.dcmd   iprscan-20061219-11360910.dsub   iprscan-20061219-11360910.params
iprscan-20061219-11360910.djob   iprscan-20061219-11360910.input  iprscan-20061219-11360910.seqs














+</entry>



<entry [Mon Dec 18 12:30:17 PST 2006] TRANSFER /Volumes/gemshd1/common TO /Volumes/gemshd4/common>



1. STOP NFS:
dsh -a SystemStarter stop AutoFS

2. TRANSFER /common, /Users, /Groups TO gemshd4
sudo cp -pr /common /Volumes/gemshd4
sudo cp -pr /Users /Volumes/gemshd4
sudo cp -pr /Groups /Volumes/gemshd4

3. BACKUP ORIGINALS
sudo mv /common /common.bak
sudo mv /Users /Users.bak
sudo mv /Groups /Groups.bak

4. CREATE SYMBOLIC LINKS:

sudo ln -s /Volumes/gemshd4/common /common
sudo ln -s /Volumes/gemshd4/Users /Users
sudo ln -s /Volumes/gemshd4/Groups /Groups

5. MODIFY EXPORTS ON HEAD NODE:

nidump -r /exports . > /tmp/exports.ni                  
>>>
{
  "name" = ( "exports" );
  CHILDREN = (
    {
      "name" = ( "/Users" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Groups" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Library/Perl" );
      "clients" = ( );
      "opts" = ( "ro", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/common" );
      "clients" = ( "129.171.101.233 node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.private  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private  node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root" );
    },
    {
      "name" = ( "/opt" );
      CHILDREN = (
        {
          "clients" = ( "129.171.101.233" );
          "name" = ( "/opt" );
          "opts" = ( "maproot=root" );
        }
      )
    }
  )
}
<<<

EXTRACT TO FILE /tmp/exports.ni AND EDIT:

em /tmp/exports.ni

{
  "name" = ( "exports" );
  CHILDREN = (
    {
      "name" = ( "/Volumes/gemshd4/Users" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Volumes/gemshd4/Groups" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Library/Perl" );
      "clients" = ( );
      "opts" = ( "ro", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Volumes/gemshd4/common" );
      "clients" = ( "129.171.101.233 node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.private  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private  node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root" );
    },
    {
      "name" = ( "/opt" );
      CHILDREN = (
        {
          "clients" = ( "129.171.101.233" );
          "name" = ( "/opt" );
          "opts" = ( "maproot=root" );
        }
      )
    }
  )
}

LOAD NEW EXPORT CONFIGURATION:

sudo niload -d -r /exports . < /tmp/exports.ni

RESTART NFS daemons ON THE HEAD NODE:
sudo killall -HUP nfsiod mountd nfsd-master nfsd-server automount

CHECK EXPORTS HAVE CHANGED:
showmount -e

CHECK MOUNTS ON genomics:
sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/Volumes/gemshd4/common /common
ls /common

6. MODIFY MOUNTS ON NODES:

COPY MOUNT SPECIFICATION FILE FROM node001 TO HEAD NODE:
scp node001:/etc/bipod/auto.common /private/tmp/auto.common

em /tmp/auto.common
common -rw,rsize=8192,wsize=8192 portal2net:/common
Users  -rw,rsize=8192,wsize=8192 portal2net:/Users
Groups  -rw,rsize=8192,wsize=8192 portal2net:/Groups
RemotePerl  -ro,rsize=8192,wsize=8192 portal2net:/Library/Perl

EDIT TO LOOK LIKE THIS, AND REMOVE OLD COPIES:

sudo emacs /etc/bipod/auto.common
# >>>
common -rw,rsize=8192,wsize=8192 portal2net:/Volumes/gemshd4/common
Users  -rw,rsize=8192,wsize=8192 portal2net:/Volumes/gemshd4/Users
Groups  -rw,rsize=8192,wsize=8192 portal2net:/Volumes/gemshd4/Groups
RemotePerl  -ro,rsize=8192,wsize=8192 portal2net:/Library/Perl
# <<<

sudo rm -fr /Users*
sudo rm -fr /Groups*
sudo rm -fr /RemotePerl
sudo reboot

#### THIS DOESN'T WORK (I.E., dsh DOESN'T WORK BECAUSE OF USERID, PASSWORD OR PERMISSIONS PROBLEM)
#### Now copy this modified file to all of the compute elements.
#
#scp /private/tmp/auto.common node001:/etc/bipod/auto.common
#
#Repeat this for each node in your cluster using node## in place of node001.
#
### Start NFS client mounts
# dsh -a SystemStarter start AutoFS

7. Confirm active mounts

#### LIST THE CONTENTS OF /Users (ACTUALL Y/Volume/MyXRAIDVol/Users)

sudo dsh -a ls /Users
>>>
Password:
executing 'ls /Users'
Password:Password:node001.cluster.private:      .DS_Store
node001.cluster.private:        .localized
node001.cluster.private:        Shared
node001.cluster.private:        local
node001.cluster.private:        root
node001.cluster.private:        vanwye
node001.cluster.private:        www
node001.cluster.private:        young
node002.cluster.private:        .DS_Store
node002.cluster.private:        .localized
node002.cluster.private:        Shared
node002.cluster.private:        local
node002.cluster.private:        root
node002.cluster.private:        vanwye
node002.cluster.private:        www
node002.cluster.private:        young
node004.cluster.private:        .DS_Store
... <<<

8. Make SGE aware of the change you made

This is a little tweak to the Sun GridEngine configuration that will tell it
to modify any PATH it sees pointing to the Xserve RAID to one that points to the
local disk.

echo "/Volumes/gemshd4/ * * /" >> /common/sge/default/common/sge_aliases

SO IT NOW READS:
>>> ...
# subm_dir      subm_host       exec_host       path_replacement
/tmp_mnt/       *                     *               /
/Volumes/gemshd4/       *               *               /
<<<


-----------------------------
Apple iNquiry
ID #1023
How do I move NFS shares from the iNquiry head node to an attached Xserve RAID?

Note: This FAQ was written before we added the shared PERL library directory to iNquiry. There will therefore be an additional directory named "RemotePerl" in some of the steps. Treat this one exactly the same as the others.

Also: If you have activated NetBoot, there will be yet another shared directory with "NetBoot" in the name. Best to leave that one alone.

This document assumes:

1) You already have iNquiry installed, and that the default NFS shares
(/common, /Users, /Groups) are being served properly from the head node

2) That you have an Xserve RAID attached to the head node with the disk(s)
properly formatted and the volume named something like "MyXRAIDVol".

3) That your goal is to continue serving NFS shares from the head node,
but you want the data to be physically located on the Xserve RAID disk(s).

4) That you have root access to the head node for performing the following steps.

5) That you won't hold BioTeam responsible for any data loss. No worries though,
it's pretty easy and safe.

Stop NFS client mounts
======================

The following command uses "dsh" to execute a UNIX SystemStarter script on all
of the compute elements and stops automount on each.

# dsh -a SystemStarter stop AutoFS

Copy local /common, /Users, /Groups to the Xserve RAID
======================================================

These commands copying the shared directories to the Xserve RAID volume,
retaining UNIX file permissions.

# cp -pr /common /Volumes/MyXRAIDVol
# cp -pr /Users /Volumes/MyXRAIDVol
# cp -pr /Groups /Volumes/MyXRAIDVol

Move the originals to a safe place until you're sure you want to delete them
============================================================================

These commands rename the copied directories in the event that you make a mistake
you can always come back to the originals.

# mv /common /common.bak
# mv /Users /Users.bak
# mv /Groups /Groups.bak

Create symbolic links from the Xserve RAID directories to the standard PATHs
============================================================================

These commands create sym-links from the copied directories on the Xserve RAID
to PATHs on the local head node volume such that PATHs remain constant even
though physical locations have changed.

# ln -s /Volumes/MyXRAIDVol/common /common
# ln -s /Volumes/MyXRAIDVol/Users /Users
# ln -s /Volumes/MyXRAIDVol/Groups /Groups

Modify NFS exports on head node
===============================

The following series of commands shows how to dump the current NFS exports to a
file, then shows how to modify this file (before and after) to change the NFS exports,
then how to load the file into NetInfo again, actually changing the exports.

The following command shows the current NFS exports.

# showmount -e
Exports list on localhost:
/common 192.168.2.0
/Users 192.168.2.0
/Library/NetBoot/NetBootSP0 192.168.2.0
/Groups 192.168.2.0

This command dumps the configured exports from NetInfo into the file /tmp/exports.ni.

# nidump -r /exports . > /tmp/exports.ni

The following shows the default exports that come with iNquiry.

PLEASE NOTE:  You should generate the exports file which is appropriate for your system by using the nidump file above.  Local customizations or changes to iNquiry may mean that this example is not appropriate for your system.

{
    "name" = ( "exports" );
    CHILDREN = (
        {
            "name" = ( "/Users" );
            "clients" = ( );
            "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
        },
        {
            "name" = ( "/Groups" );
            "clients" = ( );
            "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
        },
        {
            "name" = ( "/common" );
            "clients" = ( );
            "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
        },
        {
            "name" = ( "/Library/NetBoot/NetBootSP0" );
            "clients" = ( );
            "opts" = ( "network=192.168.2.0", "mask=255.255.255.0" );
        }
    )
}

Now modify this file to move exports from the local volume to the Xserve RAID
volume named "MyXRAIDVol". Don't modify the NetBoot share. No need.

{
"name" = ( "exports" );
    CHILDREN = (
        {
            "name" = ( "/Volumes/MyXRAIDVol/Users" );
            "clients" = ( );
            "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
        },
        {
            "name" = ( "/Volumes/MyXRAIDVol/Groups" );
            "clients" = ( );
            "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
        },
        {
            "name" = ( "/Volumes/MyXRAIDVol/common" );
            "clients" = ( );
            "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
        },
        {
            "name" = ( "/Library/NetBoot/NetBootSP0" );
            "clients" = ( );
            "opts" = ( "network=192.168.2.0", "mask=255.255.255.0" );
        }
    )
}

This command loads the modified file back into NetInfo.

# niload -d -r /exports . < /tmp/exports.ni

This command refreshes the NFS daemons on the head node such that they
apply the new configuration information.

# killall -HUP nfsiod mountd nfsd-master nfsd-server automount

After "HUPing" the NFS daemons, NFS exports should looks something like this.
If they don't, it's probably OK, but you can reboot the head node do be certain.

# showmount -e
Exports list on localhost:
/Volumes/MyXRAIDVol/common 192.168.2.0
/Volumes/MyXRAIDVol/Users 192.168.2.0
/Volumes/MyXRAIDVol/Groups 192.168.2.0
/Library/NetBoot/NetBootSP0 192.168.2.0

Modifying NFS mount points on compute elements
==============================================

Note: If you do not have a file /etc/auto.common on your compute nodes then you are running the newer "packaged based" version of iNquiry. This version stores node automount files in the following location: /etc/bipod/auto.common -- you can follow the directions below but just substitute the new path in place of /etc/auto.common.


This command copies the NFS /etc/auto.common file from node01 into
the /tmp directory on the head node.

# scp node01:/etc/auto.common /tmp

Before modification the contents of this file looks like:

common -rw,rsize=8192,wsize=8192 portal2net:/common
Users -rw,rsize=8192,wsize=8192 portal2net:/Users
Groups -rw,rsize=8192,wsize=8192 portal2net:/Groups

Modify this file to look like:

common -rw,rsize=8192,wsize=8192 portal2net:/Volumes/MyXRAIDVol/common
Users -rw,rsize=8192,wsize=8192 portal2net:/Volumes/MyXRAIDVol/Users
Groups -rw,rsize=8192,wsize=8192 portal2net:/Volumes/MyXRAIDVol/Groups

Now copy this modified file to all of the compute elements.

# scp /tmp/auto.common node01:/etc/auto.common

Repeat this for each node in your cluster using node## in place of node01.

Start NFS client mounts
=======================

# dsh -a SystemStarter start AutoFS

Unfortunately this SystemStarter script never returns control back to the
SHELL so you'll have to enter "Control C" after each prompt that says
Startup Complete.

Confirm active mounts
=====================

This command lists the contents of /Users actually /Volume/MyXRAIDVol/Users.

# dsh -a ls /Users

Make SGE aware of the change you made
=====================================

This is a little tweak to the Sun GridEngine configuration that will tell it
to modify any PATH it sees pointing to the Xserve RAID to one that points to the
local disk.

# echo "/Volumes/MyXRAIDVol/ * * /" >> /common/sge/default/common/sge_aliases

That should do it.

Last update: 2005-10-14 10:56
Author: Bill Van Etten
Revision: 1.0


+</entry>



<entry [Mon Dec 18 11:49:58 PST 2006] STILL FIXING 'Library not loaded: /usr/local/lib/libexpat.1.dylib' ERROR.>



REINSTALLED EXPAT:
cd ~/FUNNYBASE/NOTES/plmods/XML-Parser/expat-2.0.0
./configure --prefix=/usr/local
make
sudo make install
>>> ...
----------------------------------------------------------------------
Libraries have been installed in:
   /usr/local/lib

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the `-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the `DYLD_LIBRARY_PATH' environment variable
     during execution

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
----------------------------------------------------------------------
... <<<


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Dec 18 11:17:58 PST 2006

REINSTALLED iprscan ON gems BUT STILL GOT SAME 'Library not loaded: /usr/local/lib/libexpat.1.dylib' ERROR.

INSTALL LOG:

KEEP Pfam - the highest coverage among the databases integrated into InterPro.
INACTIVATE SMART and TIGRFAMs - both HMM profile databases like Pfam, but one is eukaryote specific and the other is prokaryote specific while Pfam deals with both eukaryotes and prokaryotes.

gems:/common/apps/iprscan local$ sudo ./Config.pl 
Password:

####################################################################################################
#                Welcome to the InterProScan v4.2 installation script.
#
#  Tips:
#     - If you are happy with the suggestion between [], just press <enter>
#     - Whatever is shown between '/' and '/' is considered a valid input pattern
####################################################################################################


    Reconfigure everything? (first time install) [n] /(y|n)/? : 

! Tip:
!
! InterProScan needs to know where it is installed and where perl is installed
!
    Do you want to set paths to perl and the installation directory? [n] /y|n/? : 

! Tip:
!
! To cope with bulk jobs and to enable efficient parallelization, InterProScan splits input files
! with FASTA formatted sequences into smaller parts (chunks). Default size is "10" sequences per chunk
!
    Do you want to setup chunk size [n] /(y|n)/? : y

! Tip:
!
! Here you should specify the maximum number of sequences allowed in each part (chunk).  Please note
! it is not recommended to have more then 3000 chunks.
!
    Enter chunk size [10] /[0-9]+/? : 2


! Tip:
!
! You can configure InterProScan to limit the number of input sequences (protein & nucleotide),
! set a maximum length for nucleotide sequences and a minimum length for protein sequences.
! You can also configure the minimum length for a translated orf and use the codon table value
! to translate the dna/rna sequence into six frames (http://www.ebi.ac.uk/cgi-bin/mutations/trtables.cgi).
!
    Do you want to configure this? [n] /(y|n)/? : 


! Tip:
!
! InterProScan currently encorporates searching against 13 databases. You have the choice whether or not to
! set up a particular search tool against a database and how exactly that application will be configured to run
! in this next section. You will configure the queue systems (if any) you will use, first
!
    Do you want to setup applications (if you don't, no applications will be included in InterProScan by default)? [n] /(y|n)/? : y

! Tip:
!
! InterProScan is able to execute the applications via a UNIX QUEUING system which has an INTERACTIVE mode (such as LSF).
! Currently, we mainly support LSF (which is used at EBI) however, we also provide configuration files for other systems
! which are listed in the documentation
!
    Do you wish to use a queue system? [n] /(y|n)/? : y
!
! VERY IMPORTANT NOTICE: 
! Applications from InterProScan will use a specific directory (iprscan/bin/binaries) to find application binaries
! and execute them.  'binaries' is a symbolic link pointing to a directory with a name that corresponds to the
! OS architecture (OSF1|Linux|AIX|SunOS|Darwin|IRIX).  If you have different architectures on any of the nodes
! used by your queueing system, you must make sure that the link they see points to the appropriate architecture
! directory.
! (e.g.) Host A is Linux, Host B is OSF1.  Be sure that, on host A, the binaries link points to a directory called
!       'Linux' and on host B, to 'OSF1'
!

Please choose one queueing system from the list. If you don't want to use this queueing system, just type '<enter>'

    Do you want to use pbs54? [n] /(y|n)/? : 
    Do you want to use lsf42? [n] /(y|n)/? : 
    Do you want to use sge6? [n] /(y|n)/? : y
!
! You can now configure global settings for your queue system for all the applications (such as a ! global cluster name and/or a global queue name)
!
    Do you want to set a global cluster name for all applications? [n] /(y|n)/? : 
    Do you want to set a global queue name for all applications? [n] /(y|n)/? : 
>>You will run InterProScan with sge6 queueing system using:
    
Is all this information correct? [n] /y|n/? : n
!
! You can now configure global settings for your queue system for all the applications (such as a ! global cluster name and/or a global queue name)
!
    Do you want to set a global cluster name for all applications? [n] /(y|n)/? : cluster.private
WARNING: The information input is not valid (doesn't match /(y|n)/), please try again.
    Do you want to set a global cluster name for all applications? [n] /(y|n)/? : y
    Please enter the global cluster name you want to use for all applications [] /([^\s\;\,\:]+)/? : cluster.private
    Do you want to set a global queue name for all applications? [n] /(y|n)/? : all.q
WARNING: The information input is not valid (doesn't match /(y|n)/), please try again.
    Do you want to set a global queue name for all applications? [n] /(y|n)/? : y
    Please enter the name of the global queue you want to use for all applications [] /([^\s\;\,\:]+)/? : all.q
>>You will run InterProScan with sge6 queueing system using:
>>      -Global cluster name 'cluster.private'.
>>      -Global queue name 'all.q'.
    
Is all this information correct? [n] /y|n/? : y
>>Checking your local architecture for InterProScan compatibility

! Tip:
! Blastprodom (a wrapper script on top of Blast package) is used to search against PRODOM families
    Do you want to use blastprodom ? [y] /(y|n)/? : y

! Tip:
! Coils is used to predict coiled-coil segments with the algorithm of Lupas et al.
    Do you want to use coils ? [y] /(y|n)/? : y 

! Tip:
! Gene3d Gene3D is supplementary to the CATH database. This protein sequence database contains proteins from complete genomes which have been clustered into protein families and annotated with CATH domains, Pfam domains and functional information from KEGG, GO, COG, Affymetrix and STRINGS.
    Do you want to use gene3d ? [y] /(y|n)/? : y

! Tip:
! Hmmpanther (hmmpfam from HMMER 2.3.2 package) PANTHER classifies proteins into families and specific functional subfamilies, which are then mapped to ontology terms and pathways.
    Do you want to use hmmpanther ? [y] /(y|n)/? : y

! Tip:
! Hmmpir PIR produces the Protein  Sequence Database (PSD) of functionally annotated protein sequences, which grew out of the Atlas of Protein Sequence and Structure (1965-1978) edited by Margaret Dayhoff and has been incorporated into an integrated knowledge base system of value-added databases and analytical tools.
    Do you want to use hmmpir ? [y] /(y|n)/? : y

! Tip:
! Hmmpfam (hmmpfam from HMMER 2.3.2 package) is used to search against the Pfam HMM (Profile hidden Markov models) database
    Do you want to use hmmpfam ? [y] /(y|n)/? : y

! Tip:
! Hmmsmart (hmmpfam from HMMER 2.3.2 package) is used to search against the Smart HMM (Profile hidden Markov models) database
    Do you want to use hmmsmart ? [y] /(y|n)/? : n

! Tip:
! Hmmtigr (hmmpfam from HMMER 2.3.2 package) is used to search against TIGRFAMs collection of HMMs (Profile hidden Markov models)
    Do you want to use hmmtigr ? [y] /(y|n)/? : n

! Tip:
! Fprintscan (FingerPRINTScan) is used to search against the PRINTS collection of protein signatures
    Do you want to use fprintscan ? [y] /(y|n)/? : y

! Tip:
! Scanregexp (ScanProsite) is used to search against the PROSITE patterns collection and verify the  matches by statistically significant CONFIRM patterns
    Do you want to use scanregexp ? [y] /(y|n)/? : y

! Tip:
! Profilescan (ProfileScan (pfscan & ps_scan.pl) from Pftools 2.2) package is used  to search against the PROSITE profiles collection
    Do you want to use profilescan ? [y] /(y|n)/? : y

! Tip:
! Superfamily A structural classification of proteins database for the investigation of sequences and structures.
    Do you want to use superfamily ? [y] /(y|n)/? : y

! Tip:
! Seg is used for identifying and masking segments of low compositional complexity in amino acid sequences
    Do you want to use seg ? [y] /(y|n)/? : y

! Tip:
! Signalp predicts the presence and location of signal peptide cleavage sites, using eukaryotic HMM (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use signalp ? [n] /(y|n)/? : y

! Tip:
! Tmhmm is used for prediction of transmembrane helices in proteins using a Hidden Markov Model (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use tmhmm ? [n] /(y|n)/? : y

! Tip:
!
! InterProScan can use taxonomy-based filtering of results which uses the InterPro taxonomy listed for each InterPro
! entry to hide hits not relevant to that taxon.
!
    Do you want to make it available for users? [y] /y|n/? : y

! Tip:
!
! You can specify an email address for the administrator. By default it will be you (root)
! Administrators receive an email when a problem occurs in InterProScan.
!
    Do you want to set an administrator email address? [y] /y|n/? : y
    Please enter the email address of the administrator: [] /[\w\.\-]+\@[\w\.\-]+/? : syoung@rsmas.miami.edu
>>Writing to the configuration files ... 
>>Processing file : /common/apps/iprscan/conf/blastprodom.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/coils.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/fprintscan.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/gene3d.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpanther.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpfam.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpir.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/profilescan.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/scanregexp.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/seg.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/signalp.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/superfamily.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/tmhmm.conf ... >>DONE
>>Setting File /common/apps/iprscan/conf/iprscan.conf ... >>DONE

! Tip:
!
! InterProScan can be launched through a web interface. It can also be launched through secure HTTP (https)
!
    Do you want to run InterProScan using a web interface? [n] /y|n/? : y
    Do you want to run InterProScan through a secure HTTP server? [n] /y|n/? : n
    Enter the name of your server. You can specify a specific port to listen to. (e.g. "foo.bar.com:4000") [foobar.com] /[\w\.\_\-\:\d]+/? : genomics.rsmas.miami.edu
!
! Please add the following lines to your web server configuration (httpd.conf):
#--------------------------------------
 Alias /doc/ "/common/apps/iprscan/doc/html/"
 Alias /images/ "/common/apps/iprscan/images/"
 Alias /tmp/ "/common/apps/iprscan/tmp/"

 ScriptAlias /iprscan/ "/common/apps/iprscan/bin/"

 <Directory "/common/apps/iprscan/images">
   Options Indexes MultiViews
   AllowOverride None
   Order allow,deny
   Allow from all
 </Directory>

 <Directory "/common/apps/iprscan/tmp/">
   Options FollowSymLinks Includes SymLinksIfOwnerMatch
    IndexIgnore */.??* *~ *# */HEADER* */README* */RCS
    AllowOverride AuthConfig Limit FileInfo

    Order deny,allow
    Deny from all
    Allow from .rsmas.miami.edu
 </Directory>
#--------------------------------------
! Be sure that 'apache' or 'httpd' virtual user (uid/gid) have right to read/write into your /common/apps/iprscan iprscan directory.
! Otherwise, if you are running a standalone http(s) web server, be sure also the user running it have to enough
! rights to read/wrtie /common/apps/iprscan iprscan directory
! If you encoutered problem with your web server running iprscan, read the FAQ or contact your system administrator.

! After restarting your web server try http://genomics.rsmas.miami.edu/iprscan/iprscan
!
>>Indexing datafile ... 
No specific file(s) given, indexing all files:

Pfam
prints.pval
prodom.ipr
Pfam-C
smart.desc
sf_hmm
sf.seq
Pfam-A.seed
superfamily.hmm
smart.thresholds
smart.HMMs
match.xml
interpro.xml
TIGRFAMs_HMM.LIB
Gene3D.hmm


No specific file(s) given, converting all files:

Pfam
sf_hmm
superfamily.hmm
smart.HMMs
sf_hmm_sub
TIGRFAMs_HMM.LIB
Gene3D.hmm

WARNING: Some problems occured when trying to index the data files.
 Please fix the error below and index your datafiles using the /common/apps/iprscan/bin/index_data.pl script.
 For help, type './index_data.pl -h' 
errormsg: ERROR: File [/common/apps/iprscan/data/sf.seq] not found : No such file or directory at /common/apps/iprscan/bin/index_data.pl line 217.
>>DONE

! Tip:
!
!  We would appreciate it if you would register your installation of InterProScan.  This will allow us to notify you
! of bug fixes and new releases of the software. We will not use your email address for any other purpose.
!
    Would you like to register InterProScan? [n] /(y|n)/? : n

>>Installation is completed.  Please test your installation by running on the command line:

        /common/apps/iprscan/bin/iprscan -cli -i /common/apps/iprscan/test.seq -o /common/apps/iprscan/test.out -format raw -goterms -ipr

and comparing it with the merged.raw file that is supplied.
For help and options on the commandline, run /common/apps/iprscan/bin/iprscan -cli -h


+</entry>



<entry [Mon Dec 18 10:34:55 PST 2006] iprscan ERROR ON gems>



cd /common/apps/iprscan/bin
sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml

gems:/common/apps/iprscan/tmp/20061218/iprscan-20061218-09332022 local$ em *errors

Can't load '/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle' for module XML::Parser::Expat: dlopen(/Remot\
ePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle, 1): Library not loaded: /usr/local/lib/libexpat.1.dylib
  Referenced from: /RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
  Reason: image not found at /System/Library/Perl/5.8.6/darwin-thread-multi-2level/DynaLoader.pm line 230.
 at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14
Compilation failed in require at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14.
BEGIN failed--compilation aborted at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 18.
Compilation failed in require at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
Compilation failed in require at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
Compilation failed in require at /common/apps/iprscan/bin/iprscan_wrapper.pl line 50.
BEGIN failed--compilation aborted at /common/apps/iprscan/bin/iprscan_wrapper.pl line 50.

IDENTICAL ERROR ON node001:

node001:/common/apps/iprscan/bin vanwye$ sudo /common/apps/iprscan/bin/iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml
SUBMITTED iprscan-20061218-07551900

Can't load '/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle' for module XML::Pars\
er::Expat: dlopen(/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle, 1): Library no\
t loaded: /usr/local/lib/libexpat.1.dylib
  Referenced from: /RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
  Reason: image not found at /System/Library/Perl/5.8.6/darwin-thread-multi-2level/DynaLoader.pm line 230.
 at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14
Compilation failed in require at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14.
BEGIN failed--compilation aborted at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 18.
Compilation failed in require at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
Compilation failed in require at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
Compilation failed in require at /common/apps/iprscan/bin/iprscan_wrapper.pl line 50.
BEGIN failed--compilation aborted at /common/apps/iprscan/bin/iprscan_wrapper.pl line 50.


TRY VERBOSE MODE:

sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml -verbose

sudo iprscan -cli -i ../test.nuc.seq  -o ../test.nuc.out.xml -format xml -seqtype n  -iprlookup -goterms -verbose -appl hmmpfam


SUBMITTED iprscan-20061218-11032202

PENDING   .....
RUNNING   ............................................................

+</entry>



<entry [Mon Dec 18 10:31:29 PST 2006] MONITOR INTERPROSCAN JOB>



/common/apps/iprscan/bin/meter.pl /common/apps/iprscan/tmp/20061218/iprscan-20061218-09332022
>>>
ERROR: Unable to find any 'chunk' directory in '/common/apps/iprscan/tmp/20061218/iprscan-20061218-09332022'
Usage: /common/apps/iprscan/bin/meter.pl [<session-dir>|-j <jobid>] -h <help>
                  -j jobid. Find automatically the session direcotry.
                  -h Displays this help and exits.
<<<

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Dec 18 10:00:30 PST 2006

BENCHMARKING

    * Benchmarking information: Crude benchmarking was done for InterProScan running on P50750|CDK9_HUMAN (327aa). This will be repeated for each release so that users know what to expect as far as performance goes. Machine specifications: HP Compaq with 2x Pentium 4 CPUs (3.2GHz); 512Mb RAM. InterProScan was run on 1 CPU; each program was run separately.
      Program Name	Speed in v4.3 (s)
      HMMPfam	70
      HMMPanther	12
      HMMPIR	22
      blastprodom	18
      coils	7
      gene3d	28
      HMMSmart	8
      HMMTigr	23
      FPRINTScan	12
      scanregexp	8
      profilescan	17
      superfamily	151
      seg	8
      signalp	7
      tmhmm	7
      --------------	-----------------
      total	6m38s

BENCHMARKED SEQUENCE:

>P50750|CDK9_HUMAN
MAKQYDSVEC PFCDEVSKYE KLAKIGQGTF GEVFKARHRK TGQKVALKKV LMENEKEGFP 
ITALREIKIL QLLKHENVVN LIEICRTKAS PYNRCKGSIY LVFDFCEHDL AGLLSNVLVK 
FTLSEIKRVM QMLLNGLYYI HRNKILHRDM KAANVLITRD GVLKLADFGL ARAFSLAKNS 
QPNRYTNRVV TLWYRPPELL LGERDYGPPI DLWGAGCIMA EMWTRSPIMQ GNTEQHQLAL 
ISQLCGSITP EVWPNVDNYE LYEKLELVKG QKRKVKDRLK AYVRDPYALD LIDKLLVLDP 
AQRIDSDDAL NHDFFWSDPM PSDLKGMLST HLTSMFEYLA PPRRKGSQIT QQSTNQSRNP 
ATTNQTEFER VF 


BENCHMARKS:








+</entry>



<entry [Wed Dec 13 21:50:14 EST 2006] MSG: inquiry-support@bioteam.net>



Hi Bill,

Thanks a lot for the follow up.

Here's my current status:

1) Installing a submit host - I just completed the NFS share tonight (tricky because of firewall) and followed the tip from Chris Dagdigian ('source /common/sge/default/common/settings.sh') . It appears to be working; 'qstat -f' works OK though haven't tested qsub yet. 

- I'll let you know the outcome once I have tested submitting jobs (probably tomorrow) as I presume it'll involve mounting the /Users/local directory (containing my scripts) from the head node on my local machine? 

- If I mount it under '/Users2/local' (as I already have a '/Users/local' directory on my local machine) will I have to put an alias somewhere in SGE to let the head node know I'm referring to '/Users/local'? 

- Alternately, if there's a better way of doing it, please let me know.

I also have a wee issue in that my /common/data is actually a simlink to another, larger  disc where I've stored the data. I can't access /common/data on my local machine via the NFS mount. 

- Is there some way I can enable access to a simlink via NFS? 

- Otherwise, I guess I'll have to shift /common completely over to the larger disc?

2) interproscan ON SGE - followed all instructions (below and from Chris Dwan) but get error on user 'local' submit 
_____________________________
Steps needed to make this work (technical details):

1 - Allow job submission from all nodes (!)
2 - Update qstat_ipr.pl in the iprscan/bin directory (the patch from 
interpro should work, basically-you need to use the -f flag for qstat).
3 - Create a symbolic link to iprscan/conf/sge6.env under 
iprscan/conf/sge.env
3 - Change the following line in iprscan/sge6/conf

asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e 
/dev/null [%env IPRSCAN_HOME]/conf/sge6-test.sh [%toolcmd]

To this:
asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e 
/dev/null [%toolcmd]
______________________________

iprscan JOB SUBMISSION BY USER local: (THIS CREATES A JOB THAT ENTERS THE SGE QUEUE BUT FAILS)

cd /common/apps/iprscan/bin
iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml

qstat -j
>>>
Jobs dropped because of error state
        934,    935,    939
<<<
qstat -j 939
>>>
job_number:                 939
exec_file:                  job_scripts/939
submission_time:            Wed Dec 13 20:16:36 2006
owner:                      local
uid:                        501
group:                      local
gid:                        501
sge_o_home:                 /Users/local
sge_o_log_name:             local
sge_o_path:                 /Users/local/apps/cytoscape/PLUGINS:/usr/lib/perl5/site_perl:/usr/share/apps/ActiveTcl/bin:/usr/local/mysql/bin:/usr/local/bin:/usr/share/apps/komodo:/sw/bin:/Users/local/FUNNYBASE/bin:/common/sge/bin/darwin:/bin:/sbin:/usr/bin:/usr/sbin:/common/mpich-1.2.7/ch_p4/bin:/common/bin:/common/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin:/usr/local/biotools/bin:/Applications/bioinf/phylip/exe:/Applications/bioinf/t_coffee:/usr/local/biotools/bin
sge_o_shell:                /bin/bash
sge_o_workdir:              /common/apps/iprscan/bin
sge_o_host:                 gems
account:                    sge
stderr_path_list:           /dev/null
mail_list:                  local@gems.rsmas.miami.edu
notify:                     FALSE
job_name:                   iprscan-20061213-20163633
stdout_path_list:           /dev/null
jobshare:                   0
hard_queue_list:            all.q
env_list:                   
script_file:                /common/apps/iprscan/tmp/20061213/iprscan-20061213-20163633/iprscan-20061213-20163633.dcmd
error reason    1:          can't get password entry for user "local". Either the user does not exist or NIS error!
scheduling info:            job is in error state
<<<


iprscan JOB SUBMISSION BY USER root: (THIS DOES NOT CREATE A JOB IN THE SGE QUEUE AND DOES NOT SEEM TO RUN iprscan)

cd /common/apps/iprscan/bin
sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml
>>>
Password:
SUBMITTED iprscan-20061213-20221819

<<< (no return, just hangs)

ps aux | grep iprscan
>>>
root       859   9.4  0.9    34132   8944  p2  S+    8:22PM   0:06.06 /usr/bin/perl /usr/bin/iprscan -cli -seqtype n -i ../test.nuc.seq -iprlookup -goterms -o ../test.nuc.out.xml
local      835   0.0  0.9    34132   8940  p2  S     8:21PM   0:00.46 /usr/bin/perl /usr/bin/iprscan -cli -seqtype n -i ../test.nuc.seq -iprlookup -goterms -o ../test.nuc.out.xml
local     1541   0.0  0.0    27376    416  p4  R+    8:25PM   0:00.00 grep iprscan
<<<

CHECKED FOR JOB ERROR OR RESULTS IN iprscan'S tmp DIRECTORY:

cd /common/apps/iprscan/tmp/20061213/iprscan-20061213-20221819
ls
>>>
iprscan-20061213-20221819.dcmd     iprscan-20061213-20221819.dsub     iprscan-20061213-20221819.exitcode iprscan-20061213-20221819.params
iprscan-20061213-20221819.djob     iprscan-20061213-20221819.errors   iprscan-20061213-20221819.input    iprscan-20061213-20221819.seqs
<<<

emacs *errors
>>>
Can't load '/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle' for module XML::Parser::Expat: dlopen(/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat\
.bundle, 1): Library not loaded: /usr/local/lib/libexpat.1.dylib
  Referenced from: /RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
  Reason: image not found at /System/Library/Perl/5.8.6/darwin-thread-multi-2level/DynaLoader.pm line 230.
 at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14
Compilation failed in require at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14.
BEGIN failed--compilation aborted at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 18.
Compilation failed in require at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
Compilation failed in require at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
Compilation failed in require at /common/apps/iprscan/bin/iprscan_wrapper.pl line 50.
BEGIN failed--compilation aborted at /common/apps/iprscan/bin/iprscan_wrapper.pl line 50.
<<<

BUT WHEN I GO TO node001 AND ON THE COMMAND LINE VIEW THE PERL @INC VARIABLE, IT APPEARS /RemotePerl IS THERE:

perl -V
>>> ...
@INC:
    /RemotePerl/5.8.6/darwin-thread-multi-2level
    /RemotePerl/5.8.6
    /RemotePerl
    /System/Library/Perl/5.8.6/darwin-thread-multi-2level
    /System/Library/Perl/5.8.6
    /Library/Perl/5.8.6/darwin-thread-multi-2level
    /Library/Perl/5.8.6
    /Library/Perl
    /Network/Library/Perl/5.8.6/darwin-thread-multi-2level
    /Network/Library/Perl/5.8.6
    /Network/Library/Perl
    /System/Library/Perl/Extras/5.8.6/darwin-thread-multi-2level
    /System/Library/Perl/Extras/5.8.6
    /Library/Perl/5.8.1
<<<

Any ideas?

Cheers,

Stuart.






+++++++++</entry>



<entry [From : 	William VanEtten via RT <inquiry-support@bioteam.net>] Reply-To : 	inquiry-support@bioteam.net>


Sent : 	Wednesday, December 13, 2006 7:38 PM
To : 	youngstuart@hotmail.com
Subject : 	[bioteam.net #13817] install submit host + interproscan
	

What's your status?
Did this information help you?

Bill
> [cdwan - Thu Dec 07 11:37:17 2006]:
> 
> Stuart,
> 
> To follow up a little bit more on the interproscan thing:  I've gotten
> it to work on other inquiry systems, and it's not too difficult.
> I don't know if they've changed things recently, but a few months ago
> I just downloaded the interpro package and went through
> the install documents step by step. There were only three deviations:
> 
> * make a link from IPRSCAN/conf/sge.conf to IPRSCAN/conf/sge6.conf
> * Edit sge6.conf to remove an incorrect argument to "qsub"
> * Edit "qstat_ipr.pl" to call "qstat" with the correct arguments for
> SGE 6.
> 
> Please let me know if this works out for you.
> 
> -Chris Dwan
>  The BioTeam
> 
> > [chrisdag - Mon Dec 04 13:10:57 2006]:
> >
> > Hi Stuart,
> >


+</entry>



<entry [DIRECT SUBMISSION OF iprscan JOB ON gems] Wed Dec 13 17:36:58 PST 2006>



iprscan JOB SUBMISSION BY USER local: (THIS CREATES A JOB THAT ENTERS THE SGE QUEUE BUT FAILS)

cd /common/apps/iprscan/bin
iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml

qstat -j
>>>
Jobs dropped because of error state
        934,    935,    939
<<<
qstat -j 939
>>>
job_number:                 939
exec_file:                  job_scripts/939
submission_time:            Wed Dec 13 20:16:36 2006
owner:                      local
uid:                        501
group:                      local
gid:                        501
sge_o_home:                 /Users/local
sge_o_log_name:             local
sge_o_path:                 /Users/local/apps/cytoscape/PLUGINS:/usr/lib/perl5/site_perl:/usr/share/apps/ActiveTcl/bin:/usr/local/mysql/bin:/usr/local/bin:/usr/share/apps/komodo:/sw/bin:/Users/local/FUNNYBASE/bin:/common/sge/bin/darwin:/bin:/sbin:/usr/bin:/usr/sbin:/common/mpich-1.2.7/ch_p4/bin:/common/bin:/common/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin:/usr/local/biotools/bin:/Applications/bioinf/phylip/exe:/Applications/bioinf/t_coffee:/usr/local/biotools/bin
sge_o_shell:                /bin/bash
sge_o_workdir:              /common/apps/iprscan/bin
sge_o_host:                 gems
account:                    sge
stderr_path_list:           /dev/null
mail_list:                  local@gems.rsmas.miami.edu
notify:                     FALSE
job_name:                   iprscan-20061213-20163633
stdout_path_list:           /dev/null
jobshare:                   0
hard_queue_list:            all.q
env_list:                   
script_file:                /common/apps/iprscan/tmp/20061213/iprscan-20061213-20163633/iprscan-20061213-20163633.dcmd
error reason    1:          can't get password entry for user "local". Either the user does not exist or NIS error!
scheduling info:            job is in error state
<<<


iprscan JOB SUBMISSION BY USER root: (THIS DOES NOT CREATE A JOB IN THE SGE QUEUE AND DOES NOT SEEM TO RUN iprscan)

cd /common/apps/iprscan/bin
sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml
>>>
Password:
SUBMITTED iprscan-20061213-20221819

<<< (no return, just hangs)

ps aux | grep iprscan
>>>
root       859   9.4  0.9    34132   8944  p2  S+    8:22PM   0:06.06 /usr/bin/perl /usr/bin/iprscan -cli -seqtype n -i ../test.nuc.seq -iprlookup -goterms -o ../test.nuc.out.xml
local      835   0.0  0.9    34132   8940  p2  S     8:21PM   0:00.46 /usr/bin/perl /usr/bin/iprscan -cli -seqtype n -i ../test.nuc.seq -iprlookup -goterms -o ../test.nuc.out.xml
local     1541   0.0  0.0    27376    416  p4  R+    8:25PM   0:00.00 grep iprscan
<<<

CHECKED FOR JOB ERROR OR RESULTS IN iprscan'S tmp DIRECTORY:

cd /common/apps/iprscan/tmp/20061213/iprscan-20061213-20221819
ls
>>>
iprscan-20061213-20221819.dcmd     iprscan-20061213-20221819.dsub     iprscan-20061213-20221819.exitcode iprscan-20061213-20221819.params
iprscan-20061213-20221819.djob     iprscan-20061213-20221819.errors   iprscan-20061213-20221819.input    iprscan-20061213-20221819.seqs
<<<

emacs *errors
>>>
Can't load '/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle' for module XML::Parser::Expat: dlopen(/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat\
.bundle, 1): Library not loaded: /usr/local/lib/libexpat.1.dylib
  Referenced from: /RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
  Reason: image not found at /System/Library/Perl/5.8.6/darwin-thread-multi-2level/DynaLoader.pm line 230.
 at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14
Compilation failed in require at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14.
BEGIN failed--compilation aborted at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 18.
Compilation failed in require at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm line 36.
Compilation failed in require at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
Compilation failed in require at /common/apps/iprscan/bin/iprscan_wrapper.pl line 50.
BEGIN failed--compilation aborted at /common/apps/iprscan/bin/iprscan_wrapper.pl line 50.
<<<

BUT WHEN I GO TO node001 AND ON THE COMMAND LINE VIEW THE PERL @INC VARIABLE, IT APPEARS /RemotePerl IS THERE:

perl -V
>>> ...
@INC:
    /RemotePerl/5.8.6/darwin-thread-multi-2level
    /RemotePerl/5.8.6
    /RemotePerl
    /System/Library/Perl/5.8.6/darwin-thread-multi-2level
    /System/Library/Perl/5.8.6
    /Library/Perl/5.8.6/darwin-thread-multi-2level
    /Library/Perl/5.8.6
    /Library/Perl
    /Network/Library/Perl/5.8.6/darwin-thread-multi-2level
    /Network/Library/Perl/5.8.6
    /Network/Library/Perl
    /System/Library/Perl/Extras/5.8.6/darwin-thread-multi-2level
    /System/Library/Perl/Extras/5.8.6
    /Library/Perl/5.8.1
<<<



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
RUN INTERPROSCAN:

iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
DOWNLOAD LATEST DOCUMENTATION (Fri 8 Dec 2006)

ftp://ftp.ebi.ac.uk/pub/databases/interpro/
ftp://ftp.ebi.ac.uk/pub/databases/interpro/release_notes.txt
ftp://ftp.ebi.ac.uk/pub/databases/interpro/user_manual.txt

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Thursday, December 7, 2006 4:37 PM
To : 	youngstuart@hotmail.com
Subject : 	[bioteam.net #13817] install submit host + interproscan


Stuart,

To follow up a little bit more on the interproscan thing:  I've gotten it to
work on other inquiry systems, and it's not too difficult.  
I don't know if they've changed things recently, but a few months ago I just
downloaded the interpro package and went through 
the install documents step by step. There were only three deviations:

* make a link from IPRSCAN/conf/sge.conf to IPRSCAN/conf/sge6.conf
* Edit sge6.conf to remove an incorrect argument to "qsub"
* Edit "qstat_ipr.pl" to call "qstat" with the correct arguments for SGE 6.

Please let me know if this works out for you.

-Chris Dwan
 The BioTeam

> [chrisdag - Mon Dec 04 13:10:57 2006]:
> 
> Hi Stuart,
> 
> I've been communicating with the interproscan support staff in the  
> hopes of getting an HOWTO up at the http://gridengine.info wiki site  
> - no word on that yet but the word from their support person is that  
> they are aware of SGE, have people using SGE and (perhaps) they have  
> SGE integration documents or files already in their current release.
> 
> The most recent info I can find that does not come from the  
> interproscan developers is covered here:
> http://gridengine.info/articles/tag/interproscan
> 
> Regarding SGE submit host ...
> 
> This should be pretty easy, you don't need a NFS mount if all you  
> want to do is submit work and query status but if a NFS mount is  
> possible it represents the easiest way to get it to work.
> 
> The main requirements are:
> 
> (1) You run "qconf -as <hostname>" on the SGE system to add an entry  
> for this new submit host. Your submit host should have a forward and  
> reverse DNS entry ideally so that the SGE qmaster recognizes the  
> connection when it comes in and can then match that connection to the  
> hostname you used when registering it with "qconf -as".
> 
> (2) Your submit hosts needs local copies of the SGE binaries and it  
> needs information on how to contact the SGE qmaster machine and which  
> TCP ports it should be using for that communication. For me, the  
> easiest way to do this was to just replicate (via rsync) the /common/ 
> sge/ directory onto my laptop - that moves over both the binaries as  
> well as all the setup and communication configuration parameters.  
> Then all I needed to do on my laptop was initialize the SGE  
> environmental setup scripts --> "source /common/sge/default/common/settings.sh"
> and I was all set.
> 
> Regards,
> Chris Dagdigian
> BioTeam
> 
> 
> 
> On Dec 4, 2006, at 1:01 PM, stuart young via RT wrote:
> 
> >
> > Mon Dec 04 13:01:02 2006: Request 13817 was acted upon.
> > Transaction: Ticket created by youngstuart@hotmail.com
> >        Queue: iNquiry Support
> >      Subject: install submit host + interproscan
> >        Owner: Nobody
> >   Requestors: youngstuart@hotmail.com
> >       Status: new
> >  Ticket <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >
> >
> > Hi Bill/Chris,
> >
> > I have two questions I was wondering if you could help me with:
> >
> > 1. I'd like to install SGE as a 'submit host' on a machine that is
> > not part of our Inquiry-enabled cluster. Could you point me in the
> > direction of any related instructions/FAQs. In particular, I'd like
> > to know:
> >    - if I need to do an NFS mount of /common/sge or any other
> > directories on my new submit host.
> >    - And if so, how do I go about it? I've read the manuals about
> > installing master, shadow, submit, execution hosts but it's not clear
> > to me - should I install the submit host as an execution host and
> > then add it as a submit host on the master host?
> >
> > 2. How do I go about using the job processing capabilities of SGE
> > with Interproscan?
> >
> > Thanks!
> >
> > Stuart.
> >
> > PS: I emailed the inquiry-support@bioteam.net address but it bounced
> > back. Is that the right email for the BioTeam?
> >
> > ============
> > Stuart Young
> > Research Assistant
> > Crawford Lab
> > RSMAS
> > University of Miami
> >
> >


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Dec  5 07:53:45 EST 2006

SUBMIT HOST INSTALLATION

From : 	dag via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Monday, December 4, 2006 6:11 PM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] install submit host + interproscan
	
	
Go to previous message	|	Go to next message	|	Delete	|	Inbox

Hi Stuart,

I've been communicating with the interproscan support staff in the  
hopes of getting an HOWTO up at the http://gridengine.info wiki site  
- no word on that yet but the word from their support person is that  
they are aware of SGE, have people using SGE and (perhaps) they have  
SGE integration documents or files already in their current release.

The most recent info I can find that does not come from the  
interproscan developers is covered here:
http://gridengine.info/articles/tag/interproscan

INTERPROSCAN
============

*** Michael Gang michael.gang at evogene.com
Mon Sep 4 06:09:01 EDT 2006

Dear all,


Well, it looks like we've got Interpro running on the cluster with the 
Sun Grid Engine queuing system.


Steps needed to make this work (technical details):

1 - Allow job submission from all nodes (!)
2 - Update qstat_ipr.pl in the iprscan/bin directory (the patch from 
interpro should work, basically-you need to use the -f flag for qstat).
3 - Create a symbolic link to iprscan/conf/sge6.env under 
iprscan/conf/sge.env
3 - Change the following line in iprscan/sge6/conf

asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e 
/dev/null [%env IPRSCAN_HOME]/conf/sge6-test.sh [%toolcmd]

To this:
asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e 
/dev/null [%toolcmd]

Best regards,
Michael Gang
Evogene Bioinformatics

P.S. I would like to specially thank Mr. Powers for his great help



*** [Bioclusters] running interpro at all
Michael James Michael.James at csiro.au
Tue Sep 5 00:06:20 EDT 2006

    * Previous message: [Bioclusters] running interpro on cluster
    * Next message: [Bioclusters] Re: running interpro at all
    * Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]

Before tackling interpro over sge6 I tried a single host install.

When interpro sets itself up it creates a file in  conf/localenv.sh
When a job is submitted this becomes <jobname>.dsub

It tries unsuccessfully to use a local command instead of rsh
 when we would be rsh-ing back the the same host.
To fix this bug and some limitations, change the file to:


#!/bin/sh
rhost=[%host.exec]
ihost=`hostname`
echo "[%toolexitcode]"

case $rhost in
    ''|localhost|$ihost*)
            [%cmd] ;;
    *)
            rsh $rhost "[%cmd]";;
esac
exit 0


Explanation:
Without a dollars sign on  "ihost"  it will always use rsh.
If you make the test string      case $rhost in   ''|localhost|$ihost*)
 it allows   host.exec   to be empty or localhost.
This allows a non-host-specific install,
 so you can just duplicate it across your cluster
 without having to run the install separately for each node.

michaelj

-- 
Michael James                         michael.james at csiro.au
System Administrator                    voice:  02 6246 5040
CSIRO Bioinformatics Facility             fax:  02 6246 5166

No matter how much you pay for software,
 you always get less than you hoped.
Unless you pay nothing, then you get more.

*** integrating interpro and interproscan with grid engine

Posted by chris on 09/04/2006
Questions about InterPro integration on SGE systems seem to come up fairly frequently. I thought there was a HOWTO or wiki document somewhere but I can't seem to find it now.
Recently though, Michael Gang made the following post to the bioclusters mailing list that covers the steps he needed to perform:

    "... Steps needed to make this work (technical details):

    1 - Allow job submission from all nodes (!)
    2 - Update qstat_ipr.pl in the iprscan/bin directory (the patch from interpro should work, basically-you need to use the -f flag for qstat).
    3 - Create a symbolic link to iprscan/conf/sge6.env under iprscan/conf/sge.env
    3 - Change the following line in iprscan/sge6/conf

    asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e /dev/null [%env IPRSCAN_HOME]/conf/sge6-test.sh [%toolcmd]

    To this:
    asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e /dev/null [%toolcmd]
    "


*** [Bioclusters] Questions about setting up InterProScan on clusters
Sarah Hunter hunter at ebi.ac.uk
Tue Sep 5 06:58:44 EDT 2006

    * Previous message: [Bioclusters] Re: running interpro at all
    * Next message: [Bioclusters] Questions about setting up InterProScan on clusters
    * Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]

Hi everyone,

I just wanted to post a quick message to the mailing list to firstly apologise if there have been 
any problems with setting up InterProScan on your clusters - unfortunately, we only have LSF 
in-house and we therefore can't test on other systems like SGE or PBS.  However, I always try to 
hazard a guess when we get questions about those systems. Specifically, the SGE configuration files 
themselves were re-written for SGE6 by me having remote access to a user's installation (thank you 
Jenny!) although I soon discovered that that installation had its own quirks which were not standard 
for other systems.

The fact that you guys are here supporting each other when installing the software on these atypical 
(to EBI, anyway) set-ups is a great help - particularly because we only have a limited number of 
people who can answer support emails (i.e. mainly me).

I just want to reiterate that if you have any questions about setting InterProScan on your local 
clusters, please email support (at) ebi.ac.uk or interhelp (at) ebi.ac.uk.  If you don't get a reply 
within a couple of days, hassle us again :-)

At the same time, it would be nice to encourage the development of a community of InterProScan users 
who have the program installed on different clusters, so that if anyone has a question about a 
queue-specific issue, the community could be consulted and alterations tested, rather than relying 
on our limited resources at EBI.  What do people think about this?

Sorry if you emailed and didn't get a satisfactory response.  And thanks for all the hard work you 
have evidentially put in in order to get InterProScan working.

Best regards,

Sarah Hunter

-----------------------------------------
  InterPro Programming Coordinator
  E.B.I.
  Wellcome Trust Genome Campus
  Hinxton
  CB10 1SD
-----------------------------------------


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Nov 27 14:36:34 PST 2006

REINSTALLED iprscan ON gems:

sudo ./Config.pl 
Password:

####################################################################################################
#                Welcome to the InterProScan v4.2 installation script.
#
#  Tips:
#     - If you are happy with the suggestion between [], just press <enter>
#     - Whatever is shown between '/' and '/' is considered a valid input pattern
####################################################################################################


    Reconfigure everything? (first time install) [n] /(y|n)/? : n

! Tip:
!
! InterProScan needs to know where it is installed and where perl is installed
!
    Do you want to set paths to perl and the installation directory? [n] /y|n/? : y

! Tip:
!
! If your servers are using a shared file system, such as NFS, InterProScan is able to utilise this
! and perform distributed computing across multiple servers
! You will need set up the full UNIX path to the installation directory.
! You also need to ensure that the InterProScan installation is on a SHARED DISK.
!
    Please enter the full path for the InterProScan installation [/common/apps/iprscan] /.+/? : 
    Do you want to set another Perl command in place of [/usr/bin/perl]? [n] /y|n/? : n
>>Changing Perl path in scripts ... >>DONE

>>Checking if your Perl installation has the modules required by InterProScan...
>>All modules needed are already installed.


! Tip:
!
! To cope with bulk jobs and to enable efficient parallelization, InterProScan splits input files
! with FASTA formatted sequences into smaller parts (chunks). Default size is "10" sequences per chunk
!
    Do you want to setup chunk size [n] /(y|n)/? : n


! Tip:
!
! You can configure InterProScan to limit the number of input sequences (protein & nucleotide),
! set a maximum length for nucleotide sequences and a minimum length for protein sequences.
! You can also configure the minimum length for a translated orf and use the codon table value
! to translate the dna/rna sequence into six frames (http://www.ebi.ac.uk/cgi-bin/mutations/trtables.cgi).
!
    Do you want to configure this? [n] /(y|n)/? : n


! Tip:
!
! InterProScan currently encorporates searching against 13 databases. You have the choice whether or not to
! set up a particular search tool against a database and how exactly that application will be configured to run
! in this next section. You will configure the queue systems (if any) you will use, first
!
    Do you want to setup applications (if you don't, no applications will be included in InterProScan by default)? [n] /(y|n)/? : y

! Tip:
!
! InterProScan is able to execute the applications via a UNIX QUEUING system which has an INTERACTIVE mode (such as LSF).
! Currently, we mainly support LSF (which is used at EBI) however, we also provide configuration files for other systems
! which are listed in the documentation
!
    Do you wish to use a queue system? [n] /(y|n)/? : y
!
! VERY IMPORTANT NOTICE: 
! Applications from InterProScan will use a specific directory (iprscan/bin/binaries) to find application binaries
! and execute them.  'binaries' is a symbolic link pointing to a directory with a name that corresponds to the
! OS architecture (OSF1|Linux|AIX|SunOS|Darwin|IRIX).  If you have different architectures on any of the nodes
! used by your queueing system, you must make sure that the link they see points to the appropriate architecture
! directory.
! (e.g.) Host A is Linux, Host B is OSF1.  Be sure that, on host A, the binaries link points to a directory called
!       'Linux' and on host B, to 'OSF1'
!

Please choose one queueing system from the list. If you don't want to use this queueing system, just type '<enter>'

    Do you want to use pbs54? [n] /(y|n)/? : n
    Do you want to use lsf42? [n] /(y|n)/? : n
    Do you want to use sge6? [n] /(y|n)/? : y
!
! You can now configure global settings for your queue system for all the applications (such as a ! global cluster name and/or a global queue name)
!
    Do you want to set a global cluster name for all applications? [n] /(y|n)/? : n
    Do you want to set a global queue name for all applications? [n] /(y|n)/? : n
>>You will run InterProScan with sge6 queueing system using:
    
Is all this information correct? [n] /y|n/? : n
!
! You can now configure global settings for your queue system for all the applications (such as a ! global cluster name and/or a global queue name)
!
    Do you want to set a global cluster name for all applications? [n] /(y|n)/? : y
    Please enter the global cluster name you want to use for all applications [] /([^\s\;\,\:]+)/? : gems.rsmas.miami.edu
    Do you want to set a global queue name for all applications? [n] /(y|n)/? : all.q
WARNING: The information input is not valid (doesn't match /(y|n)/), please try again.
    Do you want to set a global queue name for all applications? [n] /(y|n)/? : y
    Please enter the name of the global queue you want to use for all applications [] /([^\s\;\,\:]+)/? : all.q     
>>You will run InterProScan with sge6 queueing system using:
>>      -Global cluster name 'gems.rsmas.miami.edu'.
>>      -Global queue name 'all.q'.
    
Is all this information correct? [n] /y|n/? : y
>>Checking your local architecture for InterProScan compatibility

! Tip:
! Blastprodom (a wrapper script on top of Blast package) is used to search against PRODOM families
    Do you want to use blastprodom ? [y] /(y|n)/? : y

! Tip:
! Coils is used to predict coiled-coil segments with the algorithm of Lupas et al.
    Do you want to use coils ? [y] /(y|n)/? : y

! Tip:
! Gene3d Gene3D is supplementary to the CATH database. This protein sequence database contains proteins from complete genomes which have been clustered into protein families and annotated with CATH domains, Pfam domains and functional information from KEGG, GO, COG, Affymetrix and STRINGS.
    Do you want to use gene3d ? [y] /(y|n)/? : y

! Tip:
! Hmmpanther (hmmpfam from HMMER 2.3.2 package) PANTHER classifies proteins into families and specific functional subfamilies, which are then mapped to ontology terms and pathways.
    Do you want to use hmmpanther ? [y] /(y|n)/? : y

! Tip:
! Hmmpir PIR produces the Protein  Sequence Database (PSD) of functionally annotated protein sequences, which grew out of the Atlas of Protein Sequence and Structure (1965-1978) edited by Margaret Dayhoff and has been incorporated into an integrated knowledge base system of value-added databases and analytical tools.
    Do you want to use hmmpir ? [y] /(y|n)/? : y

! Tip:
! Hmmpfam (hmmpfam from HMMER 2.3.2 package) is used to search against the Pfam HMM (Profile hidden Markov models) database
    Do you want to use hmmpfam ? [y] /(y|n)/? : y

! Tip:
! Hmmsmart (hmmpfam from HMMER 2.3.2 package) is used to search against the Smart HMM (Profile hidden Markov models) database
    Do you want to use hmmsmart ? [y] /(y|n)/? : y

! Tip:
! Hmmtigr (hmmpfam from HMMER 2.3.2 package) is used to search against TIGRFAMs collection of HMMs (Profile hidden Markov models)
    Do you want to use hmmtigr ? [y] /(y|n)/? : y

! Tip:
! Fprintscan (FingerPRINTScan) is used to search against the PRINTS collection of protein signatures
    Do you want to use fprintscan ? [y] /(y|n)/? : y

! Tip:
! Scanregexp (ScanProsite) is used to search against the PROSITE patterns collection and verify the  matches by statistically significant CONFIRM patterns
    Do you want to use scanregexp ? [y] /(y|n)/? : y

! Tip:
! Profilescan (ProfileScan (pfscan & ps_scan.pl) from Pftools 2.2) package is used  to search against the PROSITE profiles collection
    Do you want to use profilescan ? [y] /(y|n)/? : y

! Tip:
! Superfamily A structural classification of proteins database for the investigation of sequences and structures.
    Do you want to use superfamily ? [y] /(y|n)/? : y

! Tip:
! Seg is used for identifying and masking segments of low compositional complexity in amino acid sequences
    Do you want to use seg ? [y] /(y|n)/? : y

! Tip:
! Signalp predicts the presence and location of signal peptide cleavage sites, using eukaryotic HMM (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use signalp ? [n] /(y|n)/? : y

! Tip:
! Tmhmm is used for prediction of transmembrane helices in proteins using a Hidden Markov Model (under commercial license : contact software@cbs.dtu.dk)
    Do you want to use tmhmm ? [n] /(y|n)/? : y

! Tip:
!
! InterProScan can use taxonomy-based filtering of results which uses the InterPro taxonomy listed for each InterPro
! entry to hide hits not relevant to that taxon.
!
    Do you want to make it available for users? [y] /y|n/? : y

! Tip:
!
! You can specify an email address for the administrator. By default it will be you (root)
! Administrators receive an email when a problem occurs in InterProScan.
!
    Do you want to set an administrator email address? [y] /y|n/? : y                     
    Please enter the email address of the administrator: [] /[\w\.\-]+\@[\w\.\-]+/? : syoung@rsmas.miami.edu
>>Writing to the configuration files ... 
>>Processing file : /common/apps/iprscan/conf/blastprodom.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/coils.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/fprintscan.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/gene3d.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpanther.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpfam.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmpir.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmsmart.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/hmmtigr.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/profilescan.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/scanregexp.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/seg.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/signalp.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/superfamily.conf ... >>DONE
>>Processing file : /common/apps/iprscan/conf/tmhmm.conf ... >>DONE
>>Setting File /common/apps/iprscan/conf/iprscan.conf ... >>DONE

! Tip:
!
! InterProScan can be launched through a web interface. It can also be launched through secure HTTP (https)
!
    Do you want to run InterProScan using a web interface? [n] /y|n/? : y
    Do you want to run InterProScan through a secure HTTP server? [n] /y|n/? : n
    Enter the name of your server. You can specify a specific port to listen to. (e.g. "foo.bar.com:4000") [foobar.com] /[\w\.\_\-\:\d]+/? : gems.rsmas.miami.edu
!
! Please add the following lines to your web server configuration (httpd.conf):
#--------------------------------------
 Alias /doc/ "/common/apps/iprscan/doc/html/"
 Alias /images/ "/common/apps/iprscan/images/"
 Alias /tmp/ "/common/apps/iprscan/tmp/"

 ScriptAlias /iprscan/ "/common/apps/iprscan/bin/"

 <Directory "/common/apps/iprscan/images">
   Options Indexes MultiViews
   AllowOverride None
   Order allow,deny
   Allow from all
 </Directory>

 <Directory "/common/apps/iprscan/tmp/">
   Options FollowSymLinks Includes SymLinksIfOwnerMatch
    IndexIgnore */.??* *~ *# */HEADER* */README* */RCS
    AllowOverride AuthConfig Limit FileInfo

    Order deny,allow
    Deny from all
    Allow from .rsmas.miami.edu
 </Directory>
#--------------------------------------
! Be sure that 'apache' or 'httpd' virtual user (uid/gid) have right to read/write into your /common/apps/iprscan iprscan directory.
! Otherwise, if you are running a standalone http(s) web server, be sure also the user running it have to enough
! rights to read/wrtie /common/apps/iprscan iprscan directory
! If you encoutered problem with your web server running iprscan, read the FAQ or contact your system administrator.

! After restarting your web server try http://gems.rsmas.miami.edu/iprscan/iprscan
!
>>Indexing datafile ... 
No specific file(s) given, indexing all files:

Pfam
prints.pval
prodom.ipr
Pfam-C
smart.desc
sf_hmm
sf.seq
Pfam-A.seed
superfamily.hmm
smart.thresholds
smart.HMMs
match.xml
interpro.xml
TIGRFAMs_HMM.LIB
Gene3D.hmm


No specific file(s) given, converting all files:

Pfam
sf_hmm
superfamily.hmm
smart.HMMs
sf_hmm_sub
TIGRFAMs_HMM.LIB
Gene3D.hmm

WARNING: Some problems occured when trying to index the data files.
 Please fix the error below and index your datafiles using the /common/apps/iprscan/bin/index_data.pl script.
 For help, type './index_data.pl -h' 
errormsg: ERROR: File [/common/apps/iprscan/data/sf.seq] not found : No such file or directory at /common/apps/iprscan/bin/index_data.pl line 217.
>>DONE

! Tip:
!
!  We would appreciate it if you would register your installation of InterProScan.  This will allow us to notify you
! of bug fixes and new releases of the software. We will not use your email address for any other purpose.
!


NB: FROM FAQ.txt:
=====================================================

> I am using a queueing system, I looked at the

> configuration file but I don't understand what

> are the special tag 'optqueue' and 'optresource'

=====================================================



'optqueue' is the queue name to use for an application

for example and optresource is the name of a resource

to use for this application. If you don't know what are

this terms, contact you system administrator.



Each time an application is launched, InterProScan reads its

configuration file, and launches the job. To launch the job, it

looks what is the method to do it. Queue or Local implementation?

This is mentionned in the tag 'queue' of each applications.

Then, if the queue is a queue like LSF for example (lsf42.conf)

then it launches the application using lsf command and specific queue

name and resource on the command line if specified in the application

configuration file (search for 'resource' and 'queue.name').

Thus in the command line of lsf, optqueue is replaced by the value

of the 'queue.name' tag (for this application) and optresource is then

replaced by 'resource' tag (for this application).



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
SAVE TIME IN SCANS:

http://www.ebi.ac.uk/interpro/User-FAQ-Scan.html#N885

The usefulness of InterPro comes from the fact that it incorporates all the member databases and has some level of redundancy. Each database has something different to offer, either by specifying small highly conserved site or a longer domain or specific fingerPrints diagnostic of protein families or subfamilies. Matches to multiple signatures in one InterPro entry also serve to increase the significance of a hit. By removing these you will lose the effectiveness of the database and InterProScan. Are you running your sequences through InterProScan on the web or using a local installation? If you plan to run many sequences it may be better to have the package running locally. All information and software for this is available from the ftp site: ftp://ftp.ebi.ac.uk/pub/databases/interpro/ If you have just a few sequences to run perhaps you can send them off to run during the night so that time is not an issue. The slowest application in iprscan is hmmpfam, especially when it is run against Pfam as it is a huge HMM profile database. The main thing is that Pfam a the highest coverage among the databases integrated into InterPro. Thus it is a shame not to run it. If you really need to save time, may be you can try to inactivate SMART and TIGRFAMs scanning. They are both HMM profile databases like Pfam, but one is eukaryote specific and the other is prokaryote specific while Pfam deals with both eukaryotes and prokaryotes. The results will not be fully complete but you should save quite a lot of time.


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
RUNNING iprscan2.pl AS AN ARRAY JOB ON THE CLUSTER:

PROBLEM:

em orthologues.iprscan.sh.o910.1

Using default conf file: conf/default.conf
Inputfile: orthologues.fasta.1
Outputfile: /orthologues.fasta.1
[Mon Nov 27 10:35:49 2006] iprscan: Can't locate XML/Parser.pm in @INC (@INC contains: /common/apps/iprscan/lib /System/Library/Perl/5.8.\
6/darwin-thread-multi-2level /System/Library/Perl/5.8.6 /Library/Perl/5.8.6/darwin-thread-multi-2level /Library/Perl/5.8.6 /Library/Perl \
/Network/Library/Perl/5.8.6/darwin-thread-multi-2level /Network/Library/Perl/5.8.6 /Network/Library/Perl /System/Library/Perl/Extras/5.8.\
6/darwin-thread-multi-2level /System/Library/Perl/Extras/5.8.6 /Library/Perl/5.8.1 .) at /common/apps/iprscan/lib/Dispatcher/Tool/InterPr\
oScanParser.pm line 34.
[Mon Nov 27 10:35:49 2006] iprscan: BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScanParser.pm l\
ine 34.
[Mon Nov 27 10:35:49 2006] iprscan: Compilation failed in require at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52.
[Mon Nov 27 10:35:49 2006] iprscan: BEGIN failed--compilation aborted at /common/apps/iprscan/lib/Dispatcher/Tool/InterProScan.pm line 52\
.
[Mon Nov 27 10:35:49 2006] iprscan: Compilation failed in require at /common/apps/iprscan/bin/iprscan line 56.
[Mon Nov 27 10:35:49 2006] iprscan: BEGIN failed--compilation aborted at /common/apps/iprscan/bin/iprscan line 56.


SOLUTION: INSTALL expat AND THEN XML-Parser

TRIED INSTALL XML-Parser WITHOUT expat:

gems:~/FUNNYBASE/NOTES/plmods/XML-Parser/XML-Parser-2.34 local$ perl Makefile.PL 
Note (probably harmless): No library found for -lexpat

Expat must be installed prior to building XML::Parser and I can't find
it in the standard library directories. You can download expat from:

http://sourceforge.net/projects/expat/

If expat is installed, but in a non-standard directory, then use the
following options to Makefile.PL:

    EXPATLIBPATH=...  To set the directory in which to find libexpat

    EXPATINCPATH=...  To set the directory in which to find expat.h

For example:

    perl Makefile.PL EXPATLIBPATH=/home/me/lib EXPATINCPATH=/home/me/include

Note that if you build against a shareable library in a non-standard location
you may (on some platforms) also have to set your LD_LIBRARY_PATH environment
variable at run time for perl to find the library.


INSTALL expat:

gems:~/FUNNYBASE/NOTES/plmods/XML-Parser/expat-2.0.0 local$ sudo make install
----------------------------------------------------------------------
Libraries have been installed in:
   /usr/local/lib

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the `-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the `DYLD_LIBRARY_PATH' environment variable
     during execution

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
----------------------------------------------------------------------


INSTALL XML-Parser:
gems:~/FUNNYBASE/NOTES/plmods/XML-Parser/XML-Parser-2.34 local$ perl Makefile.PL 
Checking if your kit is complete...
Looks good
Writing Makefile for XML::Parser::Expat
Writing Makefile for XML::Parser


THEN DID sudo /usr/libexec/locate.updatedb ON EACH NODE (EXCEPT node015 WHICH NEEDED RESTARTING)

BUT STILL COULDN'T FIND IT:

em orthologues.iprscan.sh.o913.1
[Mon Nov 27 11:24:15 2006] iprscan: Can't locate XML/Parser.pm in @INC (@INC contains: /common/apps/iprscan/lib /System/Library/Perl/5.8.\
6/darwin-thread-multi-2level /System/Library/Perl/5.8.6 /Library/Perl/5.8.6/darwin-thread-multi-2level /Library/Perl/5.8.6 /Library/Perl \
/Network/Library/Perl/5.8.6/darwin-thread-multi-2level /Network/Library/Perl/5.8.6 /Network/Library/Perl /System/Library/Perl/Extras/5.8.\
6/darwin-thread-multi-2level /System/Library/Perl/Extras/5.8.6 /Library/Perl/5.8.1 .) at /common/apps/iprscan/lib/Dispatcher/Tool/InterPr\
oScanParser.pm line 34.
...

TRIED REBOOTING gems BUT GOT SAME ERROR.

NEXT TRIED REINSTALLING XML-Parser
perl Makefile.PL

(NB: DIDN'T WORK WITH PREFIX:   perl Makefile.PL DESTDIR=/System/Library/Perl/5.8.6/darwin-thread-multi-2level )

AND AFTER sudo updb RETRIED orthologuesiprscan.pl AND FOUND NODES ARE FINDING XML-Parser BUT NOT THE expat LIBRARY:

Can't load '/RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle' for module XML::Parser::Expat: dlopen(/Remo\
tePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle, 1): Library not loaded: /usr/local/lib/libexpat.1.dylib
  Referenced from: /RemotePerl/5.8.6/darwin-thread-multi-2level/auto/XML/Parser/Expat/Expat.bundle
  Reason: image not found at /System/Library/Perl/5.8.6/darwin-thread-multi-2level/DynaLoader.pm line 230.
 at /RemotePerl/5.8.6/darwin-thread-multi-2level/XML/Parser.pm line 14


ON THE NODES, THE MOUNTS ARE IN /var/automount:

ls /var/automount
Groups     RemotePerl Users      common

SO REINSTALLED EXPAT ON gems TO THE /common/lib DIRECTORY:

gems:~/FUNNYBASE/NOTES/plmods/XML-Parser/expat-2.0.0 local$

./configure --prefix=/common
make
sudo make install


( NB: IT WAS ALREADY INSTALLED FROM WHEN I WAS TESTING THE XML PARSERS IN THE SUMMER:
gems:/common/lib local$ ll libex*
-rwxr-xr-x   1 root  wheel    263K Sep 27  2005 libexpat.0.1.0.dylib
lrwxr-xr-x   1 root  wheel     20B Jul 21 18:03 libexpat.0.dylib -> libexpat.0.1.0.dylib
-rw-r--r--   1 root  wheel    326K Jul 21 18:03 libexpat.a
lrwxr-xr-x   1 root  wheel     20B Jul 21 18:03 libexpat.dylib -> libexpat.0.1.0.dylib )

THEN REINSTALLED XML-Parser AND TELL IT WHERE libexpat IS:

    #export LD_LIBRARY_PATH=/common/lib
    #echo $LD_LIBRARY_PATH
    #
    #export LD_LIBRARY_PATH=/common/lib
    #echo $LD_LIBRARY_PATH

    perl Makefile.PL EXPATINCPATH=/common/include EXPATLIBPATH=/common/lib    
    make EXPATINCPATH=/common/include EXPATLIBPATH=/common/lib
    sudo make install EXPATINCPATH=/common/include EXPATLIBPATH=/common/lib

BUT STILL SAME ERROR.


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
RUNNING INTERPROSCAN WITH SGE:

Michael Gang michael.gang at evogene.com
Mon Sep 4 06:09:01 EDT 2006

Dear all,

Well, it looks like we've got Interpro running on the cluster with the 
Sun Grid Engine queuing system.

Steps needed to make this work (technical details):

1 - Allow job submission from all nodes (!)
2 - Update qstat_ipr.pl in the iprscan/bin directory (the patch from 
interpro should work, basically-you need to use the -f flag for qstat).
3 - Create a symbolic link to iprscan/conf/sge6.env under 
iprscan/conf/sge.env
3 - Change the following line in iprscan/conf/sge6.conf

asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e 
/dev/null [%env IPRSCAN_HOME]/conf/sge6-test.sh [%toolcmd]

To this:
asyncsub=qsub -N [%jobid] [%optqueue] -shell n -b y -o /dev/null -e /dev/null [%toolcmd]

Best regards,
Michael Gang
Evogene Bioinformatics

P.S. I would like to specially thank Mr. Powers for his great help


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Sun Nov 12 20:42:22 PST 2006

#!/usr/bin/perl -w
# NAME: test.iprscan.pl
use strict;
print `date`;
`iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -appl hmmpfam -goterms -o ../test.nuc.pfam.out.xml`;
print `date`;


sudo ./test.iprscan.pl
Sun Nov 12 15:39:45 PST 2006
SUBMITTED iprscan-20061112-15394551
Sun Nov 12 16:00:28 PST 2006



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
FIXED PROBLEM ON gems:

PROBLEM:
    SHORTLY (~1 MIN) AFTER EXECUTING A TEST RUN:
    
    
    IPRSCAN WOULD STOP, WITH THIS ERROR IN THE *errors FILE:
    
    fork: Resource temporarily unavailable.    

    E.G., ONE OF THE FULL *errors FILES:
    
    emacs -nw /common/apps/iprscan/tmp/20061112/iprscan-20061112-11221113/iprscan-20061112-11221113.errors

    submitJobs: submission failed: failed to execute local submit: /common/apps/iprscan/tmp/20061112/iprscan-20061112-11221113/chunk_3/iprsca\
    n-20061112-11221113-scanregexp-cnk3.dsub exited with status 32768<br>/common/apps/iprscan/tmp/20061112/iprscan-20061112-11221113/chunk_3/\
    iprscan-20061112-11221113-scanregexp-cnk3.dsub: fork: Resource temporarily unavailable<br><br><br>To help us solve the problem, please co\
    py and paste this error<br>and email it to the administrator <a href="mailto:syoung@rsmas.miami.edu?subject=Error in InterProScan job ipr\
    scan-20061112-11221113">syoung@rsmas.miami.edu</a>.<br><br>Sorry about the inconvenience and thank you for using InterProScan.<br><br>

    WHERE THE PROGRAMME AT WHICH iprscan STOPPED (scanregexp IN THIS CASE) WAS DIFFERENT EVERY TIME. E.G.,
    
    emacs -nw /common/apps/iprscan/tmp/20061111/iprscan-20061111-23091688/iprscan-20061111-23091688.errors

    submitJobs: submission failed: failed to execute local submit: /common/apps/iprscan/tmp/20061111/iprscan-20061111-23091688/chunk_3/iprsca\
    n-20061111-23091688-profilescan-cnk3.dsub exited with status 32768<br>/common/apps/iprscan/tmp/20061111/iprscan-20061111-23091688/chunk_3\
    /iprscan-20061111-23091688-profilescan-cnk3.dsub: fork: Resource temporarily unavailable<br><br><br>To help us solve the problem, please \
    copy and paste this error<br>and email it to the administrator <a href="mailto:syoung@rsmas.miami.edu?subject=Error in InterProScan job i\
    prscan-20061111-23091688">syoung@rsmas.miami.edu</a>.<br><br>Sorry about the inconvenience and thank you for using InterProScan.<br><br>

    emacs -nw /common/apps/iprscan/tmp/20061110/iprscan-20061110-11120405/iprscan-20061110-11120405.errors
    
    submitJobs: submission failed: failed to exsendmail: warning: fork: Resource temporarily unavailable
    can-20061110-11120405/chunk_3/iprscan-20061110-11120405-superfamily-cnk3.dsub exited with status 32768<br><br>To help us solve the proble\
    m, please copy and paste this error<br>and email it to the administrator <a href="mailto:syoung@rsmas.miami.edu?subject=Error in InterPro\
    Scan job iprscan-20061110-11120405">syoung@rsmas.miami.edu</a>.<br><br>Sorry about the inconvenience and thank you for using InterProScan\
    .<br><br>


SOLUTION:

    1. INCREASED THE MAX NUMBER OF PROCESSES PER USER TO 1000 BY CREATING A
    /etc/sysctl.conf FILE COPIED FROM /etc/sysctl-macosxserver.conf AND EDITING
    TO READ:
    
    # This limits the number of processes per user.
    #
    kern.maxprocperuid=1000

    AND REBOOTED.
    
    RERAN IPRSCAN AND DID top:
    
Processes:  253 total, 58 running, 195 sleeping... 576 threads         12:11:53
Load Avg:  82.52, 62.17, 34.07     CPU usage:  88.1% user, 11.9% sys, 0.0% idle
SharedLibs: num =  139, resident = 9.16M code, 1.30M data, 3.49M LinkEdit
MemRegions: num =  9522, resident =  462M + 7.30M private, 21.9M shared
PhysMem:  88.5M wired,  499M active,  424M inactive, 1012M used, 11.7M free
VM: 10.9G + 89.2M   29390(1) pageins, 8426(0) pageouts

  PID COMMAND      %CPU   TIME   #TH #PRTS #MREGS RPRVT  RSHRD  RSIZE  VSIZE
 6601 top          1.9%  0:00.50   1    19    22  1.20M   288K  1.63M  27.0M 
 6546 hmmpfam      2.0%  0:00.73   2    31    39  1.72M+  420K  2.05M+ 35.6M+
 6528 perl         0.0%  0:00.06   1    13    18  1.55M   220K  1.82M  26.7M
 6527 sh           0.0%  0:00.00   1    13    16   128K   568K   632K  27.2M
 6402 hmmpfam      2.5%  0:01.49   2    31    39   780K+  420K  1.11M+ 35.6M-
 6392 perl         0.0%  0:00.07   1    13    18  1.55M   220K  1.82M  26.7M
 6391 sh           0.0%  0:00.00   1    13    16   128K   568K   632K  27.2M
 6137 hmmpfam      2.1%  0:02.76   2    31    42   884K+  420K  1.21M+ 36.3M-
 6132 hmmpfam      2.2%  0:02.70   2    31    42  1.09M+  420K  1.43M- 36.3M-
 6131 perl         0.0%  0:00.06   1    13    18  1.55M   220K  1.82M  26.7M
 6130 sh           0.0%  0:00.00   1    13    16   112K   568K   632K  27.2M
 6121 perl         0.0%  0:00.06   1    13    18  1.55M   220K  1.82M  26.7M
 6120 sh           0.0%  0:00.00   1    13    16   112K   568K   632K  27.2M
 6114 hmmpfam      1.8%  0:02.67   2    31    45  3.81M+  420K  4.14M+ 36.6M+
 6101 perl         0.0%  0:00.06   1    13    18  1.54M   220K  1.82M  26.7M
 6100 sh           0.0%  0:00.00   1    13    16   112K   568K   632K  27.2M
 6094 hmmpfam      2.0%  0:02.97   2    31    50  2.02M+  420K  2.36M+ 41.0M+
 6088 hmmpfam      2.0%  0:02.88   2    31    46  1.95M+  420K  2.27M+ 37.6M+
 6083 hmmpfam      1.9%  0:02.88   2    31    46  1.77M+  420K  2.09M- 37.6M+
 6078 hmmpfam      2.1%  0:02.93   2    31    48  2.55M+  420K  2.82M+ 38.6M+
 6077 perl         0.0%  0:00.06   1    13    18  1.55M   220K  1.82M  26.7M
 6076 sh           0.0%  0:00.00   1    13    16   112K   568K   632K  27.2M
 6071 perl         0.0%  0:00.06   1    13    18  1.55M   220K  1.82M  26.7M
 6070 sh           0.0%  0:00.00   1    13    16   112K   568K   632K  27.2M
 6068 perl         0.0%  0:00.06   1    13    18  1.55M   220K  1.82M  26.7M
 6065 sh           0.0%  0:00.00   1    13    16   132K   568K   636K  27.2M
 6059 perl         0.0%  0:00.06   1    13    18  1.55M   220K  1.82M  26.7M
 6058 sh           0.0%  0:00.00   1    13    16   112K   568K   632K  27.2M
 6053 hmmpfam      2.1%  0:03.15   2    31    50  1.84M+  420K  2.16M+ 40.6M+
 6040 perl         0.0%  0:00.07   1    13    18  1.55M   220K  1.82M  26.7M
 6039 sh           0.0%  0:00.00   1    13    16   112K   568K   632K  27.2M
 5488 MCXCacher    0.0%  0:00.00   1    13    17   116K   344K   500K  26.8M
 5006 pfscan       2.4%  0:08.57   1    13    17   560K+  388K   920K+  139M
 5000 pfscan       2.0%  0:07.90   1    13    17   504K   388K   864K   139M
 4522 pfscan       2.0%  0:07.64   1    13    17   504K   388K   864K   139M
 4038 pfscan       2.0%  0:07.86   1    13    17   504K   388K   864K   139M
 4013 pfscan       1.9%  0:07.79   1    13    17   504K   388K   864K   139M
 4008 pfscan       2.0%  0:07.74   1    13    17   576K   388K   976K   139M
 3999 pfscan       1.7%  0:07.78   1    13    17   508K   388K   868K   139M
 3998 pfscan       1.7%  0:07.68   1    13    17   504K   388K   864K   139M
 3997 pfscan       1.7%  0:07.81   1    13    17   504K   388K   864K   139M
 3996 pfscan       1.9%  0:07.68   1    13    17   508K   388K   868K   139M
 2081 perl         0.0%  0:00.71   1    13    27  3.00M   256K  3.25M  27.8M
 2080 perl         0.0%  0:00.00   1    13    15   176K   220K   460K  26.6M
 2052 hmmpfam      3.9%  0:14.56   3    33    56  1.73M+  420K  1.62M+ 34.1M+
 2048 perl         0.0%  0:00.00   1    13    15   172K   220K   456K  26.6M
 2034 hmmpfam      4.2%  0:15.35   3    33    61  5.45M+  420K  5.29M+ 41.7M+
 2033 perl         0.0%  0:00.00   1    13    15   172K   220K   460K  26.6M
 2018 perl         0.0%  0:00.05   1    13    19  1.19M   228K  1.45M  26.6M
 2017 perl         0.0%  0:00.00   1    13    15   164K   220K   448K  26.6M
 2002 hmmpfam      4.2%  0:14.73   3    33    57  1.54M+  420K  1.59M+ 33.2M+
 2001 perl         0.0%  0:00.00   1    13    16   200K   220K   472K  26.6M
 1921 perl         0.0%  0:00.72   1    13    27  3.01M   256K  3.27M  27.8M
 1915 perl         0.0%  0:00.00   1    13    15   176K   220K   460K  26.6M
 1885 perl         0.0%  0:00.05   1    13    17  1.20M   220K  1.47M  26.7M
 1883 FingerPRIN   1.9%  0:08.50   1    13    67  51.4M   460K  51.8M  85.9M
 1881 sh           0.0%  0:00.00   1    13    16   108K   568K   604K  27.2M

    
    THEN DID ps aux | grep iprscan
    
    ps aux | grep iprscan > psaux.iprscan
    lines psaux.iprscan 
    145 lines

    em psaux.iprscan
    
    root      1666   9.8  0.5    42384   4972  p0  R+   12:01PM   0:41.38 /common/apps/iprscan/bin/binaries/hmmpfam --acc -E 1000 -A 100 -Z 7973 --cut_ga /common/apps/iprscan/data/Pfam.bin /common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/chunk_8/iprscan-20
root       598   9.2  0.1    33352   1568  p0  R+   12:00PM   0:46.71 /common/apps/iprscan/bin/binaries/hmmpfam --cpu 2 --acc -E 59.5 -A 100 /common/apps/iprscan/data/Gene3D.hmm.bin /common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/chunk_1/iprscan-20061
root      1628   8.8  0.1    34332    700  p0  R+   12:01PM   0:41.55 /common/apps/iprscan/bin/binaries/hmmpfam --cpu 2 --acc -E 59.5 -A 100 /common/apps/iprscan/data/Gene3D.hmm.bin /common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/chunk_8/iprscan-20061
root      1837   8.2  0.1    43152   1308  p0  R+   12:02PM   0:41.32 /common/apps/iprscan/bin/binaries/hmmpfam --acc -E 1000 -A 100 -Z 7973 --cut_ga /common/apps/iprscan/data/Pfam.bin /common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/chunk_9/iprscan-20
root       685   7.0  0.1    33032    860  p0  R+   12:00PM   0:45.96 /common/apps/iprscan/bin/binaries/hmmpfam --cpu 2 --acc -E 59.5 -A 100 /common/apps/iprscan/data/Gene3D.hmm.bin /common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/chunk_2/iprscan-20061
root      1453   6.9  0.1    32620    936  p0  R+   12:01PM   0:43.94 /common/apps/iprscan/bin/binaries/hmmpfam --cpu 2 --acc -E 59.5 -A 100 /common/apps/iprscan/data/Gene3D.hmm.bin /common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/chunk_7/iprscan-20061
root      1077   6.3  0.1    33040    908  p0  R+   12:00PM   0:44.48 /common/apps/iprscan/bin/binaries/hmmpfam --cpu 2 --acc -E 59.5 -A 100 /common/apps/iprscan/data/Gene3D.hmm.bin /common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/chunk_5/iprscan-20061
root       571   6.2  0.8    33824   8252  p0  R+   12:00PM   0:42.68 /usr/bin/perl /common/apps/iprscan/bin/iprscan_wrapper.pl
root       952   6.1  0.2    42648   1740  p0  R+   12:00PM   0:45.03 /common/apps/iprscan/bin/binaries/hmmpfam --acc -E 1000 -A 100 -Z 7973 --cut_ga /common/apps/iprscan/data/Pfam.bin /common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/chunk_4/iprscan-20
...
    SEE FILE 'ERROR-fork-Resource-temporarily-unavailable-psaux.iprscan'

    ... AND IPRSCAN RAN OKAY:
    
    gems:/common/apps/iprscan/bin local$ sudo iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml
Password:
SUBMITTED iprscan-20061112-12001020

=== ********************************************************** ===

Some jobs failed! Please see the report file
/common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/iprscan-20061112-12001020.report
You can try to relaunch failed jobs using ResubmitJobs.pl
script located in your iprscan/bin directory as follow :
./ResubmitJobs.pl -r /common/apps/iprscan/tmp/20061112/iprscan-20061112-12001020/iprscan-20061112-12001020.report

Then old results will be concatenated with new one

=== ********************************************************** ===


    RESULTS IN OUTPUT FILE:
    
    
gems:/common/apps/iprscan/bin local$ em ../test.nuc.out.xml

<interpro_matches>

   <protein id="NM_011029_1_ORF17" length="96" crc64="98E30407EEAFF22A" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="26" end="39" score="NA" status="?" evidence="Seg" />
            <location start="75" end="84" score="NA" status="?" evidence="Seg" />
          </match>
        </interpro>
   </protein>
   <protein id="NM_011029_1_ORF4" length="23" crc64="BFCDAEFBBEF73774" >
        <interpro id="noIPR" name="unintegrated" type="unintegrated">
          <match id="seg" name="seg" dbname="SEG">
            <location start="10" end="23" score="NA" status="?" evidence="Seg" />
          </match>






    
    2. COPIED THIS TO ALL NODES:
    E.G.
    
    node001:/etc vanwye$ sudo scp local@129.171.101.102:/etc/sysctl.conf /etc

    






+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Fri Nov 10 16:07:36 EST 2006

INTERPROSCAN WORKING OKAY ON GENOMICS BUT VERY SLOW, POSSIBLY DUE TO LOW DISK SPACE:


~/FUNNYBASE/apps/iprscan/bin young$ iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml
SUBMITTED iprscan-20061110-14532123

=== ********************************************************** ===

Some jobs failed! Please see the report file
/Users/young/FUNNYBASE/apps/iprscan/tmp/20061110/iprscan-20061110-14532123/iprscan-20061110-14532123.report
You can try to relaunch failed jobs using ResubmitJobs.pl
script located in your iprscan/bin directory as follow :
./ResubmitJobs.pl -r /Users/young/FUNNYBASE/apps/iprscan/tmp/20061110/iprscan-20061110-14532123/iprscan-20061110-14532123.report

Then old results will be concatenated with new one

=== ********************************************************** ===


dlc-genomics:~/FUNNYBASE/apps/iprscan/bin young$ em /Users/young/FUNNYBASE/apps/iprscan/tmp/20061110/iprscan-20061110-14532123/iprscan-20061110-14532123.report

[1]+  Stopped                 emacs -nw /Users/young/FUNNYBASE/apps/iprscan/tmp/20061110/iprscan-20061110-14532123/iprscan-20061110-14532123.report
dlc-genomics:~/FUNNYBASE/apps/iprscan/bin young$ 



REPORT FILE:

========================================================================
Chunk: 1
Jobid: iprscan-20061110-14532123-coils-cnk1
Application: coils
Please fix error(s) if necessary or check 'coils.conf' file.
Errors :

      10 sequences      941 aas        0 in coil


========================================================================

========================================================================
Chunk: 1
Jobid: iprscan-20061110-14532123-superfamily-cnk1
Application: superfamily
Please fix error(s) if necessary or check 'superfamily.conf' file.
Errors :

Runs finished, but the result file /Users/young/FUNNYBASE/apps/iprscan/tmp/tmp/dlc-genomics.rsmas.miami.edu116318840527949.res is incomplete!


========================================================================

========================================================================
Chunk: 2
Jobid: iprscan-20061110-14532123-coils-cnk2
Application: coils
Please fix error(s) if necessary or check 'coils.conf' file.
Errors :

       6 sequences      515 aas        0 in coil


========================================================================

========================================================================
Chunk: 2
Jobid: iprscan-20061110-14532123-superfamily-cnk2
Application: superfamily
Please fix error(s) if necessary or check 'superfamily.conf' file.
Errors :

Runs finished, but the result file /Users/young/FUNNYBASE/apps/iprscan/tmp/tmp/dlc-genomics.rsmas.miami.edu116318841128053.res is incomplete!


========================================================================

========================================================================
Chunk: 2
Jobid: iprscan-20061110-14532123-tmhmm-cnk2
Application: tmhmm
Please fix error(s) if necessary or check 'tmhmm.conf' file.
Errors :

cat: /Users/young/FUNNYBASE/apps/iprscan/tmp/20061110/iprscan-20061110-14532123/chunk_2/iprscan-20061110-14532123.nocrc: Bad file descriptor


========================================================================

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

INTERPROSCAN
************

INSTALL
+++++++
SET THE $ENV{IPRSCAN_HOME} VARIABLE IN MOST FILES TO:

$ENV{IPRSCAN_HOME} ='/Users/young/FUNNYBASE/apps/iprscan'

RUN IPRSCAN
+++++++++++ 
NUCLEOTIDE SEQUENCE
iprscan -cli -seqtype n -i ../test.nuc.seq  -iprlookup -goterms -o ../test.nuc.out.xml

PEPTIDE SEQUENCE (DEFAULT)
iprscan -cli -i ../test.seq -iprlookup -goterms -o iprscan.OUTPUT.xml

OPTIONS
+++++++
-seqtype 	n for DNA/RNA, p for PROTEINS
-iprlookup	lookup the corresponding InterPro references to the output
-goterms 	look up the corresponding entries in GO (Gene Ontology)
 
For more options/informations about what iprscan is able to do type:
 ./iprscan -cli -h

NB: -cli option is mandatory for command line usage. It specifies to the script
       that user is using it as command line and not as CGI script.

WHAT IT DOES
++++++++++++
 The program will prepare a temporary directory (something like
 'tmp/20041011/iprscan-20041011-11123456') where:
 iprscan-20041011-11123456 is a the session directory in format:

 iprscan : toolname
 20041011: date of the day (YYYYMMDD)
 
    11123456: time of the day (hhmmssNN) NN=random number

   When the scanning is finished the results will be displayed on the STDOUT
   unless you used the -o option to precise an output file where to put results.
   Anyway the raw file is writen in the session directory. What is displayed on
   on the screen is just a conversion of this raw file.
   To check that everything works correctly you can compare your results with
   the 'merged.xml' file included in the root of the distribution.

   % diff merged.raw tmp/20041011/iprscan-20041011-11123456/merged.raw




