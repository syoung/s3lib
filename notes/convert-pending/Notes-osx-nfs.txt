++++++++</entry>



<entry [Thu Dec 13 13:27:18 PST 2007] FIX NFS SHARE AFTER MOVING gems>




node001 PORTMAPPER IS RUNNING AND /common LINKS ARE PRESENT:


node001:/ vanwye$ sudo SystemStarter start AutoFS
node001:/ vanwye$ niutil -list . /exports
106      /Library/NetBoot/NetBootSP0
node001:/ vanwye$ showmount -e gems.rsmas.miami.edu
Exports list on gems.rsmas.miami.edu:
/Library/Perl                      192.168.2.0 
/Volumes/gemshd2                   node001.cluster.private 
/Volumes/gemshd3                   129.171.101.233 node001.cluster.private 
node001:/ vanwye$ ls /
Applications    Groups.old      System          automount       data            mach_kernel     scratch         var
Desktop DB      Library         Users           bin             dev             nfs             test_mount
Desktop DF      Network         Users.old       collection      etc             opt             tm
Developer       RemotePerl      Volumes         common          mach            private         tmp
Groups          Shared Items    auto.common     cores           mach.sym        sbin            usr
node001:/ vanwye$ ll /
-bash: ll: command not found
node001:/ vanwye$ ls -al /
total 12098
drwxrwxr-t   44 root    admin     1598 Dec 12 08:02 .
drwxrwxr-t   44 root    admin     1598 Dec 12 08:02 ..
-rw-r--r--    1 root    admin     6148 Oct 24  2005 .DS_Store
drw-------    8 root    admin      272 Oct 24  2005 .Spotlight-V100
d-wx-wx-wt    2 root    admin       68 Mar 13  2007 .Trashes
-rw-------    1 root    wheel  1048576 Jul  7  2006 .hotfiles.btree
-rw-------    1 root    admin     1024 Jul  7  2006 .rnd
dr-xr-xr-x    2 root    wheel       96 Dec 12 08:02 .vol
drwxrwxr-x   30 root    admin     1020 Dec 29  2006 Applications
-rw-r--r--    1 root    admin    19968 Dec 30  2006 Desktop DB
-rw-r--r--    1 root    admin   115458 Oct 24  2005 Desktop DF
drwxrwxr-x   14 root    admin      476 Oct 24  2005 Developer
lrwxr-xr-x    1 root    admin       19 Jan 22  2007 Groups -> /nfs/gemshd2/Groups
drwxrwxr-x    4 vanwye  admin      136 Jan 22  2007 Groups.old
drwxrwxr-t   48 root    admin     1632 Jul  9  2006 Library
drwxr-xr-x    5 root    wheel      170 Aug 22  2005 Network
drwxr-xr-x    2 root    admin       68 Jan  9  2007 RemotePerl
drwxrwxr-x    4 vanwye  admin      136 Jul  9  2006 Shared Items
drwxr-xr-x    4 root    wheel      136 Dec 29  2006 System
lrwxr-xr-x    1 root    admin       18 Jan 22  2007 Users -> /nfs/gemshd2/Users
drwxrwxr-t    6 root    admin      204 Jan 22  2007 Users.old
drwxrwxrwt    4 root    admin      136 Dec 12 08:02 Volumes
-rw-r--r--    1 vanwye  admin      307 Dec 18  2006 auto.common
drwxr-xr-x    4 root    admin      136 Jul  7  2006 automount
drwxr-xr-x   40 root    wheel     1360 Dec 29  2006 bin
drwxr-xr-x    3 www     admin      102 Feb  5  2007 collection
lrwxr-xr-x    1 root    admin       19 Jan 22  2007 common -> /nfs/gemshd3/common
drwxrwxr-t    4 root    admin      136 Oct 18 14:38 cores
drwxr-xr-x    3 root    admin      102 Oct 19 21:57 data
dr-xr-xr-x    2 root    wheel      512 Dec 12 08:02 dev
lrwxr-xr-x    1 root    admin       11 Oct 24  2005 etc -> private/etc
lrwxr-xr-x    1 root    admin        9 Dec 12 08:02 mach -> /mach.sym
-r--r--r--    1 root    admin   603684 Dec 12 08:02 mach.sym
-rw-r--r--    1 root    wheel  4343332 Sep  8  2006 mach_kernel
drwxr-xr-x    5 root    admin      170 Jan 12  2007 nfs
drwxr-xr-x    3 root    wheel      102 Oct 19  2005 opt
drwxr-xr-x    6 root    wheel      204 Dec 12 08:02 private
drwxr-xr-x   63 root    wheel     2142 Dec 29  2006 sbin
drwxrwxrwx    2 root    wheel       68 Jul 11  2006 scratch
drwxr-xr-x    2 root    admin       68 Jul 24  2006 test_mount
-rw-r--r--    1 root    admin     5683 Jul 18  2006 tm
lrwxr-xr-x    1 root    admin       11 Oct 24  2005 tmp -> private/tmp
drwxr-xr-x   11 root    wheel      374 Oct 24  2005 usr
lrwxr-xr-x    1 root    admin       11 Oct 24  2005 var -> private/var
node001:/ vanwye$ ls -al Groups  
lrwxr-xr-x   1 root  admin  19 Jan 22  2007 Groups -> /nfs/gemshd2/Groups
node001:/ vanwye$ ls -al nfs
total 0
drwxr-xr-x    5 root  admin   170 Jan 12  2007 .
drwxrwxr-t   44 root  admin  1598 Dec 12 08:02 ..
drwxr-xr-x    2 root  admin    68 Jan 12  2007 gemshd2
drwxr-xr-x    2 root  admin    68 Jan 12  2007 gemshd3
drwxr-xr-x    2 root  admin    68 Jan  9  2007 gemshd4
node001:/ vanwye$ ls -al nfs/gemshd2
total 0
drwxr-xr-x   2 root  admin   68 Jan 12  2007 .
drwxr-xr-x   5 root  admin  170 Jan 12  2007 ..
node001:/ vanwye$ rpcinfo -p
   program vers proto   port
    100000    2   tcp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp   1020  status
    100024    1   tcp   1014  status
    100021    0   udp   1008  nlockmgr
    100021    1   udp   1008  nlockmgr
    100021    3   udp   1008  nlockmgr
    100021    4   udp   1008  nlockmgr
    100021    0   tcp   1013  nlockmgr
    100021    1   tcp   1013  nlockmgr
    100021    3   tcp   1013  nlockmgr
    100021    4   tcp   1013  nlockmgr
    100005    1   udp    989  mountd
    100005    3   udp    989  mountd
    100005    1   tcp   1011  mountd
    100005    3   tcp   1011  mountd
    100003    2   udp   2049  nfs
    100003    3   udp   2049  nfs
    100003    2   tcp   2049  nfs
    100003    3   tcp   2049  nfs
node001:/ vanwye$ nidump -r /exports .
{
  "name" = ( "exports" );
  CHILDREN = (
    {
      "name" = ( "/Library/NetBoot/NetBootSP0" );
      "opts" = ( "ro" );
    }
  )
}


BUT WHEN I TRY TO RUN /Library/StartupItems/ClusterServices, IT DOESN'T MOUNT THE NFS SHARES /Volumes/gemshd2 AND /Volumes/gemshd3 :


node002:/ vanwye$ cd /Library/StartupItems/ClusterServices
node002:/Library/StartupItems/ClusterServices vanwye$ ls
ClusterServices         StartupParameters.plist
node002:/Library/StartupItems/ClusterServices vanwye$ sudo ./ClusterServices
Password:
ClusterServices 1.0
HOSTNAME node002.cluster.private
Using Server portal2net
Mounting Shared Volume
ln: /common: File exists
ln: /Users: File exists
ln: /Groups: File exists
mount_nfs: can't access /Volumes/gemshd3: Permission denied
mount_nfs: can't access /Volumes/gemshd2: Permission denied
node002:/Library/StartupItems/ClusterServices vanwye$ 


node002:/Library/StartupItems/ClusterServices vanwye$ mount
/dev/disk0s3 on / (NFS exported, local, journaled)
devfs on /dev (local)
fdesc on /dev (union)
<volfs> on /.vol
portal2net:/Library/Perl on /RemotePerl
node002:/Library/StartupItems/ClusterServices vanwye$ sudo mount_nfs -T -s -i portal2net:/Volumes/gemshd3/common /common
mount_nfs: can't access /Volumes/gemshd3/common: Permission denied





++++++++</entry>



<entry [Wed Feb 21 09:41:19 EST 2007] RESTORE NODE STATE FROM 'E' STATE>



qstat -explain E
qmod -c all.q@*


++++++++</entry>



<entry [Thu Jan 25 12:38:05 EST 2007] SETUP NFS MOUNT OF /common ON genomics>



COPIED THE /Library/StartupItems/ClusterServices DIRECTORY FROM node001 TO genomics

dlc-genomics:/Library/StartupItems/ClusterServices young$ ll
total 32
drwxr-xr-x    6 root  wheel  204B Jan 25 12:31 .
drwxr-xr-x   13 root  wheel  442B Jan 25 12:18 ..
-rwxr-xr-x    1 root  wheel    1K Jan 25 12:31 ClusterServices
-rwxr-xr-x    1 root  wheel    1K Jan 25 12:31 ClusterServices.bkp
-rwxr-xr-x    1 root  wheel    1K Jan 25 12:23 ClusterServices~
-r--r--r--    1 root  wheel  148B Jan 25 12:17 StartupParameters.plist

EDITED ClusterServices TO INCLUDE ONLY gemshd3 EXPORT AND SERVER AS gems.rsmas.miami.edu:

em ClusterServices
>>>
#!/bin/ksh
#
# ClusterServices Startup script
# Performs static NFS mounts.  Replaces AutoFS structure in default iNquiry

VERSION=1.0

. /etc/rc.common

ConsoleMessage "ClusterServices ${VERSION}"
PARTITION=gemshd4
set -A NFS_SERVERS "portal2net"
HOSTNAME=`hostname`
ConsoleMessage "HOSTNAME $HOSTNAME"
HOSTNAME=${HOSTNAME%.cluster.private}
HOSTNAME=${HOSTNAME%.local}
NODE_NUMBER=${HOSTNAME#node}

# SERVER=${NFS_SERVERS[$(( $NODE_NUMBER % ${#NFS_SERVERS[*]} ))]}

SERVER=gems.rsmas.miami.edu
ConsoleMessage "Using Server $SERVER"
# SERVER=nfs002
# ConsoleMessage "Mounting $PARTITION from $SERVER"

#
# Mounts the shared NFS volumes at boot time to avoid the automounter.
#
ConsoleMessage "Mounting Shared Volume"

#### Set up directories for mount of /common
# if [ ! -d /nfs/gemshd2 ]; then mkdir -p /nfs/gemshd2; fi
if [ ! -d /nfs/gemshd3 ]; then mkdir -p /nfs/gemshd3; fi
if [ ! -e /common ]; then ln -s /nfs/gemshd3/common /common; fi

#### Perform the mount, checking to see if it's already mounted.
if [ ! -d /nfs/gemshd3/common ];
  then mount_nfs -i -s $SERVER:/Volumes/gemshd3 /nfs/gemshd3
fi

#################################################
# STUFF BELOW THIS LINE IS NEEDED FOR NODES ONLY
#################################################

# Set up directories for mount of /Users
# if [ ! -e /Users ]; then ln -s /nfs/gemshd2/Users /Users; fi
# if [ ! -e /Groups ]; then ln -s /nfs/gemshd2/Groups /Groups; fi
# Perform the mount, checking to see if it's already mounted.
# if [ ! -d /nfs/gemshd2/Users ];
#   then mount_nfs -i -s $SERVER:/Volumes/gemshd2 /nfs/gemshd2
# fi


# if [ ! -e /RemotePerl ]; then mkdir /RemotePerl; fi
# if [ ! -d /RemotePerl/5.8.6 ];
#   then mount_nfs -i -s $SERVER:/Library/Perl /RemotePerl
# fi

#
# End of ClusterServices Startup script
#
<<<

RAN ClusterServices AND GOT THIS ERROR:

cd /Library/StartupItems/ClusterServices
sudo ./ClusterServices
>>>
ClusterServices 1.0
HOSTNAME dlc-genomics.rsmas.miami.edu
Using Server gems.rsmas.miami.edu
Mounting Shared Volume
NFS Portmap: RPC: Port mapper failure - RPC: Timed out
NFS Portmap: RPC: Port mapper failure - RPC: Timed out
...<<<

DID rpcinfo -p AND FOUND THAT portmap, etc. WEREN'T WORKING ON genomics:

dlc-genomics:/Library/StartupItems/ClusterServices young$ rpcinfo -p
rpcinfo: can't contact portmapper: RPC: Remote system error - Connection refused

SO STARTED portmapper:
portmap

dlc-genomics:/Library/StartupItems/ClusterServices young$ rpcinfo -p   program vers proto   port
    100000    2   tcp    111  portmapper
    100000    2   udp    111  portmapper

AND  ADDED THESE LINES TO ipfw ON GEMS:
sudo ipfw add 40000 allow ip from 129.171.101.233 to 129.171.101.102
sudo ipfw add 40001 allow ip from 129.171.101.102 to 129.171.101.233


AND RAN ClusterServices AGAIN, WHICH WORKED:
dlc-genomics:/Library/StartupItems/ClusterServices young$ sudo ./ClusterServices
Password:
ClusterServices 1.0
HOSTNAME dlc-genomics.rsmas.miami.edu
Using Server gems.rsmas.miami.edu
Mounting Shared Volume


REBOOTED AND /common WAS NOT MOUNTED AUTOMATICALLY BUT MOUNTED AFTER DOING sudo ./ClusterServices



++++++++</entry>



<entry [Tue Jan 23 14:00:40 EST 2007] COMPLETED RESTORE OF node012 AND node016 FROM node002>



gems:~ local$ sudo dsh -w node016 SystemStarter start SGE
Password:
executing 'SystemStarter start SGE'
node016:        @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
node016:        @    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
node016:        @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
node016:        IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
node016:        Someone could be eavesdropping on you right now (man-in-the-middle attack)!
node016:        It is also possible that the RSA host key has just been changed.
node016:        The fingerprint for the RSA key sent by the remote host is
node016:        71:4a:c4:fd:f3:11:8f:91:ad:e7:30:aa:17:01:d3:45.
node016:        Please contact your system administrator.
node016:        Add correct host key in /private/var/root/.ssh/known_hosts to get rid of this message.
node016:        Offending key in /private/var/root/.ssh/known_hosts:30
node016:        Password authentication is disabled to avoid man-in-the-middle attacks.
node016:        Keyboard-interactive authentication is disabled to avoid man-in-the-middle attacks.
node016:        Checking disks
node016:        ipfilter:state = "STOPPED"
node016:        ipfilter:status = 0

ALSO GOT THIS ERROR WHEN RUNNINIG qstat -f ON node016:

node002:~ vanwye$ qstat -f
error: commlib error: access denied (client IP resolved to host name "node016.cluster.private". This is not identical to clients host name "node002.cluster.private")
unable to contact qmaster using port 701 on host "gems.rsmas.miami.edu"
node002:~ vanwye$ sudo em /etc/hostconfig

SO EDITED /etc/hostconfig AND CHANGED FROM HOSTNAME=-AUTOMATIC- TO HOSTNAME=node016 AND REBOOTED, AFTER WHICH node016 APPEARED AS STATUS 'd' WITH qstat -f ON gems:


gems:~ local$ qstat -f 
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       0.06     darwin        d
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.03     darwin        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   0/2       0.06     darwin        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.15     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   0/2       0.02     darwin        d
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.04     darwin        d



THEN ENABLED node012 AND node016:

sudo qmod -e all.q@node012
sudo qmod -e all.q@node016


++++++++</entry>



<entry [Tue Jan 23 00:11:53 EST 2007] RESTORE OF NODES 12 and 16 FROM IMAGE OF NODE 2 (gem3)>




1. SCAN .dmg IMAGE AND CREATE FILECHECKSUM
 sudo asr imagescan --source /Volumes/gemshd2/disc.images/node002.070122.dmg --filechecksum

2. RESTORE FROM IMAGE
sudo asr restore -source /Volumes/gemshd2/disc.images/node002.070122.dmg -target /Volumes/gems3

THIS WORKED ON node012 BUT node016 GAVE AN ERROR:

Filesystem              512-blocks     Used     Avail Capacity  Mounted on
/dev/disk2s3             160574256 64068968  95993288    40%    /
devfs                          202      202         0   100%    /dev
fdesc                            2        2         0   100%    /dev
<volfs>                       1024     1024         0   100%    /.vol
/dev/disk0s3             160563120 65997104  94566016    41%    /Volumes/gemshd2
/dev/disk1s3             160565168 68395216  92169952    43%    /Volumes/gemshd3
automount -nsl [325]             0        0         0   100%    /Network
automount -fstab [353]           0        0         0   100%    /automount/Servers
automount -static [353]          0        0         0   100%    /automount/static
/dev/disk3s3             160565168 19890000 140675168    12%    /Volumes/gems16


gems:~ local$ sudo asr restore -source /Volumes/gemshd2/disc.images/node002.070122.dmg -target /Volumes/gems16
        Validating target...done
        Validating source...done
        Retrieving scan information...done
        Validating sizes...done
        Restoring...
        Copying "/Volumes/gems3" (/dev/disk4) to "/Volumes/gems16" (/dev/disk3s3)...
asr: could not copy /Volumes/gems16/RemotePerl; Not a directory
asr: Bom copy exited with error 2
asr: couldn't restore - No such file or directory
gems:~ local$ sudo asr restore -source /Volumes/gemshd2/disc.images/node002.070122.dmg -target /Volumes/gems16
Password:
        Validating target...done
        Validating source...asr: Couldn't validate source - Device not configured
gems:~ local$ df -a
Filesystem              512-blocks     Used     Avail Capacity  Mounted on
/dev/disk2s3             160574256 64078160  95984096    40%    /
devfs                          202      202         0   100%    /dev
fdesc                            2        2         0   100%    /dev
<volfs>                       1024     1024         0   100%    /.vol
/dev/disk0s3             160563120 65997136  94565984    41%    /Volumes/gemshd2
/dev/disk1s3             160565168 68395216  92169952    43%    /Volumes/gemshd3
automount -nsl [325]             0        0         0   100%    /Network
automount -fstab [353]           0        0         0   100%    /automount/Servers
automount -static [353]          0        0         0   100%    /automount/static
/dev/disk3s3             160565168 19959368 140605800    12%    /Volumes/gems16
gems:~ local$ 




++++++++</entry>



<entry [Mon Jan 22 18:06:20 EST 2007] TROUBLESHOOTING IPRSCAN>



./iprscan3.pl -p hmmpanther -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.2 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1

>>>
Queryfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.2.hmmpanther.0
Outfile: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.hmmpanther.0
TSV file: /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.hmmpanther.0.tsv
No output file present... doing iprscan
/common/iprscan/bin/iprscan -cli -seqtype n -i /private/tmp/-Users-local-FUNNYBASE-pipeline-orthologues-fasta-orthologues.fasta.2.hmmpanther.0 -o /private/tmp/-Users-local-FUNNYBASE-pipeline-orthologues-fasta-orthologues.out.1.hmmpanther.0 -iprlookup -goterms -appl hmmpanther
SUBMITTED iprscan-20070122-21025196

/common/iprscan/bin/iprscan: /common/iprscan/tmp/20070122/iprscan-20070122-21025196/iprscan-20070122-21025196.xml unavailable : No such file or directory

supervise: doRawResults:

error running system command /common/iprscan/bin/converter.pl -format xml -input /common/iprscan/tmp/20070122/iprscan-20070122-21025196/chunk_6/merged.raw -jobid iprscan-20070122-21025196
...
Opening outfile '/Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.hmmpanther.0'
Can't open file '/private/tmp/-Users-local-FUNNYBASE-pipeline-orthologues-fasta-orthologues.out.1.hmmpanther.0'
<<<

SO TRIED RUNNING THE SYSTEM COMMAND AND SAW THAT XML/Quote.pm CAN'T BE FOUND:

/common/iprscan/bin/converter.pl -format xml -input /common/iprscan/tmp/20070122/iprscan-20070122-21272347/chunk_4/merged.raw -jobid iprscan-20070122-21272347
>>>
Can't locate XML/Quote.pm in @INC (@INC contains: /common/iprscan/lib /RemotePerl /System/Library/Perl/5.8.6/darwin-thread-multi-2level /System/Library/Perl/5.8.6 /Library/Perl/5.8.6/darwin-thread-multi-2level /Library/Perl/5.8.6 /Library/Perl /Network/Library/Perl/5.8.6/darwin-thread-multi-2level /Network/Library/Perl/5.8.6 /Network/Library/Perl /System/Library/Perl/Extras/5.8.6/darwin-thread-multi-2level /System/Library/Perl/Extras/5.8.6 /Library/Perl/5.8.1 .) at /common/iprscan/bin/converter.pl line 40.
BEGIN failed--compilation aborted at /common/iprscan/bin/converter.pl line 40.
<<<

SO INSTALLED XML::Quote:




+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
AUTOFS VS STATIC MOUNT

https://lists.sdsc.edu/pipermail/npaci-rocks-discussion/2003-November.txt

From yduan at udel.edu  Sun Nov 16 19:55:01 2003
From: yduan at udel.edu (Yong Duan)
Date: Sun, 16 Nov 2003 22:55:01 -0500
Subject: [Rocks-Discuss]RE: Rocks 3.0 Problem
In-Reply-To: <1069036403.3206.12.camel@protein.scalableinformatics.com>
Message-ID: <000201c3acbe$9c619370$6501a8c0@duandell>

What you observed was probably due to autofs. It takes a notable delay
to "autofs" even on a small cluster (about 10 nodes) that are not
terribly busy (90% idle). For a cluster of 100 nodes, the delay could be
a bit longer. For a busy cluster, when multiple nodes request autofs
mount simultaneously, you probably should expect a few time outs.

I would suggest you change to static mount for the long run.
Periodically restarting yp/autofs will likely to add a bit of stress to
the system which is precisely what you try to avoid. Autofs could be a
reasonable solution for a cluster of workstations if you try to spread
files and localize each user's file to his/her home machine. This is not
the case for a compute cluster where you probably have a centralized
storage. It would be really cool if ROCKS can provide an option to allow
static NFS, rather than forcing everybody to use autofs.


++++++++</entry>



<entry [Mon Jan 22 12:30:43 EST 2007] RESTORED gemshd1 FROM IMAGE>



AND GOT THIS ERROR WHEN TRYING TO RUN funnybasearrayblast.pl:

qstat -j 6637
>>>...
error reason    1:          can't create directory active_jobs/6637.1: Permission denied
error reason    2:          can't create directory active_jobs/6637.2: Permission denied
error reason    3:          can't create directory active_jobs/6637.3: Permission denied
error reason    4:          can't create directory active_jobs/6637.4: Permission denied
<<<

CHECKED THE PERMISSIONS BUT SEEMS ALL IS OKAY SO IT SEEMS TO BE AN NFS PROBLEM, SINCE I HAVEN'T
ADJUSTED THE NFS EXPORTS (ON gems) AND MOUNTS (ON nodes) YET.

VIEW NETINFO exports ON gems AFTER RESTORE:

nidump -r /exports .
{
  "name" = ( "exports" );
  CHILDREN = (
    {
      "name" = ( "/Users" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Groups" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/common" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Library/Perl" );
      "clients" = ( );
      "opts" = ( "ro", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Library/NetBoot/NetBootSP0" );
      "opts" = ( "ro" );
    },
    {
      "name" = ( "/Volumes/gemshd3/Library/NetBoot/NetBootSP1" );
      "opts" = ( "ro" );
    }
  )
}


USED NETINFO MANAGER TO CHANGE TO THIS:

{
  "name" = ( "exports" );
  
  CHILDREN = (
    {
      "name" = ( "/Volumes/gemshd2" );
      "clients" = ( "node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.private  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private  node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root", "alldirs" );
    },
    {
      "name" = ( "/Library/Perl" );
      "clients" = ( );
      "opts" = ( "ro", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Volumes/gemshd3" );
      "clients" = ( "129.171.101.233 node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.private  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private  node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root", "alldirs" );
    }
  )
}

NB: COMPARED TO ORIGINAL (BACKED UP) VERSION OF NETINFO exports (/Volumes/gemshd2/disc.images/nidump.export.txt):

{
  "name" = ( "exports" );
  CHILDREN = (
    {
      "name" = ( "/Volumes/gemshd2" );
      "clients" = ( "129.171.101.233 node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.pri\
vate  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private \
 node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root", "alldirs" );
    },
    {
      "name" = ( "/Volumes/gemshd3" );
      "clients" = ( "129.171.101.233 node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.pri\
vate  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private \
 node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root", "alldirs" );
    },
    {
      "name" = ( "/Library/Perl" );
      "clients" = ( );
  all      "opts" = ( "ro", "network=192.168.2.0", "mask=255.255.255.0" );
    }
  )
}


NB: ACCIDENTALLY CREATED EXPORTS FOR THE SUBDIRECTORIES Users AND common RATHER THAN THE PATHS /Volumes/gemshd2 AND /Volumes/gemshd3 

WHICH MEANT THAT THE NODES COULDN'T FIND THE MOUNT POINTS, SO EDITED AUTOFS SETTINGS ON node001 TO SEE IF THAT WOULD WORK,

FROM:

node001:/etc/bipod vanwye$ sudo em /etc/bipod/auto.common
>>>
common -rw,rsize=8192,wsize=8192 portal2net:/Volumes/gemshd4/common
Users  -rw,rsize=8192,wsize=8192 portal2net:/Volumes/gemshd4/Users
Groups  -rw,rsize=8192,wsize=8192 portal2net:/Volumes/gemshd4/Groups
RemotePerl  -ro,rsize=8192,wsize=8192 portal2net:/Library/Per
<<<

TO THIS:
>>>
common -rw,rsize=8192,wsize=8192 portal2net:/Volumes/gemshd3/common
Users  -rw,rsize=8192,wsize=8192 portal2net:/Volumes/gemshd2/Users
RemotePerl  -ro,rsize=8192,wsize=8192 portal2net:/Library/Perl
<<<

THEN REBOOTED NODES:

gems:/Volumes/gemshd3 root# dsh -a reboot
executing 'reboot'
Password:Password:node001.cluster.private:      Connection to 192.168.2.1 closed by remote host.
node002.cluster.private:        Connection to 192.168.2.2 closed by remote host.
node004.cluster.private:        Connection to 192.168.2.4 closed by remote host.
node005.cluster.private:        Connection to 192.168.2.5 closed by remote host.
node006.cluster.private:        Connection to 192.168.2.6 closed by remote host.
node007.cluster.private:        @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
node007.cluster.private:        @    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
node007.cluster.private:        @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
node007.cluster.private:        IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
node007.cluster.private:        Someone could be eavesdropping on you right now (man-in-the-middle attack)!
node007.cluster.private:        It is also possible that the RSA host key has just been changed.
node007.cluster.private:        The fingerprint for the RSA key sent by the remote host is
node007.cluster.private:        71:4a:c4:fd:f3:11:8f:91:ad:e7:30:aa:17:01:d3:45.
node007.cluster.private:        Please contact your system administrator.
node007.cluster.private:        Add correct host key in /private/var/root/.ssh/known_hosts to get rid of this message.
node007.cluster.private:        Offending key in /private/var/root/.ssh/known_hosts:24
node007.cluster.private:        Password authentication is disabled to avoid man-in-the-middle attacks.
node007.cluster.private:        Keyboard-interactive authentication is disabled to avoid man-in-the-middle attacks.
node007.cluster.private:        Connection to 192.168.2.7 closed by remote host.
node008.cluster.private:        Connection to 192.168.2.8 closed by remote host.
node009.cluster.private:        Connection to 192.168.2.9 closed by remote host.
node010.cluster.private:        Connection to 192.168.2.10 closed by remote host.
node011.cluster.private:        Connection to 192.168.2.11 closed by remote host.

BUT THIS DIDN'T WORK BECAUSE AUTOMATIC MOUNTING WITH AutoFS HAD BEEN DISABLED ON THE NODES:

node001:/Library/StartupItems/AutoFS vanwye$ ls
AutoFS                           StartupParameters.plist.disabled

MOUNTING MANUALLY WORKED:
sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/Volumes/gemshd3/common /common
sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/Volumes/gemshd2/Users /Users

AND common AND Users WAS SUCCESSFULLY MOUNTED BUT THE MOUNT DISAPPEARED ON REBOOT (BECAUSE THE EXPORT WAS SET TO common AND Users INSTEAD OF THEIR PARENTS /Volumes/gemshd3 AND /Volumes/gemshd2).

THE STATIC MOUNT USES THIS SGE MOUNT SCRIPT IN '/Library/StartupItems/ClusterServices' (ON node001):
em ClusterServices
>>>
#!/bin/ksh
#
# ClusterServices Startup script
# Performs static NFS mounts.  Replaces AutoFS structure in default iNquiry

VERSION=1.0

. /etc/rc.common

ConsoleMessage "ClusterServices ${VERSION}"
PARTITION=gemshd4
set -A NFS_SERVERS "portal2net"
HOSTNAME=`hostname`
ConsoleMessage "HOSTNAME $HOSTNAME"
HOSTNAME=${HOSTNAME%.cluster.private}
HOSTNAME=${HOSTNAME%.local}
NODE_NUMBER=${HOSTNAME#node}
SERVER=${NFS_SERVERS[$(( $NODE_NUMBER % ${#NFS_SERVERS[*]} ))]}
ConsoleMessage "Using Server $SERVER"
# SERVER=nfs002
# ConsoleMessage "Mounting $PARTITION from $SERVER"
#
# Mounts the shared NFS volumes at boot time to avoid the automounter.
#
ConsoleMessage "Mounting Shared Volume"

# Set up directories.
if [ ! -d /nfs/gemshd2 ]; then mkdir -p /nfs/gemshd2; fi
if [ ! -d /nfs/gemshd3 ]; then mkdir -p /nfs/gemshd3; fi
if [ ! -e /common ]; then ln -s /nfs/gemshd3/common /common; fi
if [ ! -e /Users ]; then ln -s /nfs/gemshd2/Users /Users; fi
if [ ! -e /Groups ]; then ln -s /nfs/gemshd2/Groups /Groups; fi

# Perform the mount, checking to see if it's already mounted.
if [ ! -d /nfs/gemshd3/common ];
  then mount_nfs -i -s $SERVER:/Volumes/gemshd3 /nfs/gemshd3
fi

# Perform the mount, checking to see if it's already mounted.
if [ ! -d /nfs/gemshd2/Users ];
  then mount_nfs -i -s $SERVER:/Volumes/gemshd2 /nfs/gemshd2
fi


if [ ! -e /RemotePerl ]; then mkdir /RemotePerl; fi
if [ ! -d /RemotePerl/5.8.6 ];
  then mount_nfs -i -s $SERVER:/Library/Perl /RemotePerl
fi

#
# End of ClusterServices Startup script
#
<<<

TRIED RUNNING THE ClusterServices STARTUP SCRIPT WITH THE WRONG EXPORT (I.E., common AND Users, NOT THEIR PARENTS):

node001:/Library/StartupItems vanwye$ sudo ./ClusterServices/ClusterServices 

ClusterServices 1.0
HOSTNAME node001.cluster.private
Using Server portal2net
Mounting Shared Volume
mount_nfs: can't access /Volumes/gemshd3: Permission denied
mount_nfs: can't access /Volumes/gemshd2: Permission denied

THIS ERROR USUALLY MEANS THE CORRECT DIRECTORIES ARE NOT BEING EXPORTED. THIS WAS CONFIRMED BY A QUICK CHECK
OF mount ON NODE001 WHICH SHOWED THAT /RemotePerl WAS MOUNTED FROM portal2net, AS WERE THE MANUALLY ADDED STATIC MOUNTS:

mount
>>>
/dev/disk0s3 on / (NFS exported, local, journaled)
devfs on /dev (local)
fdesc on /dev (union)
<volfs> on /.vol
portal2net:/Library/Perl on /RemotePerl
gems.rsmas.miami.edu:/Volumes/gemshd3/common on /common
gems.rsmas.miami.edu:/Volumes/gemshd2/Users on /Users
<<<

BUT THE MANUAL STATIC MOUNTS DISAPPEARED AFTER sudo reboot

mount
>>>
/dev/disk0s3 on / (NFS exported, local, journaled)
devfs on /dev (local)
fdesc on /dev (union)
<volfs> on /.vol
portal2net:/Library/Perl on /RemotePerl
node001:/ vanwye$ sudo reboot
<<<

sudo mount_nfs -T -s -i portal2net:/Volumes/gemshd3/common /common
sudo mount_nfs -T -s -i portal2net:/Volumes/gemshd2/Users /Users


NB: ClusterConfig CREATES THESE DIRECTORIES:

node002:/Library/StartupItems/ClusterServices vanwye$ ls -al /nfs
total 0
drwxr-xr-x    5 root  admin   170 Jan 12 07:39 .
drwxrwxr-t   39 root  admin  1428 Jan 22 07:39 ..
drwxr-xr-x    2 root  admin    68 Jan 12 07:39 gemshd2
drwxr-xr-x    2 root  admin    68 Jan 12 07:39 gemshd3
drwxr-xr-x    2 root  admin    68 Jan  9 08:48 gemshd4



++++++++</entry>



<entry [Thu Jan 18 19:34:02 EST 2007] FREEZE POSTMORTEM:>



tail -f /var/log/system.log
>>>...
Jan 18 17:16:30 gems bootpd[2589]: bootpd: NetBoot service turned off
Jan 18 17:18:31 gems sudo:    local : TTY=ttyp5 ; PWD=/private/etc ; USER=root ; COMMAND=/bin/mkdir lookupd
Jan 18 17:18:47 gems sudo:    local : TTY=ttyp5 ; PWD=/private/etc ; USER=root ; COMMAND=/bin/sh -c echo TimeToLive 300 > /etc/lookupd/hosts
Jan 18 17:18:56 gems sudo:    local : TTY=ttyp5 ; PWD=/private/etc ; USER=root ; COMMAND=/bin/sh -c echo ValidateCache NO >> /etc/lookupd/hosts
Jan 18 17:19:38 gems su: local to root on /dev/ttyp4
<<<

ps -auxmM # M=SHOW THREADS, m=ORDER BY MEMORY USAGE, r=ORDER BY CPU USAGE
>>>
Thu Jan 18 17:23:28 PST 2007
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
USER       PID %CPU %MEM      VSZ    RSS  TT  STAT STARTED      TIME PRI     STIME     UTIME COMMAND
mysql      456   0.0  5.6   480000  58224  ??  S     2:33PM   0:03.34  31   0:00.14   0:00.06 /usr/libexec/mysqld --basedir=/usr --datadir=/var/mysql --user=mysql --pid-file=/var/mysql/gems.rsmas.miami.edu.pid --port=3306 --socket=/var/mysql/mysql.sock --skip-networ
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34  31   0:00.00   0:00.00                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34  31   0:00.01   0:00.00                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34  31   0:00.00   0:00.00                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34  31   0:00.00   0:00.00                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34  31   0:00.17   0:01.38                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34  31   0:00.10   0:01.29                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34  31   0:00.00   0:00.00                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34  31   0:00.00   0:00.00                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34   8   0:00.11   0:00.04                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34   8   0:00.01   0:00.00                 
           456   0.0  5.6   480000  58224      S     2:33PM   0:03.34   8   0:00.00   0:00.00                 
windowse  1627   0.1  1.2   136228  12132  ??  S     3:37PM   0:09.66  51   0:02.71   0:05.09 /System/Library/Frameworks/ApplicationServices.framework/Frameworks/CoreGraphics.framework/Resources/WindowServer -daemon
          1627   0.0  1.2   136228  12132      S     3:37PM   0:09.66  63   0:00.06   0:00.07                 
          1627   0.0  1.2   136228  12132      S     3:37PM   0:09.66  51   0:00.07   0:01.67                 
nobody    1637   0.0  1.1   137108  11424  ??  S     3:37PM   0:09.60  46   0:00.63   0:00.84 /System/Library/CoreServices/RemoteManagement/AppleVNCServer.bundle/Contents/MacOS/AppleVNCServer
          1637   0.0  1.1   137108  11424      S     3:37PM   0:09.60  46   0:00.12   0:00.19                 
          1637   0.0  1.1   137108  11424      S     3:37PM   0:09.60  46   0:00.34   0:00.12                 
          1637   0.9  1.1   137108  11424      S     3:37PM   0:09.60  46   0:02.10   0:05.26                 
          1637   0.0  1.1   137108  11424      S     3:37PM   0:09.60  46   0:00.00   0:00.00                 
          1637   0.0  1.1   137108  11424      S     3:37PM   0:09.60  53   0:00.00   0:00.00                 
local     1984   0.0  0.8   158772   8484  ??  S     4:54PM   0:01.13  46   0:00.31   0:00.71 /System/Library/CoreServices/Finder.app/Contents/MacOS/Finder -psn_0_1441793
          1984   0.0  0.8   158772   8484      S     4:54PM   0:01.13  46   0:00.00   0:00.00                 
          1984   0.0  0.8   158772   8484      S     4:54PM   0:01.13  46   0:00.01   0:00.01                 
local     1991   0.5  0.7   153596   7444  ??  S     4:54PM   0:06.32  47   0:03.29   0:03.03 /Applications/Utilities/Console.app/Contents/MacOS/Console -psn_0_1835009
local     2812   0.2  0.7   154792   7144  ??  S     5:21PM   0:00.70  46   0:00.13   0:00.55 /Applications/Utilities/Terminal.app/Contents/MacOS/Terminal -psn_0_2097153
          2812   0.0  0.7   154792   7144      S     5:21PM   0:00.70  46   0:00.01   0:00.01                 
          2812   0.0  0.7   154792   7144      S     5:21PM   0:00.70  46   0:00.00   0:00.00                 
          2812   0.0  0.7   154792   7144      S     5:21PM   0:00.70  46   0:00.00   0:00.00                 
root        46   0.0  0.7    47324   6980  ??  S     2:33PM   0:18.52  31   0:05.01   0:11.98 servermgrd -x
            46   0.0  0.7    47324   6980      S     2:33PM   0:18.52  31   0:00.00   0:00.00                 
            46   0.0  0.7    47324   6980      S     2:33PM   0:18.52  31   0:00.07   0:00.02                 
            46   0.0  0.7    47324   6980      S     2:33PM   0:18.52  63   0:00.34   0:00.62                 
root        81   0.0  0.6    36868   6740  ??  S     2:33PM   0:00.82  31   0:00.50   0:00.32 /System/Library/CoreServices/coreservicesd
            81   0.0  0.6    36868   6740      S     2:33PM   0:00.82  31   0:00.00   0:00.00                 
            81   0.0  0.6    36868   6740      S     2:33PM   0:00.82  31   0:00.00   0:00.00                 
local     1992   3.8  0.6   155192   6248  ??  S     4:54PM   0:28.36  46   0:01.07   0:27.28 /Applications/Utilities/Activity Monitor.app/Contents/MacOS/Activity Monitor -psn_0_1966081
          1992   0.0  0.6   155192   6248      S     4:54PM   0:28.36  46   0:00.00   0:00.00                 
local     1982   0.0  0.5   152404   5364  ??  S     4:54PM   0:00.57  46   0:00.25   0:00.32 /System/Library/CoreServices/SystemUIServer.app/Contents/MacOS/SystemUIServer -psn_0_1310721
          1982   0.0  0.5   152404   5364      S     4:54PM   0:00.57  63   0:00.00   0:00.00                 
sge        459   0.0  0.5    43712   5056  ??  S     2:33PM   0:14.12  31   0:00.07   0:00.09 /common/sge/bin/darwin/sge_qmaster
           459   0.0  0.5    43712   5056      S     2:33PM   0:14.12  31   0:00.21   0:00.50                 
           459   0.0  0.5    43712   5056      S     2:33PM   0:14.12  31   0:00.12   0:00.21                 
           459   0.0  0.5    43712   5056      S     2:33PM   0:14.12  31   0:01.22   0:01.27                 
           459   0.0  0.5    43712   5056      S     2:33PM   0:14.12  31   0:00.58   0:00.62                 
           459   0.0  0.5    43712   5056      S     2:33PM   0:14.12  31   0:00.17   0:00.44                 
           459   0.0  0.5    43712   5056      S     2:33PM   0:14.12  31   0:00.20   0:02.19                 
           459   0.0  0.5    43712   5056      S     2:33PM   0:14.12  31   0:00.00   0:00.00                 
           459   0.0  0.5    43712   5056      S     2:33PM   0:14.12  31   0:00.40   0:02.71                 
           459   0.1  0.5    43712   5056      S     2:33PM   0:14.12  31   0:00.41   0:02.73                 
root       468   0.0  0.4    41696   4504  ??  S     2:33PM   0:00.35  31   0:00.20   0:00.15 /usr/sbin/httpd
root        53   0.0  0.4   158048   4408  ??  S     2:33PM   0:01.13  31   0:00.06   0:00.04 /usr/libexec/slapd -d 0 -h ldap:/// ldapi://%2Fvar%2Frun%2Fldapi
            53   0.0  0.4   158048   4408      S     2:33PM   0:01.13  31   0:00.20   0:00.15                 
            53   0.0  0.4   158048   4408      S     2:33PM   0:01.13  31   0:00.08   0:00.28                 
            53   0.0  0.4   158048   4408      S     2:33PM   0:01.13  31   0:00.06   0:00.23                 
            53   0.0  0.4   158048   4408      S     2:33PM   0:01.13  31   0:00.00   0:00.01                 
www        473   0.0  0.4    37488   4268  ??  S     2:33PM   0:01.09  31   0:00.58   0:00.51 /usr/sbin/httpd
www       1219   0.0  0.4    37488   4260  ??  S     3:17PM   0:00.87  31   0:00.51   0:00.36 /usr/sbin/httpd
www       1309   0.0  0.4    37488   4244  ??  S     3:17PM   0:00.78  31   0:00.47   0:00.31 /usr/sbin/httpd
root       329   0.0  0.4    43856   4224  ??  S     2:33PM   0:29.70  31   0:01.49   0:00.11 /System/Library/Frameworks/CoreServices.framework/Frameworks/Metadata.framework/Versions/A/Support/mds
           329   0.0  0.4    43856   4224      S     2:33PM   0:29.70  31   0:00.33   0:00.31                 
           329   0.0  0.4    43856   4224      S     2:33PM   0:29.70  31   0:00.00   0:00.00                 
           329   0.0  0.4    43856   4224      S     2:33PM   0:29.70  31   0:00.01   0:00.00                 
           329   0.0  0.4    43856   4224      S     2:33PM   0:29.70  31   0:01.61   0:02.47                 
           329   0.0  0.4    43856   4224      S     2:33PM   0:29.70  50   0:22.31   0:01.01                 
           329   0.0  0.4    43856   4224      S     2:33PM   0:29.70  15   0:00.02   0:00.02                 
root       327   0.0  0.4    78384   4088  ??  S     2:33PM   0:00.38  31   0:00.04   0:00.01 /usr/sbin/AppleFileServer
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.11   0:00.22                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
           327   0.0  0.4    78384   4088      S     2:33PM   0:00.38  31   0:00.00   0:00.00                 
local     2368   0.0  0.4    57860   3900  p3  S     5:09PM   0:00.05  31   0:00.03   0:00.02 emacs -nw pser
local     1625   0.0  0.4   125124   3868  ??  S     3:37PM   0:00.36  48   0:00.19   0:00.14 /System/Library/CoreServices/loginwindow.app/Contents/MacOS/loginwindow console
          1625   0.0  0.4   125124   3868      S     3:37PM   0:00.36  48   0:00.00   0:00.00                 
          1625   0.0  0.4   125124   3868      S     3:37PM   0:00.36  48   0:00.00   0:00.00                 
www       2796   0.0  0.3    57420   3500  p4  S     5:20PM   0:00.06  31   0:00.02   0:00.04 emacs -nw ./orthologuesiprscan.pl
local     1636   0.0  0.3   123868   3352  ??  S     3:37PM   0:00.23  46   0:00.06   0:00.08 /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/MacOS/ARDAgent -psn_0_524289
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  46   0:00.00   0:00.00                 
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  46   0:00.00   0:00.00                 
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  46   0:00.00   0:00.00                 
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  97   0:00.01   0:00.02                 
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  46   0:00.00   0:00.00                 
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  46   0:00.00   0:00.00                 
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  46   0:00.01   0:00.00                 
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  46   0:00.00   0:00.00                 
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  46   0:00.00   0:00.00                 
          1636   0.0  0.3   123868   3352      S     3:37PM   0:00.23  46   0:00.00   0:00.00                 
local     1988   0.0  0.3   127564   3312  ??  S     4:54PM   0:01.03  46   0:00.38   0:00.66 /System/Library/PreferencePanes/UniversalAccessPref.prefPane/Contents/Resources/UniversalAccess.app/Contents/MacOS/UniversalAccess -psn_0_1703937
www       2803   0.0  0.3    31608   3280  p4  S     5:21PM   0:00.13  31   0:00.04   0:00.09 /usr/bin/perl -w ./orthologuesiprscan.pl -d orthologues -t 20 -p hmmpanther -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta -o /Users/local/FUNNYBASE
root        63   0.0  0.3    33240   3204  ??  S     2:33PM   0:01.70  31   0:00.04   0:00.05 /usr/sbin/DirectoryService
            63   0.0  0.3    33240   3204      S     2:33PM   0:01.70  31   0:00.36   0:00.46                 
            63   0.0  0.3    33240   3204      S     2:33PM   0:01.70  31   0:00.00   0:00.00                 
            63   0.0  0.3    33240   3204      S     2:33PM   0:01.70  31   0:00.00   0:00.00                 
local     1981   0.0  0.3   137028   3152  ??  S     4:54PM   0:00.25  46   0:00.08   0:00.06 /System/Library/CoreServices/Dock.app/Contents/MacOS/Dock -psn_0_1179649
          1981   0.0  0.3   137028   3152      S     4:54PM   0:00.25  46   0:00.04   0:00.07                 
www       2805   0.0  0.2    39804   2384  ??  S     5:21PM   0:00.13  13   0:00.03   0:00.08 /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Support/mdimportserver
          2805   0.0  0.2    39804   2384      S     5:21PM   0:00.13  13   0:00.00   0:00.00                 
          2805   0.0  0.2    39804   2384      S     5:21PM   0:00.13  13   0:00.01   0:00.01                 
nobody     354   0.0  0.2    27516   2312  ??  S     2:33PM   0:06.22  31   0:03.97   0:02.26 /common/sbin/gmond
local       96   0.0  0.2    57844   2280  ??  S     2:33PM   0:00.74  31   0:00.42   0:00.33 /System/Library/Frameworks/ApplicationServices.framework/Frameworks/ATS.framework/Support/ATSServer
            96   0.0  0.2    57844   2280      S     2:33PM   0:00.74  31   0:00.00   0:00.00                 
local     1986   0.0  0.2    39812   2276  ??  S     4:54PM   0:00.15  13   0:00.04   0:00.09 /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Support/mdimportserver
          1986   0.0  0.2    39812   2276      S     4:54PM   0:00.15  13   0:00.00   0:00.00                 
          1986   0.0  0.2    39812   2276      S     4:54PM   0:00.15  13   0:00.01   0:00.01                 
          1986   0.0  0.2    39812   2276      S     4:54PM   0:00.15  13   0:00.00   0:00.00                 
sge       1867   0.0  0.2    39204   2236  ??  S     4:34PM   0:00.24  13   0:00.03   0:00.09 /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Support/mdimportserver
          1867   0.0  0.2    39204   2236      S     4:34PM   0:00.24  13   0:00.00   0:00.00                 
          1867   0.0  0.2    39204   2236      S     4:34PM   0:00.24  13   0:00.06   0:00.07                 
root        45   0.0  0.2    33888   2064  ??  S     2:33PM   0:00.14  31   0:00.04   0:00.01 /usr/sbin/PasswordService -n
            45   0.0  0.2    33888   2064      S     2:33PM   0:00.14  31   0:00.05   0:00.04                 
            45   0.0  0.2    33888   2064      S     2:33PM   0:00.14  31   0:00.00   0:00.00                 
            45   0.0  0.2    33888   2064      S     2:33PM   0:00.14  31   0:00.00   0:00.00                 
            45   0.0  0.2    33888   2064      S     2:33PM   0:00.14  31   0:00.00   0:00.00                 
            45   0.0  0.2    33888   2064      S     2:33PM   0:00.14  31   0:00.00   0:00.00                 
            45   0.0  0.2    33888   2064      S     2:33PM   0:00.14  31   0:00.00   0:00.00                 
            45   0.0  0.2    33888   2064      S     2:33PM   0:00.14  31   0:00.00   0:00.00                 
            45   0.0  0.2    33888   2064      S     2:33PM   0:00.14  31   0:00.00   0:00.00                 
sge        479   0.0  0.2    30620   2008  ??  S     2:33PM   0:06.23  31   0:00.17   0:04.38 /common/sge/bin/darwin/sge_schedd
           479   0.0  0.2    30620   2008      S     2:33PM   0:06.23  31   0:00.11   0:00.21                 
           479   0.0  0.2    30620   2008      S     2:33PM   0:06.23  31   0:00.09   0:00.17                 
           479   0.0  0.2    30620   2008      S     2:33PM   0:06.23  31   0:00.27   0:00.32                 
           479   0.0  0.2    30620   2008      S     2:33PM   0:06.23  31   0:00.21   0:00.30                 
root      2068   5.7  0.2    27588   2008  p2  S     4:59PM   2:04.65  31   1:44.13   0:20.53 top -u
root        52   0.0  0.2    85924   1960  ??  S     2:33PM   0:00.34  31   0:00.14   0:00.20 /usr/sbin/named -f
local     1974   0.0  0.2    55032   1872  ??  S     4:54PM   0:00.22  31   0:00.02   0:00.02 /System/Library/CoreServices/pbs
          1974   0.0  0.2    55032   1872      S     4:54PM   0:00.22  31   0:00.13   0:00.06                 
root        59   0.0  0.2    29200   1852  ??  S     2:33PM   0:00.29  31   0:00.11   0:00.12 /usr/sbin/securityd
root        55   0.3  0.2    29924   1748  ??  S     2:33PM   0:10.64  31   0:05.52   0:04.10 /usr/sbin/configd
            55   0.0  0.2    29924   1748      S     2:33PM   0:10.64  31   0:00.50   0:00.52                 
            55   0.0  0.2    29924   1748      S     2:33PM   0:10.64  31   0:00.00   0:00.00                 
local     1987   0.0  0.2    99388   1636  ??  S     4:54PM   0:00.05  46   0:00.03   0:00.02 /Applications/iTunes.app/Contents/Resources/iTunesHelper.app/Contents/MacOS/iTunesHelper -psn_0_1572865
root        56   0.0  0.2    31248   1608  ??  S     2:33PM   0:00.15  31   0:00.06   0:00.03 /usr/sbin/coreaudiod
root       719   0.0  0.1    28508   1408  ??  S     2:44PM   0:00.03  31   0:00.02   0:00.01 /usr/sbin/cupsd -f
           719   0.0  0.1    28508   1408      S     2:44PM   0:00.03  31   0:00.00   0:00.00                 
local     2384   0.0  0.1    27372   1376  p3  S     5:09PM   0:00.77  31   0:00.58   0:00.19 /usr/bin/perl -w /usr/bin/pser
root        40   1.5  0.1    31728   1324  ??  S     2:33PM   0:37.31  31   0:28.13   0:09.16 hwmond
            40   0.0  0.1    31728   1324      S     2:33PM   0:37.31  31   0:00.00   0:00.00                 
            40   0.0  0.1    31728   1324      S     2:33PM   0:37.31  31   0:00.00   0:00.00                 
            40   0.0  0.1    31728   1324      S     2:33PM   0:37.31  31   0:00.00   0:00.00                 
            40   0.0  0.1    31728   1324      S     2:33PM   0:37.31  31   0:00.00   0:00.00                 
            40   0.0  0.1    31728   1324      S     2:33PM   0:37.31  31   0:00.00   0:00.00                 
            40   0.0  0.1    31728   1324      S     2:33PM   0:37.31  31   0:00.00   0:00.00                 
            40   0.0  0.1    31728   1324      S     2:33PM   0:37.31  31   0:00.01   0:00.00                 
local     2377   0.0  0.1    27372   1300  p3  S     5:09PM   0:00.03  31   0:00.02   0:00.01 /usr/bin/perl -w /usr/bin/pser 5
local     2374   0.0  0.1    27372   1300  p3  S     5:09PM   0:00.02  31   0:00.01   0:00.00 /usr/bin/perl -w /usr/bin/pser
www       2804   0.0  0.1    29372   1296  p4  S     5:21PM   0:00.03  31   0:00.01   0:00.00 qsub -sync y -t 1-20 /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.iprscan.sh
          2804   0.0  0.1    29372   1296      S     5:21PM   0:00.03  31   0:00.00   0:00.00                 
          2804   0.0  0.1    29372   1296      S     5:21PM   0:00.03  31   0:00.01   0:00.01                 
root      2886   0.0  0.1    30700   1240  ??  S     5:22PM   0:00.04  31   0:00.02   0:00.02 /usr/sbin/sshd -i
root       236   0.0  0.1    29020   1224  ??  S     2:33PM   0:10.52  31   0:00.01   0:00.01 /usr/sbin/serialnumberd
           236  99.0  0.1    29020   1224      R     2:33PM   0:10.52  22   0:10.45   0:00.04                 
           236   0.0  0.1    29020   1224      S     2:33PM   0:10.52  31   0:00.00   0:00.00                 
root      2397   0.0  0.1    30700   1204  ??  S     5:10PM   0:00.03  31   0:00.02   0:00.02 /usr/sbin/sshd -i
root      2302   0.0  0.1    30700   1184  ??  S     5:07PM   0:00.03  31   0:00.02   0:00.02 /usr/sbin/sshd -i
root      2072   0.0  0.1    30700   1184  ??  S     5:00PM   0:00.03  31   0:00.02   0:00.02 /usr/sbin/sshd -i
root      2016   0.0  0.1    30700   1184  ??  S     4:59PM   0:00.04  31   0:00.02   0:00.02 /usr/sbin/sshd -i
root        43   0.0  0.1    28076   1148  ??  S     2:33PM   0:02.04  31   0:00.67   0:00.58 /usr/sbin/mDNSResponder -launchdaemon
            43   0.0  0.1    28076   1148      S     2:33PM   0:02.04  31   0:00.63   0:00.16                 
local     1979   0.0  0.1    27832   1144  ??  S     4:54PM   0:00.02  46   0:00.01   0:00.01 /Library/StartupItems/WDNotifier.app/Contents/MacOS/WDNotifier -psn_0_1048577
          1979   0.0  0.1    27832   1144      S     4:54PM   0:00.02  46   0:00.00   0:00.00                 
root       301   0.0  0.1    29712   1120  ??  S     2:33PM   0:00.04  31   0:00.01   0:00.01 /usr/sbin/automount -f -m /Network -nsl -mnt /private/var/automount
           301   0.0  0.1    29712   1120      S     2:33PM   0:00.04  31   0:00.01   0:00.01                 
           301   0.0  0.1    29712   1120      S     2:33PM   0:00.04  31   0:00.00   0:00.00                 
root        51   0.0  0.1    28572   1120  ??  S     2:33PM   0:00.04  31   0:00.03   0:00.01 /usr/sbin/kadmind -passwordserver -nofork
root        57   0.0  0.1    27804   1112  ??  S     2:33PM   0:00.36  31   0:00.21   0:00.15 /usr/sbin/diskarbitrationd
root       307   0.0  0.1    29416   1076  ??  S     2:33PM   0:00.02  31   0:00.01   0:00.01 /usr/sbin/automount -f -m /automount/Servers -fstab -mnt /private/Network/Servers -m /automount/static -static -mnt /private/var/automount
           307   0.0  0.1    29416   1076      S     2:33PM   0:00.02  31   0:00.00   0:00.00                 
           307   0.0  0.1    29416   1076      S     2:33PM   0:00.02  31   0:00.00   0:00.00                 
root       148   0.0  0.1    30392   1048  ??  S     2:33PM   0:04.47  31   0:00.33   0:00.45 /usr/sbin/lookupd
           148   0.0  0.1    30392   1048      S     2:33PM   0:04.47  31   0:00.29   0:00.35                 
           148   0.0  0.1    30392   1048      S     2:33PM   0:04.47  31   0:00.31   0:00.43                 
           148   0.0  0.1    30392   1048      S     2:33PM   0:04.47  31   0:00.28   0:00.32                 
root        35   0.0  0.1    28212   1008  ??  S     2:32PM   0:01.11  31   0:00.95   0:00.16 kextd
            35   0.0  0.1    28212   1008      S     2:32PM   0:01.11  31   0:00.00   0:00.00                 
root       258   0.0  0.1    28336   1008  ??  S     2:33PM   0:00.01  31   0:00.01   0:00.00 /usr/sbin/krb5kdc -n -a
root       223   0.0  0.1    30932    980  ??  S     2:33PM   0:00.09  31   0:00.01   0:00.00 slpd -f /etc/slpsa.conf
           223   0.0  0.1    30932    980      S     2:33PM   0:00.09  31   0:00.00   0:00.00                 
           223   0.0  0.1    30932    980      S     2:33PM   0:00.09  31   0:00.00   0:00.00                 
           223   0.0  0.1    30932    980      S     2:33PM   0:00.09  31   0:00.02   0:00.01                 
           223   0.0  0.1    30932    980      S     2:33PM   0:00.09  31   0:00.01   0:00.00                 
           223   0.0  0.1    30932    980      S     2:33PM   0:00.09  31   0:00.01   0:00.01                 
daemon    1638   0.0  0.1    30748    956  ??  S     3:37PM   0:00.04  31   0:00.03   0:00.01 /System/Library/CoreServices/RemoteManagement/rmdb.bundle/bin/postmaster -D /var/db/RemoteManagement/RMDB/rmdb.data
root        39   0.0  0.1    27872    924  ??  S     2:33PM   0:00.05  31   0:00.03   0:00.02 /usr/bin/WDDrvSvc
            39   0.0  0.1    27872    924      S     2:33PM   0:00.05  31   0:00.00   0:00.00                 
root       362   0.0  0.1    47736    920  ??  S     2:33PM   0:00.31  31   0:00.00   0:00.00 /usr/bin/dsmcad
           362   0.0  0.1    47736    920      S     2:33PM   0:00.31  31   0:00.00   0:00.00                 
           362   0.0  0.1    47736    920      S     2:33PM   0:00.31  31   0:00.14   0:00.16                 
nobody     393   0.0  0.1    31776    892  ??  S     2:33PM   1:05.29  31   0:02.04   0:00.93 /common/sbin/gmetad
           393   0.0  0.1    31776    892      S     2:33PM   1:05.29  31   0:00.00   0:00.00                 
           393   0.0  0.1    31776    892      S     2:33PM   1:05.29  31   0:00.00   0:00.00                 
           393   0.0  0.1    31776    892      S     2:33PM   1:05.29  31   0:00.00   0:00.00                 
           393   0.0  0.1    31776    892      S     2:33PM   1:05.29  31   0:00.02   0:00.01                 
           393   0.0  0.1    31776    892      S     2:33PM   1:05.29  31   0:00.00   0:00.00                 
           393   0.0  0.1    31776    892      S     2:33PM   1:05.29  31   0:00.01   0:00.01                 
           393   0.0  0.1    31776    892      S     2:33PM   1:05.29  31   0:38.64   0:23.57                 
           393   0.0  0.1    31776    892      S     2:33PM   1:05.29  31   0:00.00   0:00.05                 
local     2891   0.0  0.1    27808    888  p6  S     5:22PM   0:00.03  31   0:00.02   0:00.01 -bash
local     2075   0.0  0.1    27808    888  p0  S     5:00PM   0:00.03  31   0:00.02   0:00.01 -bash
local     2305   0.0  0.1    27808    888  p3  S     5:07PM   0:00.09  31   0:00.06   0:00.03 -bash
www       2745   0.0  0.1    27808    880  p4  S     5:20PM   0:00.06  31   0:00.04   0:00.02 -su
local     2019   0.0  0.1    27808    880  p2  S     4:59PM   0:00.03  31   0:00.02   0:00.01 -bash
sge        512   0.0  0.1    28596    880  ??  S     2:33PM   0:01.29  31   0:00.43   0:00.86 /common/sge/bin/darwin/sge_execd
local     2815   0.0  0.1    27808    876  p5  S     5:21PM   0:00.03  31   0:00.01   0:00.01 -bash
local     2400   0.0  0.1    27808    872  p4  S     5:10PM   0:00.03  31   0:00.02   0:00.01 -bash
root      2698   0.0  0.1    27808    836  p4  S     5:19PM   0:00.02  31   0:00.01   0:00.00 su
root        41   0.0  0.1    27748    784  ??  S     2:33PM   0:00.52  31   0:00.30   0:00.22 /usr/sbin/kdcmond -n -a
            41   0.0  0.1    27748    784      S     2:33PM   0:00.52  31   0:00.00   0:00.00                 
root        64   0.0  0.1    27668    780  ??  S     2:33PM   0:00.12  31   0:00.03   0:00.09 /usr/sbin/distnoted
postfix    108   0.0  0.1    27412    772  ??  S     2:33PM   0:00.21  31   0:00.17   0:00.04 qmgr -l -t fifo -u
root        48   0.0  0.1    27680    736  ??  S     2:33PM   0:00.09  63   0:00.06   0:00.03 watchdogtimerd
root      2695   0.0  0.1    27856    704  p4  S     5:19PM   0:00.01  31   0:00.01   0:00.00 su
root       363   0.0  0.1    27808    692  ??  S     2:33PM   0:00.04  31   0:00.02   0:00.01 /bin/sh /usr/bin/mysqld_safe --user=mysql --skip-networking
postfix   1813   0.0  0.1    27368    688  ??  S     4:13PM   0:00.02  31   0:00.01   0:00.00 pickup -l -t fifo -u
root        54   0.0  0.1    27344    648  ??  S     2:33PM   0:00.27  31   0:00.23   0:00.03 master
root        58   0.0  0.1    28316    636  ??  S     2:33PM   0:00.09  31   0:00.02   0:00.06 /usr/sbin/memberd -x
            58   0.0  0.1    28316    636      S     2:33PM   0:00.09  31   0:00.00   0:00.00                 
            58   0.0  0.1    28316    636      S     2:33PM   0:00.09  31   0:00.00   0:00.00                 
root      1993   4.5  0.1    37332    628  ??  S     4:54PM   0:31.00  31   0:17.14   0:13.85 /Applications/Utilities/Activity Monitor.app/Contents/Resources/pmTool
www        472   0.0  0.1    60868    612  ??  S     2:33PM   0:00.19  31   0:00.00   0:00.00 /usr/sbin/webperfcache
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.06   0:00.11                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
           472   0.0  0.1    60868    612      S     2:33PM   0:00.19  31   0:00.00   0:00.00                 
cyrusima    50   0.0  0.1    27332    600  ??  S     2:33PM   0:00.20  31   0:00.06   0:00.15 master
root        42   0.0  0.1    27840    600  ??  S     2:33PM   0:00.03  31   0:00.01   0:00.02 /usr/sbin/KernelEventAgent
            42   0.0  0.1    27840    600      S     2:33PM   0:00.03  31   0:00.00   0:00.00                 
root        44   0.0  0.1    27588    588  ??  S     2:33PM   0:01.18  31   0:00.69   0:00.49 /usr/sbin/netinfod -s local
local     2942   0.0  0.1    27372    572  p6  S     5:22PM   0:00.03  31   0:00.02   0:00.01 /usr/bin/perl -w /usr/bin/qs
local     2864   0.0  0.1    27372    568  p5  S     5:21PM   0:00.06  31   0:00.04   0:00.01 /usr/bin/perl -w /usr/bin/qs
root      2814   0.0  0.1    27528    560  p5  S     5:21PM   0:00.01  31   0:00.01   0:00.00 login -pf local
root      2743   0.0  0.1    27428    540  p4  S     5:20PM   0:00.01  31   0:00.01   0:00.00 su - www
root        49   0.0  0.0    27528    500  ??  S     2:33PM   0:00.02  31   0:00.02   0:00.01 /usr/sbin/cron
local     2890   0.0  0.0    30628    496  ??  S     5:22PM   0:00.01  31   0:00.01   0:00.01 /usr/sbin/sshd -i
local     2399   0.0  0.0    30628    484  ??  S     5:10PM   0:00.05  31   0:00.03   0:00.02 /usr/sbin/sshd -i
local     2074   0.0  0.0    30628    476  ??  S     5:00PM   0:00.02  31   0:00.01   0:00.01 /usr/sbin/sshd -i
local     2018   0.0  0.0    30628    476  ??  S     4:59PM   0:00.38  31   0:00.21   0:00.17 /usr/sbin/sshd -i
root        61   0.0  0.0    27860    476  ??  S     2:33PM   0:00.49  31   0:00.01   0:00.00 /usr/sbin/notifyd
            61   0.0  0.0    27860    476      S     2:33PM   0:00.49  31   0:00.27   0:00.20                 
root      2985   0.0  0.0    27364    472  p3  R     5:23PM   0:00.00  31   0:00.00   0:00.00 ps -auxMm
local     2304   0.0  0.0    30628    464  ??  S     5:07PM   0:00.45  31   0:00.24   0:00.21 /usr/sbin/sshd -i
root        47   0.0  0.0    27280    420  ??  S     2:33PM   0:00.39  31   0:00.08   0:00.31 /usr/sbin/syslogd
root       284   0.0  0.0    27716    396  ??  S     2:33PM   0:00.03  31   0:00.02   0:00.01 mountd
root         1   0.0  0.0    28344    384  ??  S     2:32PM   0:00.45  32   0:00.10   0:00.03 /sbin/launchd
             1   0.0  0.0    28344    384      S     2:32PM   0:00.45  32   0:00.01   0:00.00                 
             1   0.0  0.0    28344    384      S     2:32PM   0:00.45  32   0:00.22   0:00.09                 
root       205   0.0  0.0    27504    380  ??  S     2:33PM   0:00.57  31   0:00.38   0:00.19 ntpd -f /var/run/ntp.drift -p /var/run/ntpd.pid
daemon     139   0.0  0.0    27248    376  ??  S     2:33PM   0:00.01  31   0:00.01   0:00.00 /usr/sbin/portmap
local     2125   0.0  0.0    27376    356  p0  S     5:00PM   0:00.01  31   0:00.00   0:00.00 tail -f /var/log/system.log
root        98   0.0  0.0    27256    340  ??  S     2:33PM   0:00.01  31   0:00.01   0:00.00 /usr/libexec/getty serial.57600 tty.serial
daemon    1640   0.0  0.0    29316    280  ??  S     3:37PM   0:00.00  31   0:00.00   0:00.00 postgres: stats collector process                                                                                                                    n  
root       141   0.0  0.0    27328    252  ??  S     2:33PM   0:00.01  31   0:00.01   0:00.00 rpc.lockd
root       257   0.0  0.0    27328    248  ??  S     2:33PM   0:05.01  31   0:03.94   0:01.07 /usr/sbin/natd -f /etc/nat/natd.conf.apple
root       138   0.0  0.0   289400    236  ??  S     2:33PM   0:00.00  31   0:00.00   0:00.00 rpc.statd
root       326   0.0  0.0    29076    224  ??  S     2:33PM   0:00.00  31   0:00.00   0:00.00 /usr/sbin/AppleFileServer
root        68   0.0  0.0    27244    220  ??  S     2:33PM   0:10.07  31   0:09.93   0:00.14 /usr/sbin/update
daemon    1639   0.0  0.0    30304    216  ??  S     3:37PM   0:00.00  31   0:00.00   0:00.00 postgres: stats buffer process                                                                                                                       n  
root       157   0.0  0.0    27248    204  ??  S     2:33PM   0:00.00  31   0:00.00   0:00.00 /usr/libexec/crashreporterd
root       130   0.0  0.0    29304    184  ??  S     2:33PM   0:00.00  31   0:00.00   0:00.00 nfsiod -n 4
           130   0.0  0.0    29304    184      S     2:33PM   0:00.00  31   0:00.00   0:00.00                 
           130   0.0  0.0    29304    184      S     2:33PM   0:00.00  31   0:00.00   0:00.00                 
           130   0.0  0.0    29304    184      S     2:33PM   0:00.00  31   0:00.00   0:00.00                 
           130   0.0  0.0    29304    184      S     2:33PM   0:00.00  31   0:00.00   0:00.00                 
root       252   0.0  0.0    27240    172  ??  S     2:33PM   0:00.00  31   0:00.00   0:00.00 /usr/libexec/ipfwloggerd
root        31   0.0  0.0    27260    168  ??  S     2:32PM   0:00.00  63   0:00.00   0:00.00 /sbin/dynamic_pager -F /private/var/vm/swapfile
root       144   0.0  0.0    27328    160  ??  S     2:33PM   0:00.00  31   0:00.00   0:00.00 rpc.lockd
root       289   0.0  0.0    27240    156  ??  S     2:33PM   0:00.00  31   0:00.00   0:00.00 nfsd-master    
root       334   0.0  0.0    27252    152  ??  S     2:33PM   0:00.41  31   0:00.38   0:00.03 /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Support/ARDHelper
root       295   0.0  0.0    27240    144  ??  S     2:33PM   0:00.95  31   0:00.95   0:00.00 nfsd-server    
root       296   0.0  0.0    27240    144  ??  S     2:33PM   0:00.23  31   0:00.23   0:00.00 nfsd-server    
root       292   0.0  0.0    27240    144  ??  S     2:33PM   0:01.80  31   0:01.80   0:00.00 nfsd-server    
root       297   0.0  0.0    27240    144  ??  S     2:33PM   0:01.43  31   0:01.43   0:00.00 nfsd-server    
root       298   0.0  0.0    27240    144  ??  S     2:33PM   0:00.50  31   0:00.50   0:00.00 nfsd-server    
root       294   0.0  0.0    27240    144  ??  S     2:33PM   0:00.99  31   0:00.99   0:00.00 nfsd-server    
root       291   0.1  0.0    27240    144  ??  S     2:33PM   0:35.06  31   0:35.06   0:00.00 nfsd-server    
root       300   0.0  0.0    27240    144  ??  S     2:33PM   0:00.26  31   0:00.26   0:00.00 nfsd-server    
root       290   0.0  0.0    27240    144  ??  S     2:33PM   0:08.29  31   0:08.29   0:00.00 nfsd-server    
root       299   0.0  0.0    27240    144  ??  S     2:33PM   0:00.22  31   0:00.22   0:00.00 nfsd-server    
root       471   0.0  0.0    27324    132  ??  S     2:33PM   0:00.00  31   0:00.00   0:00.00 /usr/sbin/webperfcache
<<<


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
LIST PORTS FOR ALL SERVICES WITH lookupd:

lookupd -q service

LIST INFO FOR ALL USERS WITH lookupd:

lookupd -q users

USAGE:
           lookupd -q category [[-a key [value ...]] ...]

WHERE category may be:

user
group
host
network
service
protocol
rpc
mount
printer
bootparam
bootp
alias
netgroup

lookupd -q network

</entry>



<entry [Thu Jan 18 15:40:08 EST 2007] LOOKUPD ERROR CAUSES FREEZE>



Jan 17 21:48:34 gems ctl_cyrusdb[7562]: done checkpointing cyrus databases
Jan 17 21:50:06 gems su: local to root on /dev/ttyp0
Jan 17 22:01:32 gems lookupd[157]: NetInfo connection failed for server 127.0.0.1/local
Jan 17 22:01:32 gems lookupd[157]: ni_statistics: Communication failure
Jan 17 22:03:17 gems lookupd[157]: NetInfo connection failed for server 127.0.0.1/local
Jan 17 22:03:17 gems lookupd[157]: ni_statistics: Communication failure
Jan 17 22:05:01 gems lookupd[157]: NetInfo connection failed for server 127.0.0.1/local
Jan 17 22:05:01 gems lookupd[157]: ni_statistics: Communication failure

Another Terminal-based workaround, previously published on MacFixIt for a similar issue, has proven successful in this case. It involves setting a higher timeout for the lookupd process. Open the Terminal, and enter the following commands, pressing return after each one.

   1. sudo mkdir /etc/lookupd
   2. sudo sh -c "echo TimeToLive 300 > /etc/lookupd/hosts"
   3. sudo sh -c "echo ValidateCache NO >> /etc/lookupd/hosts"
   4. sudo sh -c "echo LogFile /etc/lookupd/lookupd.log >> /etc/lookupd/hosts"
   5. sudo sh -c "echo LogPriority 7 >> /etc/lookupd/hosts"
   6. sudo sh -c "echo StatisticsEnabled YES >> /etc/lookupd/hosts"
   7. sudo sh -c "echo MaxThreads 128 >> /etc/lookupd/hosts"

COPY TO NetInfo CONFIGURATION DIRECTORY:

sudo cp /etc/lookupd/hosts /locations/lookupd/hosts

After entering these commands, restart your Mac and power cycle any network adapters (Cable/DSL modems), AirPort base stations, other routers, etc. 

CHECK IF LOOKUPD IS RUNNING:
ps aux | grep lookupd
root       148   0.4  0.1    30264    976  ??  Ss    2:33PM   0:03.21 /usr/sbin/lookupd
local     1729   0.0  0.3    57324   2996  p0  S     3:46PM   0:00.04 emacs -nw /etc/lookupd
local     1735   0.0  0.3    57636   3320  p0  S     3:47PM   0:00.04 emacs -nw /private/etc/mach_init.d/lookupd.plist
local     1741   0.0  0.0    18048    248  p0  S+    3:47PM   0:00.00 grep lookupd


CHECK WHICH SERVICES LOOKUPD IS USING WITH dscl (Directory Service command line utility):
gems:~ local$ dscl localhost
 > ls
Bonjour
BSD
LDAPv3
NetInfo
SLP

Search
Contact
 > quit


lookupd -f MaxThreads
         Maximum number of threads in the query dispatcher.  The default is
         64.  Under moderatly heavy load, only 5 or 6 threads are used, so 64
         is usually more than enough.

         MaxIdleThreads

FROM man lookupd:
>>>...

When lookupd searches for information about an item, it queries agents in
a specific order until the item is found or until all sources of informa-information
tion have been consulted without finding the desired item.  By default,
lookupd first queries its cache agent, then NetInfo, then the Directory
Services agent.  If the item is a host or network, lookupd will query the
cache, the Flat File agent, then the DNS agent, then NetInfo, and Direc-Directory
tory Services last.  The default search order for services, protocols,
and rpc protocols uses the cache, then the Flat File agent, NetInfo, and
then Directory Services.

...

CONFIGURATION

     Configuration parameters may be placed in a set of files in the local
     file system, or they may be written in a set of directories in NetInfo.
     These parameters will override default settings.  There may be one file
     or one configuration directory in NetInfo for each agent, and one file or
     NetInfo directory for each lookup category.  There may also be a global
     configuration file or NetInfo directory.  Additionally, each agent may
     have category-specific configuration.

     lookupd searches for configuration in the local file system first.  If no
     configuration is given in the file system, lookupd checks for a configu-configuration
     ration directory in the local NetInfo domain.

     The configuration source may be specified on the command line as a
     startup option.  The configuration source may be specified using one of
     the following command-line options:

           -c default
           -c file path
           -c netinfo domain path

   FILE-BASED CONFIGURATION
     Configuration settings for lookup may be placed in files under the direc-directory
     tory /etc/lookupd.  A file named ``global'' is used to store global set-settings
     tings for lookupd and for individual agents.  Other files specify set-settings
     tings for each lookup category.  It is not necessary to create every con-configuration
     figuration file.  Just create those in which you wish to override the
     default values of configuration parameters.  The files that may be cre-created
     ated are:

           /etc/lookupd/global
           /etc/lookupd/users
           /etc/lookupd/groups
           /etc/lookupd/hosts
           /etc/lookupd/networks
           /etc/lookupd/services
           /etc/lookupd/protocols
           /etc/lookupd/rpcs
           /etc/lookupd/mounts
           /etc/lookupd/printers
           /etc/lookupd/bootparams
           /etc/lookupd/bootp
           /etc/lookupd/aliases
           /etc/lookupd/netgroups
           /etc/lookupd/agents
           /etc/lookupd/agents/CacheAgent/global
           /etc/lookupd/agents/DNSAgent/global
           /etc/lookupd/agents/FFAgent/global
           /etc/lookupd/agents/NIAgent/global
           /etc/lookupd/agents/NILAgent/global
           /etc/lookupd/agents/NISAgent/global

     Category-specific configuration files may appear in an agent's subdirec-subdirectory.
     tory.  For example, category-specific files for NIAgent are:

           /etc/lookupd/agents/NIAgent/global
           /etc/lookupd/agents/NIAgent/users
           /etc/lookupd/agents/NIAgent/groups
           /etc/lookupd/agents/NIAgent/hosts
           /etc/lookupd/agents/NIAgent/networks
           /etc/lookupd/agents/NIAgent/services
           /etc/lookupd/agents/NIAgent/protocols
           /etc/lookupd/agents/NIAgent/rpcs
           /etc/lookupd/agents/NIAgent/mounts
           /etc/lookupd/agents/NIAgent/printers
           /etc/lookupd/agents/NIAgent/bootparams
           /etc/lookupd/agents/NIAgent/bootp
           /etc/lookupd/agents/NIAgent/aliases
           /etc/lookupd/agents/NIAgent/netgroups

     Note that only some agents make use of category-specific configurations.
     They are described in the AGENTS section below.

   NETINFO-BASED CONFIGURATION
     Configuration directories in NetInfo must be placed in a subtree begin-beginning
     ning at either the /config/lookupd or the /locations/lookupd directory.
     /config/lookupd is checked first, and /locations/lookupd is checked if
     /config/lookupd does not exist.  /locations/lookupd may contain global
     settings, stored as values for various keys.  Configuration options for
     specific categories reside in the directories:

           /locations/lookupd/users
           /locations/lookupd/groups
           /locations/lookupd/hosts
           /locations/lookupd/networks
           /locations/lookupd/services
           /locations/lookupd/protocols
           /locations/lookupd/rpcs
           /locations/lookupd/mounts
           /locations/lookupd/printers
           /locations/lookupd/bootparams
           /locations/lookupd/bootp
           /locations/lookupd/aliases
           /locations/lookupd/netgroups

     There may also be configuration directories for each agent.  These must
     be subdirectories of the /locations/lookupd/agents directory:

           /locations/lookupd/agents/CacheAgent
           /locations/lookupd/agents/DNSAgent
           /locations/lookupd/agents/FFAgent
           /locations/lookupd/agents/NIAgent
           /locations/lookupd/agents/NILAgent
           /locations/lookupd/agents/NISAgent

     Each of these agent-specific directories may have category specific sub-subdirectories,
     directories, for example:

           /locations/locations/agents/NIAgent/printers
           /locations/locations/agents/NIAgent/hosts
           ...

   CONFIGURATION KEYS
     If configuration parameters are stored in a file, each line of the file
     will be of the form:

           key value [value ...]

     Lines beginning with ``#'' are treated as comments.

     Configuration directories in NetInfo have property keys and values as
     specified below.

     Keys and permissible values for the main (global) lookupd configuration
     directory or file are shown in the following table.  keys and values that
     apply to specific agents are described in the AGENTS section.

         LogFile
         Name of a log file that contains a copy of all messages sent to sys-syslog.
         log.  There is no default (i.e. no log file is kept).

         LogPriority
         Sets the maximum priority that will be logged.  Note that syslog's
         highest priority (LOG_EMERG) is 0, with priority 7 being the lowest
         priority (LOG_DEBUG).  The default is LOG_NOTICE, meaning that only
         messages of LOG_NOTICE or higher priority will be logged.  This value
         can also be set on the command line using the -l priority option.

         StatisticsEnabled
         If given the value YES, this setting will enable statistics-gather-statistics-gathering.
         ing.  These statistics can then be fetched by calling lookupd with
         the -statistics command line option.  Details on these statistics are
         found in the PERFORMANCE TUNING AND TROUBLESHOOTING section.  The
         default value is NO, unless lookupd is run in debug mode with the -d
         or -D options.

         Debug
         If given the value YES, statistics gathering is enabled, and the Log-LogPriority
         Priority is set to LOG_DEBUG.

         MaxThreads
         Maximum number of threads in the query dispatcher.  The default is
         64.  Under moderatly heavy load, only 5 or 6 threads are used, so 64
         is usually more than enough.

         MaxIdleThreads
         When a thread finishes servicing a query, it will usually go back to
         the message queue to wait for another query.  This setting limits the
         maximum number of idle threads waiting on the queue.  If a thread
         finishes servicing a query and MaxIdleThreads are already waiting on
         the queue, the thread will exit.  The default value is 2.

         MaxIdleServers
         The dispatcher uses a server object to actually answer a client
         lookup.  One server is required for each active thread.  The dis-dispatcher
         patcher keeps a pool of servers so that they can be re-used.  This
         setting limits the maximum number of servers in the pool, waiting for
         a query to answer.  The default value is 4.

         ValidateCache
         This boolean value determines whether cache validation is enabled for
         all cache categories.  The default is YES.  Use NO to disable valida-validation.
         tion.  The setting of this value may be over-ridden for specific
         cache categories (see below).

         ValidationLatency
         If Cache validation is enabled, this integer value specifies the num-number
         ber of seconds that may elapse between successive validation checks
         for a particular agent.  The default is 15 seconds.  This value
         applies to specific agents rather than to the cache.  The setting of
         this value may be over-ridden for specific agents (see below).

         CacheCapacity
         Maximum number of objects in the cache for each category (e.g. this
         many users, this many hosts, ...).  Least-recently-used objects are
         removed when space is required.  By default, there is no limit to the
         cache size.

         TimeToLive
         Time to live (measured in seconds) in cache.  The default is 43200
         seconds (12 hours).  This is the default mechanism used to limit the
         growth of the cache.

         LookupOrder
         Sets the lookup order for all categories, although you may override
         this for specific categories.  This key takes multiple values.  The
         default for most categories is CacheAgent, NIAgent, and then DSAgent.
         For hosts and networks, the default lookup order is CacheAgent, FFA-FFAgent,
         gent, DNSAgent, NIAgent, then DSAgent.  For services, protocols, and
         rpc, the default order is CacheAgent, FFAgent, NIAgent, then DSAgent.
         Details about specifying agents in a lookup order may be found in the
         AGENTS section.

         Timeout
         Time to wait for a response from a server.  The default value is 30
         seconds.  Note that this timeout applies individually to all agents.
         It is not a global timeout for any lookupd query.  The total time
         that might be taken for a single query to lookupd depends on how many
         agents are involved in the lookup order for that category of item.

     Options that can be set per lookup category are ValidateCache,
     CacheCapacity, TimeToLive, and LookupOrder.

... <<<

</entry>



<entry [Tue Jan 16 02:48:10 EST 2007] Reply-To : 	inquiry-support@bioteam.net>



Hi Chris,

Thanks for spotting that. I'm afraid I must have changed the permissions while I was trying to get iprscan3.pl to work.

I'm on the road on Tuesday and Wednesday this week but I'll get back to it on Wednesday evening if I can't check it remotely and I'll let you know how it goes. Alternately, if you manage to find out the cause of the freezes, please let me know.

The system is down now (no ssh and no ping) and I'm not quite sure why as I haven't run anything on it today. Unless you were testing it? In any case, I'll restart it before I leave.

Cheers,

Stuart.


From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Tuesday, January 16, 2007 2:01 AM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] problem with SGE array job - froze on array job
	

Stuart,

I looked in the message file:

/common/sge/default/spool/qmaster/messages

And I saw this:

01/15/2007 20:56:12|qmaster|gems|E|can't open "/common/sge/default/ 
common/act_qmaster" for writing qmaster hostname: Permission denied

So I did this:

gems:/common/sge/default/common root# ls -l
total 13800
-rw-r--r--   1 www  admin  6993495 Jan 12 23:40 accounting
-rw-r--r--   1 www  admin       21 Jan 12 18:52 act_qmaster
-r--r--r--   1 www  admin      358 Jul 25 07:49 bootstrap
-rw-r--r--   1 www  admin       48 Jul 25 07:49 host_aliases
-rw-r--r--   1 www  admin     1994 Jul 25 07:49 qtask
-rw-r--r--   1 www  admin      726 Jul 25 07:49 settings.csh
-rwxr-xr-x   1 www  admin      717 Jul 25 07:49 settings.sh
-rwxr-xr-x   1 www  admin      717 Dec 26 20:43 settings.sh.bkp
-rwxr-xr-x   1 www  admin      717 Dec 26 20:44 settings.sh.bkp2
-rw-r--r--   1 www  admin     1680 Jan 12 10:21 sge_aliases
-rw-r--r--   1 www  admin     2164 Jul 25 07:49 sge_request
-rwxr-xr-x   1 www  admin     8285 Jul 25 07:49 sgeexecd
-rwxr-xr-x   1 www  admin    14088 Jul 25 07:49 sgemaster

Those files ought to be owned by user "sge", not "www".

I have no idea how that happened.   I changed it back like this:

gems:/common root# chown -R sge sge

And things started right back up:

gems:/common root# SystemStarter start SGE
Checking disks
gems:/common root# ps -auxc | grep sge
sge       2185   0.0  0.2    39204   2276  ??  SNs   8:56PM   0:00.14  
mdimportse
sge       2377   0.0  0.4    42668   4484  ??  S     8:59PM   0:00.33  
sge_qmaste
sge       2382   0.0  0.2    30620   1912  ??  S     8:59PM   0:00.07  
sge_schedd
sge       2415   0.0  0.1    28596    868  ??  S     8:59PM   0:00.01  
sge_execd
gems:/common root# qstat -f
queuename                      qtype used/tot. load_avg arch           
states
------------------------------------------------------------------------ 
----
all.q@gems.rsmas.miami.edu     BIP   0/2       0.05     darwin        d
------------------------------------------------------------------------ 
----
all.q@node001.cluster.private  BIP   2/2       0.00     darwin
    6244 0.55500 iprscan-20 www          t     01/15/2007 20:59:59     1
    6253 0.55500 iprscan-20 www          t     01/15/2007 20:59:59     1


In fact, while I was writing this, a bunch of jobs completed, so far  
as I can tell.

Let me know what you see when you log in.  Good idea removing the  
firewire drive, by the way.

-Chris Dwan
  The BioTeam


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
On Jan 15, 2007, at 12:31 PM, stuart young via RT wrote:


Hi Chris,

I've removed the firewire drive from the system but I now get this error:

gems:~ local$ qstat -f
error: commlib error: can't connect to service (Connection refused)
unable to contact qmaster using port 701 on host "gems.rsmas.miami.edu"
gems:~ local$ source /common/sge/default/common/settings.sh
gems:~ local$ qstat -f
error: commlib error: can't connect to service (Connection refused)
unable to contact qmaster using port 701 on host "gems.rsmas.miami.edu"
gems:~ local$ 

Are there any files I should copy over from the firewire drive to allow SGE to run?

Cheers,

Stuart.


>From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
>Reply-To: inquiry-support@bioteam.net
>To: youngstuart@hotmail.com
>Subject: Re: [bioteam.net #13817] problem with SGE array job - froze on array job
>Date: Mon, 15 Jan 2007 09:17:00 -0500
>
>
>Stuart,
>
>I'm sorry, but I have to call a temporary halt on my efforts with
>debugging the system until it's in a more final hardware
>configuration.  We're no longer dealing with anything that I can even
>remotely claim is part of iNquiry or the BioTeam software.
>
>One suggestion I have is to stop the export of the firewire disk,
>since that's our current hypothesis for why the system has been
>freezing.  It's possible that some machine is still mounting that
>volume and generating lots of activity.
>
>What do you think is the timetable on which you'll be able to resolve
>the storage issue?  I.e:  How long are you going to be in this
>partially functional configuration?
>
>-Chris Dwan
>   The BioTeam




>On Jan 13, 2007, at 11:18 PM, stuart young via RT wrote:
>
> >
> > <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >
> > Hi Chris,
> >
> > I just ran the array job 'killer app'
> > (/Users/local/FUNNYBASE/bin/unigene/othologuesiprscan.pl) and it
> > froze.
> > Can't ssh or ping it. Here's the qstat -f output just before it froze:
> >
> > Sat Jan 13 23:07:43 PST 2007
> > ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
> > :::::::::::::::::::::::::
> > queuename                      qtype used/tot. load_avg
> > arch          states
> > ----------------------------------------------------------------------
> > ------
> > all.q@gems.rsmas.miami.edu     BIP   0/2       0.55
> > darwin        d
> > ----------------------------------------------------------------------
> > ------
> > all.q@node001.cluster.private  BIP   2/2       0.17     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 1
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 18
> > ----------------------------------------------------------------------
> > ------
> > all.q@node002.cluster.private  BIP   2/2       0.30     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 13
> >    6239 0.55500 iprscan-20 www          r     01/13/2007
> > 23:02:37     1
> > ----------------------------------------------------------------------
> > ------
> > all.q@node004.cluster.private  BIP   2/2       0.10     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 4
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 15
> > ----------------------------------------------------------------------
> > ------
> > all.q@node005.cluster.private  BIP   2/2       0.04     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 8
> >    6234 0.55500 iprscan-20 www          r     01/13/2007
> > 23:02:37     1
> > ----------------------------------------------------------------------
> > ------
> > all.q@node006.cluster.private  BIP   2/2       0.09     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 6
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 14
> > ----------------------------------------------------------------------
> > ------
> > all.q@node007.cluster.private  BIP   2/2       0.04     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 10
> >    6236 0.55500 iprscan-20 www          r     01/13/2007
> > 23:02:37     1
> > ----------------------------------------------------------------------
> > ------
> > all.q@node008.cluster.private  BIP   2/2       0.09     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 11
> >    6237 0.55500 iprscan-20 www          r     01/13/2007
> > 23:02:37     1
> > ----------------------------------------------------------------------
> > ------
> > all.q@node009.cluster.private  BIP   2/2       0.11     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 9
> >    6235 0.55500 iprscan-20 www          r     01/13/2007
> > 23:02:37     1
> > ----------------------------------------------------------------------
> > ------
> > all.q@node010.cluster.private  BIP   2/2       0.10     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 2
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 16
> > ----------------------------------------------------------------------
> > ------
> > all.q@node011.cluster.private  BIP   2/2       0.15     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 3
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 19
> > ----------------------------------------------------------------------
> > ------
> > all.q@node012.cluster.private  BIP   0/2       -NA-
> > darwin        adu
> > ----------------------------------------------------------------------
> > ------
> > all.q@node013.cluster.private  BIP   2/2       0.07     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 7
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 20
> > ----------------------------------------------------------------------
> > ------
> > all.q@node014.cluster.private  BIP   2/2       0.09     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 12
> >    6238 0.55500 iprscan-20 www          r     01/13/2007
> > 23:02:37     1
> > ----------------------------------------------------------------------
> > ------
> > all.q@node015.cluster.private  BIP   2/2       0.16     darwin
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 5
> >    6233 0.55500 orthologue www          r     01/13/2007
> > 23:02:22     1 17
> > ----------------------------------------------------------------------
> > ------
> > all.q@node016.cluster.private  BIP   0/2       -NA-
> > darwin        adu
> >
> > ######################################################################
> > ######
> > - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS -
> > PENDING JOBS
> > ######################################################################
> > ######
> >    6240 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6241 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6242 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6243 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6244 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6245 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6246 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6247 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6248 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6249 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6250 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6251 0.55500 iprscan-19 www          qw    01/13/2007
> > 23:02:24     1
> >    6252 0.55500 iprscan-19 www          qw    01/13/2007
> > 23:02:24     1
> >    6253 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:24     1
> >    6254 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:41     1
> >    6255 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:41     1
> >    6256 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:41     1
> >    6257 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:41     1
> >    6258 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:41     1
> >    6259 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:41     1
> >    6260 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:41     1
> >    6261 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6262 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6263 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6264 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6265 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6266 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6267 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6268 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6269 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6270 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6271 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6272 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6273 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:42     1
> >    6274 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:43     1
> >    6275 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:43     1
> >    6276 0.55500 iprscan-20 www          qw    01/13/2007
> > 23:02:43     1
> >
> > Sat Jan 13 23:07:53 PST 2007
> > ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
> > :::::::::::::::::::::::::
> >
> >
> > Cheers,
> >
> > Stuart.
> >
> >
> >
> >> From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
> >> Reply-To: inquiry-support@bioteam.net
> >> To: youngstuart@hotmail.com
> >> Subject: Re: [bioteam.net #13817] problem with SGE array job -
> >> froze on
> >> array job
> >> Date: Fri, 12 Jan 2007 10:53:54 -0500
> >>
> >>
> >> Stuart,
> >>
> >> Okay, disk space is really tight, but I think that the system is now
> >> stable enough to demonstrate functionality.
> >>
> >> I wound up moving /common onto /Volumes/gemshd3 and /Users and /
> >> Groups onto /Volumes/gemshd2.  There is about 6GB left on each of
> >> these.  This means that as soon as you start writing results you're
> >> going to run out of space.
> >>
> >> I ran an all vs. all BLASTP of the ecoli proteins.  This knocked the
> >> system over instantly before, and now it ran to completion.
> >>
> >> I left all the original files on the firewire disk.  My suggestion is
> >> that you dig around in /Users and /common, and delete some files that
> >> you're not using right now.  Then, when you get the new disks you can
> >> do the "real" data move.
> >>
> >> Let me know what you see.
> >>
> >> -Chris Dwan
> >>   The BioTeam
> >>
> >> On Jan 12, 2007, at 9:16 AM, stuart young via RT wrote:
> >>
> >>>
> >>> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >>>
> >>> Hi Chris,
> >>>
> >>> Okay, I'll stay off until you're done.
> >>>
> >>> Cheers,
> >>>
> >>> Stuart.
> >>>
> >>>
> >>>> From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
> >>>> Reply-To: inquiry-support@bioteam.net
> >>>> To: youngstuart@hotmail.com
> >>>> Subject: Re: [bioteam.net #13817] problem with SGE array job -
> >>>> froze on
> >>>> array job
> >>>> Date: Fri, 12 Jan 2007 01:34:00 -0500
> >>>>
> >>>>
> >>>> Stuart,
> >>>>
> >>>> Not quite balanced yet.  There is too much data in the firewire
> >>>> disk
> >>>> to fit on one disk, so I'm balancing /common onto one and /Users
> >>>> onto
> >>>> the other.
> >>>>
> >>>> I'll send a note in the morning when it's sorted.
> >>>>
> >>>> -Chris Dwan
> >>>>   The BioTeam
> >>>>
> >>>> On Jan 11, 2007, at 5:27 PM, stuart young via RT wrote:
> >>>>
> >>>>>
> >>>>> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >>>>>
> >>>>> Hi Chris,
> >>>>>
> >>>>> Okay, let me know if you have any problems getting on with remote
> >>>>> desktop.
> >>>>> I've found that remote desktop can be a bit temperamental
> >>>>> sometimes
> >>>>> but it
> >>>>> can be fixed by just restarting it on the head node:
> >>>>>
> >>>>> /System/Library/CoreServices/RemoteManagement/ARDAgent.app/
> >>>>> Contents/
> >>>>> Resources/kickstart
> >>>>> -stop
> >>>>>
> >>>>> /System/Library/CoreServices/RemoteManagement/ARDAgent.app/
> >>>>> Contents/
> >>>>> Resources/kickstart
> >>>>> -restart -agent
> >>>>>
> >>>>> Cheers,
> >>>>>
> >>>>> Stuart.
> >>>>>
> >>>>>
> >>>>>> From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
> >>>>>> Reply-To: inquiry-support@bioteam.net
> >>>>>> To: youngstuart@hotmail.com
> >>>>>> Subject: Re: [bioteam.net #13817] problem with SGE array job -
> >>>>>> froze on
> >>>>>> array job
> >>>>>> Date: Thu, 11 Jan 2007 17:07:14 -0500
> >>>>>>
> >>>>>>
> >>>>>> Stuart,
> >>>>>>
> >>>>>> I'm going to try to get into the machine via Remote Desktop.
> >>>>>> There's
> >>>>>> something truly weird going on here.  These things shouldn't
> >>>>>> reboot
> >>>>>> at the drop of a hat.
> >>>>>>
> >>>>>> -Chris Dwan
> >>>>>>   The BioTeam
> >>>>>>
> >>>>>> On Jan 11, 2007, at 1:06 PM, stuart young via RT wrote:
> >>>>>>
> >>>>>>>
> >>>>>>> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >>>>>>>
> >>>>>>> Hi Chris,
> >>>>>>>
> >>>>>>> I found the same kind of problem - I couldn't remotely access
> >>>>>>> the
> >>>>>>> head node
> >>>>>>> though I got the ssh password prompt for users 'local',
> >>>>>>> 'vanwye' and
> >>>>>>> 'local2'. When I went to reboot the head node manually and
> >>>>>>> login on
> >>>>>>> the
> >>>>>>> console, I saw the message that it couldn't find the '/Users/
> >>>>>>> local/' folder
> >>>>>>> and the RAID drive had disappeared. After I power on/offed the
> >>>>>>> RAID
> >>>>>>> drive,
> >>>>>>> reconnected it and logged in on the head node local console as
> >>>>>>> user
> >>>>>>> 'local',
> >>>>>>> I found it was back to normal.
> >>>>>>>
> >>>>>>> I could also ssh from the head node to my remote workstation and
> >>>>>>> login by
> >>>>>>> ssh from my remote workstation on the head node as users
> >>>>>>> 'vanwye',
> >>>>>>> 'local'
> >>>>>>> and 'local2' (btw, I changed the password to the same for user
> >>>>>>> 'local').
> >>>>>>>
> >>>>>>> As far as I can tell, there's no special network setup for the
> >>>>>>> head
> >>>>>>> node
> >>>>>>> compared to my workstation. I also checked up with the guys who
> >>>>>>> run
> >>>>>>> the
> >>>>>>> machine room and they haven't done anything with their network
> >>>>>>> structure
> >>>>>>> lately. So hopefully the RAID drive was the source of the
> >>>>>>> problem.
> >>>>>>> I'll run
> >>>>>>> the array job 'killer app' to see if rebooting the RAID drive
> >>>>>>> has
> >>>>>>> solved
> >>>>>>> that problem too.
> >>>>>>>
> >>>>>>> Cheers,
> >>>>>>>
> >>>>>>> Stuart.
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>>> From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
> >>>>>>>> Reply-To: inquiry-support@bioteam.net
> >>>>>>>> To: youngstuart@hotmail.com
> >>>>>>>> Subject: Re: [bioteam.net #13817] problem with SGE array job -
> >>>>>>>> froze on
> >>>>>>>> array job
> >>>>>>>> Date: Thu, 11 Jan 2007 08:49:12 -0500
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> Stuart,
> >>>>>>>>
> >>>>>>>> Here's an interesting fact about the system freezes:
> >>>>>>>>
> >>>>>>>> This morning, I couldn't even get a "Password:" prompt when I
> >>>>>>>> tried
> >>>>>>>> to ssh in as the user "vanwye", but when I tried either
> >>>>>>>> "root" or
> >>>>>>>> "local2", the password prompt came right up.  Unfortunately,
> >>>>>>>> the
> >>>>>>>> password you provided for that account doesn't work for me.  I
> >>>>>>>> probably transcribed it incorrectly.
> >>>>>>>>
> >>>>>>>> This might indicate that some network service is being
> >>>>>>>> overwhelmed,
> >>>>>>>> since network resolution and user authentication go hand in
> >>>>>>>> hand.
> >>>>>>>> This also lines up with my file corruption issues when I was
> >>>>>>>> downloading software updates directly to the portal.
> >>>>>>>>
> >>>>>>>> Could you please describe the network setup by which I'm
> >>>>>>>> accessing
> >>>>>>>> the cluster?  How is "genomics" wired differently than "gems?"
> >>>>>>>>
> >>>>>>>> -Chris Dwan
> >>>>>>>>   The BioTeam
> >>>>>>>>
> >>>>>>>> On Jan 10, 2007, at 6:46 PM, stuart young via RT wrote:
> >>>>>>>>
> >>>>>>>>>
> >>>>>>>>> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >>>>>>>>>
> >>>>>>>>> Hi Chris,
> >>>>>>>>>
> >>>>>>>>> I just tried to run an array job using BLAST, something which
> >>>>>>>>> I've
> >>>>>>>>> hitherto
> >>>>>>>>> been able to do without any problems but now I get this:
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> ../funnybasearrayblast.pl -d funnybase9 -t refseq-dog
> >>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>> Using default conf file: conf/default.conf
> >>>>>>>>>
> >>>>>>>>> Doing BLAST against these databases: refseq-dog
> >>>>>>>>>
> >>>>>>>>> ....
> >>>>>>>>>
> >>>>>>>>> Doing ARRAYBLAST of version 0.01 of database
> >>>>>>>>> 'funnybase9' (total
> >>>>>>>>> accessions
> >>>>>>>>> 0) against refseq-dog (E-value cutoff: 1e-05)
> >>>>>>>>> qsub command: qsub -sync y -t 1-32
> >>>>>>>>> /Users/local/FUNNYBASE/pipeline/funnybase9/blast/funnybase9-
> >>>>>>>>> refseq-
> >>>>>>>>> dog.sh
> >>>>>>>>> Completed qsub. Job output:
> >>>>>>>>>
> >>>>>>>>> Your job 6048.1-32:1 ("funnybase9-refseq-dog.sh") has been
> >>>>>>>>> submitted.
> >>>>>>>>> Job 6048.4 exited with exit code 1.
> >>>>>>>>> Job 6048.8 exited with exit code 1.
> >>>>>>>>> Job 6048.9 exited with exit code 1.
> >>>>>>>>> ....<<<
> >>>>>>>>>
> >>>>>>>>> Stuart.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Thu Jan 11 16:23:06 PST 2007
MONITOR FREEZE

OPEN WINDOW:
top -u
OPEN ANOTHER WINDOW:
tail -f /var/log/system.log 
OPEN YET ANOTHER WINDOW:
qs

CLEAR ERROR STATE ON NODES:
qmod -cq all.q@*

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Thu Jan 11 15:53:52 EST 2007
To : 	Chris Dwan via RT <inquiry-support@bioteam.net>

Hi Chris,

I think the head node froze again, although after about an hour this time. This is the last output I got as I was monitoring it with 'qs' which does 'qstat -f' every 10 seconds:

>>>...
   6083 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        
   6084 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        
   6085 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        
   6086 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        
   6087 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        
   6088 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        
   6089 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        
   6090 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        
   6091 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        
   6092 0.55500 iprscan-20 www          qw    01/11/2007 13:19:36     1        

Thu Jan 11 15:30:25 PST 2007
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
unable to contact qmaster using port 701 on host "gems.rsmas.miami.edu"

Thu Jan 11 15:32:58 PST 2007
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
Read from remote host 129.171.101.102: Connection reset by peer
Connection to 129.171.101.102 closed.
<<<

Stuart.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Thursday, January 11, 2007 7:13 PM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] problem with SGE array job - froze on array job
	

"qconf -msconf" will open an editing session.  This is the line of  
interest:

schedule_interval                 0:0:15

System isn't dead yet.  I'm not sure what, exactly, we did.  Maybe  
the stars are in better alignment now.

-Chris Dwan
  The BioTeam


sudo qconf -msconf
(vi editing mode)
>>>
algorithm                         default
schedule_interval                 0:0:15
maxujobs                          0
queue_sort_method                 load
job_load_adjustments              np_load_avg=0.50
load_adjustment_decay_time        0:7:30
load_formula                      np_load_avg
schedd_job_info                   true
flush_submit_sec                  0
flush_finish_sec                  0
params                            none
reprioritize_interval             0:0:0
halftime                          168
usage_weight_list                 cpu=1.000000,mem=0.000000,io=0.000000
compensation_factor               5.000000
weight_user                       0.250000
weight_project                    0.250000
weight_department                 0.250000
weight_job                        0.250000
weight_tickets_functional         0
weight_tickets_share              0
share_override_tickets            TRUE
share_functional_shares           TRUE
max_functional_jobs_to_schedule   400
report_pjob_tickets               TRUE
max_pending_tasks_per_job         50
halflife_decay_list               none
policy_hierarchy                  OFS
weight_ticket                     0.010000
weight_waiting_time               0.000000
weight_deadline                   3600000.000000
weight_urgency                    0.100000
weight_priority                   1.000000
max_reservation                   0
default_duration                  0:10:0
<<<


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Thu Jan 11 14:17:48 PST 2007
ARRAY JOB IS RUNNING!


Hi Chris,

This is definitely an improvement. It was freezing with a minute or so before, at the most 10 mins after running the array job. I'll be keeping my fingers crossed too..

Btw, how did you reset the scheduler wait time (I couldn't find it as an option of qmod or qalter)?

That's a great tip about the output filehandles - I'll incorporate that into the scripts.

Cheers,

Stuart.



>From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
>Reply-To: inquiry-support@bioteam.net
>To: youngstuart@hotmail.com
>Subject: Re: [bioteam.net #13817] problem with SGE array job - froze on array job
>Date: Thu, 11 Jan 2007 13:52:24 -0500
>
>
>Stuart,
>
>I'm watching your jobs run, and things seem to not be dead yet.  My
>impression is that this is an improvement.  How long was it taking
>before locking up, previously?
>
>I made one other small change as I watched jobs pile up in the
>queue:  I increased the scheduler wait time from 4 seconds to 15
>seconds.  This is the interval at which SGE's "schedd" comes back
>around for another pass at re-ordering the queue and scheduling
>jobs.  For clusters in interactive use (by web users, mostly), we set
>this to 4 seconds to reduce the "why is nothing happening yet?"
>wait.  In this case, I increased it to be sure that even if the
>system is heavily loaded, it will have time to complete a scheduling
>loop before starting the next one.  I've seen the scheduler crash
>when that time is set too short, though not in a long time.
>
>Just had a little scare where the machine didn't respond for about 30
>seconds.  Then things piled right back out.  This happened at the
>same time that a job finished, and I'm suspecting that it may have
>been writing a bunch of results to the disk.
>
>I notice while looking at the script you are running (/Volumes/
>gemshd4/Users/local/FUNNYBASE/bin/unigene/orthologuesiprscan.pl) that
>all the outputs are writing directly back to the shared directory.  I
>don't know if you created that script, or someone else, but it's
>almost always a good idea to write output to a local space on the
>node (/tmp) and then copy it back when the job finishes, rather than
>maintaining a whole bunch of open write filehandles over NFS for the
>duration of a run.    This might also cause the nodes to spend more
>time working and less time in I/O wait.
>
>For now, I'll leave my trace window open and watch the jobs progress.
>
>-Chris Dwan
>   The Bioteam
>
>On Jan 11, 2007, at 1:06 PM, stuart young via RT wrote:
>
> >
> > <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >
> > Hi Chris,
> >
> > I found the same kind of problem - I couldn't remotely access the
> > head node
> > though I got the ssh password prompt for users 'local', 'vanwye' and
> > 'local2'. When I went to reboot the head node manually and login on
> > the
> > console, I saw the message that it couldn't find the '/Users/
> > local/' folder
> > and the RAID drive had disappeared. After I power on/offed the RAID
> > drive,
> > reconnected it and logged in on the head node local console as user
> > 'local',
> > I found it was back to normal.
> >
> > I could also ssh from the head node to my remote workstation and
> > login by
> > ssh from my remote workstation on the head node as users 'vanwye',
> > 'local'
> > and 'local2' (btw, I changed the password to the same for user
> > 'local').
> >
> > As far as I can tell, there's no special network setup for the head
> > node
> > compared to my workstation. I also checked up with the guys who run
> > the
> > machine room and they haven't done anything with their network
> > structure
> > lately. So hopefully the RAID drive was the source of the problem.
> > I'll run
> > the array job 'killer app' to see if rebooting the RAID drive has
> > solved
> > that problem too.
> >
> > Cheers,
> >
> > Stuart.
> >
> >
> >
> >
> >
> >> From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
> >> Reply-To: inquiry-support@bioteam.net
> >> To: youngstuart@hotmail.com
> >> Subject: Re: [bioteam.net #13817] problem with SGE array job -
> >> froze on
> >> array job
> >> Date: Thu, 11 Jan 2007 08:49:12 -0500
> >>
> >>
> >> Stuart,
> >>
> >> Here's an interesting fact about the system freezes:
> >>
> >> This morning, I couldn't even get a "Password:" prompt when I tried
> >> to ssh in as the user "vanwye", but when I tried either "root" or
> >> "local2", the password prompt came right up.  Unfortunately, the
> >> password you provided for that account doesn't work for me.  I
> >> probably transcribed it incorrectly.
> >>
> >> This might indicate that some network service is being overwhelmed,
> >> since network resolution and user authentication go hand in hand.
> >> This also lines up with my file corruption issues when I was
> >> downloading software updates directly to the portal.
> >>
> >> Could you please describe the network setup by which I'm accessing
> >> the cluster?  How is "genomics" wired differently than "gems?"
> >>
> >> -Chris Dwan
> >>   The BioTeam
> >>
> >> On Jan 10, 2007, at 6:46 PM, stuart young via RT wrote:
> >>
> >>>
> >>> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >>>
> >>> Hi Chris,
> >>>
> >>> I just tried to run an array job using BLAST, something which I've
> >>> hitherto
> >>> been able to do without any problems but now I get this:
> >>>
> >>>
> >>> ../funnybasearrayblast.pl -d funnybase9 -t refseq-dog
> >>>
> >>>>>>
> >>> Using default conf file: conf/default.conf
> >>>
> >>> Doing BLAST against these databases: refseq-dog
> >>>
> >>> ....
> >>>
> >>> Doing ARRAYBLAST of version 0.01 of database 'funnybase9' (total
> >>> accessions
> >>> 0) against refseq-dog (E-value cutoff: 1e-05)
> >>> qsub command: qsub -sync y -t 1-32
> >>> /Users/local/FUNNYBASE/pipeline/funnybase9/blast/funnybase9-refseq-
> >>> dog.sh
> >>> Completed qsub. Job output:
> >>>
> >>> Your job 6048.1-32:1 ("funnybase9-refseq-dog.sh") has been
> >>> submitted.
> >>> Job 6048.4 exited with exit code 1.
> >>> Job 6048.8 exited with exit code 1.
> >>> Job 6048.9 exited with exit code 1.
> >>> ....<<<
> >>>
> >>> Stuart.
> >>>
> >>> _________________________________________________________________
> >>> Express yourself instantly with MSN Messenger! Download today it's
> >>> FREE!
> >>> http://messenger.msn.click-url.com/go/onm00200471ave/direct/01/
> >>>
> >>
> >>
> >
> > _________________________________________________________________
> > Express yourself instantly with MSN Messenger! Download today it's
> > FREE!
> > http://messenger.msn.click-url.com/go/onm00200471ave/direct/01/
> >
>
>

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Jan 10 18:50:16 PST 2007
QSTAT JOB STATUS RESULTS

qstat -f

one can obtain a job status listing on the running, queued jobs.

If your job status

the status of the job -

         d(eletion), t(ransfering), r(unning),
         R(estarted),  s(uspended),  S(uspended),  T(hreshold),  w(aiting) or
         h(old).



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Jan 10 18:50:16 PST 2007
Reply-To : 	inquiry-support@bioteam.net


Hi Chris,

I just tried to run an array job using BLAST, something which I've hitherto been able to do without any problems but now I get this:


./funnybasearrayblast.pl -d funnybase9 -t refseq-dog

>>>
Using default conf file: conf/default.conf

Doing BLAST against these databases: refseq-dog

...

Doing ARRAYBLAST of version 0.01 of database 'funnybase9' (total accessions 0) against refseq-dog (E-value cutoff: 1e-05)
qsub command: qsub -sync y -t 1-32 /Users/local/FUNNYBASE/pipeline/funnybase9/blast/funnybase9-refseq-dog.sh
Completed qsub. Job output:

Your job 6048.1-32:1 ("funnybase9-refseq-dog.sh") has been submitted.
Job 6048.4 exited with exit code 1.
Job 6048.8 exited with exit code 1.
Job 6048.9 exited with exit code 1.
...<<<

Stuart.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Jan 10 18:45:30 PST 2007
BOOT IN FIREWIRE MODE

Which boot modes correspond to which lights on the front panel of an xserve g5?

XServe Front Panel

On the front of an XServe G5 are two rows of blue LEDs, as well as two buttons. The buttons and the LEDs can serve as a primitive interface to the system, to instruct it to boot in various modes. This page describes how to interact with that interface.
Starting the configuration
Begin with the system completely powered off. You can always power down the system by pressing and holding the power button for 10 to 15 seconds. When the system has powered down, release the button

    * Press and hold the system alert button
    * Press and release the power button (keep holding the alert button). The system will begin to boot, and various lights will flash. In short order (approximately 45 seconds), the top row of LEDs will begin to process back and forth.
    * Release the alert button, once the top row of LEDs has begun to process back and forth.
    * Press and release the alert button to advance the light on the bottom row of LEDs. It should start all the way to the right. The positions are described below:

          o Light 1 (far right): Start up from the optical drive. Also ejects the optical drive.
          o Light 2: Netboot
          o Light 3: Start from an internal drive (leftmost, if there are more than one)
          o Light 4: Bypass the current startup disk in favor of something else
          o Light 5: begin target disk mode (become a firewire disk)
          o Light 6: Restore default firmware settings
          o Light 7: Enter open firmware.

    * Once you have selected the mode that you want, press and hold the alert button. The top row of LEDs will fill from right to left. Once all the lights are on, release the alert button.
    * The system will now reboot in whatever mode you selected.

Last update: 2005-12-11 15:08
Author: Chris Dwan
Revision: 1.0

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wednesday, January 10, 2007 1:10 PM
Reply-To : 	inquiry-support@bioteam.net

Hi Chris,

I froze again, as the number of pending jobs was increasing beyond about 50 jobs. I'll go reboot it in 20 mins time.

Btw, in reply to your previous question, the cluster is located in our machine room which has AC and controlled & backup power. I've run full loads before (each CPU occuped with a job) with no problems. One difference is that I've never had so many pending jobs as when I run the array jobs with 'orthologuesiprscan.pl'.


Cheers,

Stuart

From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Wednesday, January 10, 2007 5:15 PM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] problem with SGE array job - froze on array job


Stuart,

I just looked all the way back at the title of this ticket, and it  
occurred to me that there is one additional complexity that we  
haven't addressed:

"Install submit host + interproscan"

Are you submitting these jobs from your workstation, or from the  
command line on the portal?  If it's the workstation, let's try  
directly from the portal and see if we can isolate things a bit more.

-Chris Dwan
  The BioTeam 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wednesday, January 10, 2007 1:00 PM
Reply-To : 	inquiry-support@bioteam.net

Hi Chris,

Thanks for repairing the permissions - that looked very messed up. I've been submitting the jobs on the portal to avoid the complications of submitting from my workstation. I'll try with the array job 'killer app' now to see if it'll freeze again.

Stuart.

From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Wednesday, January 10, 2007 4:30 PM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] problem with SGE array job - froze on array job
	
Stuart,

Wow.

I repaired permissions on the boot disk with "disk util".  A copy- 
paste of what was changed is below.

Then I downloaded the "10.4.8 Combo update" disk image to genomics  
and copied it over to gems.  When I downloaded it directly to gems,  
the image showed up corrupted.

Anyway, I got the update to install.  Please try your system-killer  
program again.

-Chris Dwan
  The BioTeam

gems:/Volumes/Mac OS X Server 10.4.8 Update 1 root# diskutil  
repairPermissions /
Started verify/repair permissions on disk disk0s3 gemshd1
Determining correct file permissions.
Permissions differ on ./Applications/Server/AppleShare IP  
Migration.app/Contents/MacOS/AppleShare IP Migration, should be - 
rwsrwsr-x , they are -rwxrwxr-x
Owner and group corrected on ./Applications/Server/AppleShare IP  
Migration.app/Contents/MacOS/AppleShare IP Migration
Permissions corrected on ./Applications/Server/AppleShare IP  
Migration.app/Contents/MacOS/AppleShare IP Migration
Permissions differ on ./Applications/Utilities/NetInfo Manager.app/ 
Contents/MacOS/NetInfo Manager, should be -rwxrwxr-x , they are - 
rwsrwxr-x
Owner and group corrected on ./Applications/Utilities/NetInfo  
Manager.app/Contents/MacOS/NetInfo Manager
Permissions corrected on ./Applications/Utilities/NetInfo Manager.app/ 
Contents/MacOS/NetInfo Manager
Permissions differ on ./Library/Application Support/Macromedia/ 
Shockwave 10/Shockwave.bundle/Contents/PkgInfo, should be -rwxrwxr- 
x , they are -rw-rw-r--
Owner and group corrected on ./Library/Application Support/Macromedia/ 
Shockwave 10/Shockwave.bundle/Contents/PkgInfo
Permissions corrected on ./Library/Application Support/Macromedia/ 
Shockwave 10/Shockwave.bundle/Contents/PkgInfo
Group differs on ./Library/WebServer/Documents/index.html, should be  
80, group is 0
Permissions differ on ./Library/WebServer/Documents/index.html,  
should be -rw-rw-r-- , they are -rwxr-xr-x
Owner and group corrected on ./Library/WebServer/Documents/index.html
Permissions corrected on ./Library/WebServer/Documents/index.html
Permissions differ on ./System/Library/Perl/5.8.6/CGI/Carp.pm, should  
be -rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./System/Library/Perl/5.8.6/CGI/Carp.pm
Permissions corrected on ./System/Library/Perl/5.8.6/CGI/Carp.pm
Permissions differ on ./System/Library/Perl/5.8.6/CGI/Cookie.pm,  
should be -rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./System/Library/Perl/5.8.6/CGI/Cookie.pm
Permissions corrected on ./System/Library/Perl/5.8.6/CGI/Cookie.pm
Permissions differ on ./System/Library/Perl/5.8.6/CGI/Pretty.pm,  
should be -rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./System/Library/Perl/5.8.6/CGI/Pretty.pm
Permissions corrected on ./System/Library/Perl/5.8.6/CGI/Pretty.pm
Permissions differ on ./System/Library/Perl/5.8.6/CGI/Util.pm, should  
be -rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./System/Library/Perl/5.8.6/CGI/Util.pm
Permissions corrected on ./System/Library/Perl/5.8.6/CGI/Util.pm
Permissions differ on ./System/Library/Perl/5.8.6/CGI.pm, should be - 
rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./System/Library/Perl/5.8.6/CGI.pm
Permissions corrected on ./System/Library/Perl/5.8.6/CGI.pm
Permissions differ on ./System/Library/Perl/5.8.6/darwin-thread- 
multi-2level/auto/List/Util/Util.bundle, should be -rwxr-xr-x , they  
are -r-xr-xr-x
Owner and group corrected on ./System/Library/Perl/5.8.6/darwin- 
thread-multi-2level/auto/List/Util/Util.bundle
Permissions corrected on ./System/Library/Perl/5.8.6/darwin-thread- 
multi-2level/auto/List/Util/Util.bundle
User differs on ./Users, should be 0, owner is 508
Group differs on ./Users, should be 80, group is -2
Owner and group corrected on ./Users
Permissions corrected on ./Users
Permissions differ on ./private/var/log/secure.log, should be - 
rw------- , they are -rw-r-----
Owner and group corrected on ./private/var/log/secure.log
Permissions corrected on ./private/var/log/secure.log
Symbolic link ./usr/lib/gcc/darwin/default->3.3 repaired
Symbolic link ./usr/lib/libcc_dynamic.a->gcc/darwin/default/libgcc.a  
repaired
Symbolic link ./usr/lib/libcc_kext.a->gcc/darwin/default/libcc_kext.a  
repaired
Symbolic link ./usr/lib/libgcc.a->gcc/darwin/default/libgcc.a repaired
Symbolic link ./usr/lib/libstdc++.a->gcc/darwin/default/libstdc++.a  
repaired
Symbolic link ./usr/lib/libsupc++.a->gcc/darwin/default/libsupc++.a  
repaired
Permissions differ on ./usr/share/man/man3/CGI.3pm, should be -rw-r-- 
r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/CGI.3pm
Permissions corrected on ./usr/share/man/man3/CGI.3pm
Permissions differ on ./usr/share/man/man3/CGI::Apache.3pm, should be  
-rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/CGI::Apache.3pm
Permissions corrected on ./usr/share/man/man3/CGI::Apache.3pm
Permissions differ on ./usr/share/man/man3/CGI::Carp.3pm, should be - 
rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/CGI::Carp.3pm
Permissions corrected on ./usr/share/man/man3/CGI::Carp.3pm
Permissions differ on ./usr/share/man/man3/CGI::Cookie.3pm, should be  
-rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/CGI::Cookie.3pm
Permissions corrected on ./usr/share/man/man3/CGI::Cookie.3pm
Permissions differ on ./usr/share/man/man3/CGI::Fast.3pm, should be - 
rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/CGI::Fast.3pm
Permissions corrected on ./usr/share/man/man3/CGI::Fast.3pm
Permissions differ on ./usr/share/man/man3/CGI::Pretty.3pm, should be  
-rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/CGI::Pretty.3pm
Permissions corrected on ./usr/share/man/man3/CGI::Pretty.3pm
Permissions differ on ./usr/share/man/man3/CGI::Push.3pm, should be - 
rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/CGI::Push.3pm
Permissions corrected on ./usr/share/man/man3/CGI::Push.3pm
Permissions differ on ./usr/share/man/man3/CGI::Switch.3pm, should be  
-rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/CGI::Switch.3pm
Permissions corrected on ./usr/share/man/man3/CGI::Switch.3pm
Permissions differ on ./usr/share/man/man3/CGI::Util.3pm, should be - 
rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/CGI::Util.3pm
Permissions corrected on ./usr/share/man/man3/CGI::Util.3pm
Permissions differ on ./usr/share/man/man3/List::Util.3pm, should be - 
rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/List::Util.3pm
Permissions corrected on ./usr/share/man/man3/List::Util.3pm
Permissions differ on ./usr/share/man/man3/Scalar::Util.3pm, should  
be -rw-r--r-- , they are -r--r--r--
Owner and group corrected on ./usr/share/man/man3/Scalar::Util.3pm
Permissions corrected on ./usr/share/man/man3/Scalar::Util.3pm
The privileges have been verified or repaired on the selected volume
Verify/repair finished permissions on disk disk0s3 gemshd1


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Wednesday, January 10, 2007 3:57 PM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] problem with SGE array job - froze on array job
	
Stuart,

I remember that we had some trouble getting mail going from the nodes  
to your satisfaction.  I don't recall what solution we wound up with.

My suggestion is to set up a couple of terminals and watch the portal  
die:  Run "top -u" in one, "tail -f /var/log/system.log" in another.   
Then start your killer job.  The blue lights seem to indicate a high  
process load.

One other possibility that comes to mind is that there could be some  
sort of environmental thing going on.  Have you ever gotten the  
system really loaded with work before?  These machines draw  
substantially more power (and generate a lot more heat) when they're  
working than when they are idling.  What sort of physical environment  
have you got the nodes in?  Are you sure that power and cooling are  
adequate to the task?

I tried to run software updates on the portal, but I'm getting this  
error:

Package failed: Error SUSessionErrorDomain 3

I tried downloading the updater for 10.4.8 to try to do the  
installation by hand, but I was refused:

gems:/Volumes/Mac OS X Server 10.4.8 Update 1 root# installer -pkg  
MacOSXServerUpd10.4.8PPC.mpkg -target /
installer: Package name is Mac OS X Server Update (PowerPC)
installer: Upgrading volume mounted at /.
2007-01-10 10:58:43.788 installer[1465]  
IFPkg::_parseOldStyleForLanguage - can't find .info file (QTSSPublisher)
installer: The upgrade failed.

I'll continue looking for the root cause of this.  I'm becoming  
convinced that there is some underlying pathology here.

-Chris Dwan
  The BioTeam


On Jan 9, 2007, at 10:40 PM, stuart young via RT wrote:

>
> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
>
> Hi Chris,
>
> It seemed to go okay when I ran 'orthologuesengineiprscan2.pl' but  
> it froze
> again when I ran the array job using 'orthologuesiprscan.pl'. I  
> wasn't even
> able to ping the head node this time.
>
> Btw, when I went to manually reboot I found the head node's bottom  
> row of
> blue lights were all lit and the first two or three of the top row  
> of blue
> lights would periodically come on. I guess this means it was  
> thrashing?
>
> In '/var/log/system.log' I found:
>
> Jan  9 21:54:31 gems postfix/master[54]: warning: process
> /usr/libexec/postfix/trivial-rewrite pid 29331 exit status 1
> Jan  9 21:54:31 gems postfix/master[54]: warning:
> /usr/libexec/postfix/trivial-rewrite: bad command startup --  
> throttling
> Jan  9 21:55:29 gems lookupd[150]: NetInfo connection failed for  
> server
> 127.0.0.1/local
> Jan  9 21:55:55 gems DirectoryService[63]: NetInfo connection  
> failed for
> server 127.0.0.1/local
> Jan  9 21:57:00 gems ctl_cyrusdb[29336]: checkpointing cyrus databases
> Jan  9 21:57:00 gems ctl_cyrusdb[29336]: done checkpointing cyrus  
> databases
> Jan  9 21:57:14 gems lookupd[150]: NetInfo connection failed for  
> server
> 127.0.0.1/local
> Jan  9 21:57:14 gems lookupd[150]: ni_statistics: Communication  
> failure
> Jan  9 21:58:25 gems DirectoryService[63]: NetInfo connection  
> failed for
> server 127.0.0.1/local
> Jan  9 21:58:58 gems lookupd[150]: NetInfo connection failed for  
> server
> 127.0.0.1/local
> Jan  9 21:59:30 gems postfix/qmgr[29333]: fatal: file /etc/postfix/ 
> main.cf:
> parameter default_privs: unknown user name value: nobo\
> dy
>
>
> And also:
>
> Jan  9 22:17:01 gems postfix/master[54]: warning: process
> /usr/libexec/postfix/pickup pid 29340 exit status 1
> Jan  9 22:17:01 gems postfix/master[54]: warning:
> /usr/libexec/postfix/pickup: bad command startup -- throttling
> Jan  9 22:18:08 gems lookupd[150]: NetInfo connection failed for  
> server
> 127.0.0.1/local
> Jan  9 22:18:08 gems lookupd[150]: ni_statistics: Communication  
> failure
> Jan  9 22:18:25 gems DirectoryService[63]: NetInfo connection  
> failed for
> server 127.0.0.1/local
> Jan  9 22:19:52 gems lookupd[150]: NetInfo connection failed for  
> server
> 127.0.0.1/local
> Jan  9 22:20:55 gems DirectoryService[63]: NetInfo connection  
> failed for
> server 127.0.0.1/local
> Jan  9 22:22:21 localhost kernel[0]: standard timeslicing quantum  
> is 10000
> us
> Jan  9 22:22:21 localhost named[52]: starting BIND 9.2.2 -f
>
> Could the 'throttling' of /usr/libexec/postfix/pickup be causing  
> the problem
> because of the increased number of jobs? I'd noticed recently a  
> huge number
> of '|E|mailer had timeout - killing' messages in the spool/node*/ 
> messages
> file on all nodes. Here's the tail end of the messages file for  
> node001:
>
>
> 01/09/2007 08:47:25|execd|node001|I|starting up 6.0u6
> 01/09/2007 08:49:31|execd|node001|I|controlled shutdown 6.0u6
> 01/09/2007 08:50:16|execd|node001|I|starting up 6.0u6
> 01/09/2007 10:46:22|execd|node001|W|reaping job "5735" ptf  
> complains: Job
> does not exist
> 01/09/2007 10:46:58|execd|node001|W|reaping job "5744" ptf  
> complains: Job
> does not exist
> 01/09/2007 10:47:10|execd|node001|W|reaping job "5747" ptf  
> complains: Job
> does not exist
> 01/09/2007 10:47:22|execd|node001|W|reaping job "5751" ptf  
> complains: Job
> does not exist
> 01/09/2007 10:47:34|execd|node001|W|reaping job "5755" ptf  
> complains: Job
> does not exist
> 01/09/2007 19:37:55|execd|node001|E|unable to read qmaster name:  
> can't open
> "/common/sge/default/common/act_qmaster" for reading qmaster hostname
> 01/09/2007 19:38:06|execd|node001|E|shepherd of job 5862.1 exited  
> with exit
> status = 11
> 01/09/2007 19:38:06|execd|node001|E|abnormal termination of  
> shepherd for job
> 5862.1: "exit_status" file is empty
> 01/09/2007 19:38:06|execd|node001|E|shepherd of job 5856.9 exited  
> with exit
> status = 11
> 01/09/2007 19:38:06|execd|node001|E|abnormal termination of  
> shepherd for job
> 5856.9: "exit_status" file is empty
> 01/09/2007 19:39:06|execd|node001|E|mailer had timeout - killing
> 01/09/2007 19:39:06|execd|node001|E|mailer exited with exit status = 1
> 01/09/2007 19:39:06|execd|node001|E|mailer had timeout - killing
> 01/09/2007 19:39:06|execd|node001|E|mailer exited with exit status = 1
> 01/09/2007 19:39:06|execd|node001|E|mailer had timeout - killing
> 01/09/2007 19:39:06|execd|node001|E|mailer exited with exit status = 1
> 01/09/2007 19:42:03|execd|node001|E|commlib error: endpoint is not  
> unique
> error (endpoint "node001.cluster.private/execd/1" is already  
> connected)
> 01/09/2007 19:42:03|execd|node001|E|getting configuration: failed  
> sending
> gdi request
> 01/09/2007 19:42:04|execd|node001|E|there is already a client endpoint
> node001.cluster.private/execd/1 connected to qmaster service
> 01/09/2007 19:42:09|execd|node001|I|starting up 6.0u6
> 01/09/2007 19:43:33|execd|node001|I|controlled shutdown 6.0u6
> 01/09/2007 19:43:42|execd|node001|I|starting up 6.0u6
> 01/09/2007 19:44:21|execd|node001|I|controlled shutdown 6.0u6
> 01/09/2007 19:45:07|execd|node001|I|starting up 6.0u6
> 01/09/2007 19:46:18|execd|node001|I|controlled shutdown 6.0u6
> 01/09/2007 19:46:19|execd|node001|I|starting up 6.0u6
> 01/09/2007
> 19:48:07|execd|node001|E|fopen("/common/sge/default/common/ 
> act_qmaster")
> failed: Input/output error
> 01/09/2007 19:48:07|execd|node001|E|unable to read qmaster name:  
> can't open
> "/common/sge/default/common/act_qmaster" for reading qmaster hostname
> 01/09/2007 19:48:07|execd|node001|E|commlib error: can't connect to  
> service
> (Connection refused)
>
>
> Any ideas?
>
> Cheers,
>
> Stuart.
>
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

From : 	Chris Dwan via RT <inquiry-support@bioteam.net>
Reply-To : 	inquiry-support@bioteam.net
Sent : 	Wednesday, January 10, 2007 3:56 PM
To : 	youngstuart@hotmail.com
Subject : 	Re: [bioteam.net #13817] problem with SGE array job

I see what you see:

all.q@node001.cluster.private  BIP   0/2       0.01     darwin        E

I asked SGE to explain itself:

node001:~ root# qstat -explain E
queuename                      qtype used/tot. load_avg arch           
states
------------------------------------------------------------------------ 
----
all.q@node001.cluster.private  BIP   0/2       0.03     darwin        E
         queue all.q marked QERROR as result of job 5856's failure at  
host node001.cluster.private
         queue all.q marked QERROR as result of job 5862's failure at  
host node001.cluster.private

That indicates that job 5856 failed in some spectacular way, and so  
SGE marked the queue with an error state to prevent other jobs from  
going there until an operator clears them by hand.

SGE is still running on the nodes, just in an error state:

gems:/Volumes/Mac OS X Server 10.4.8 Update 1 root# dsh -a ps -auxc  
\| grep sge
executing 'ps -auxc | grep sge'
node001.cluster.private:        sge       2535   0.0  0.1    28504     
816  ??  S     7:26AM   0:00.32 sge_execd
node002.cluster.private:        sge        518   0.0  0.1    28504     
948  ??  S     7:47PM   0:04.85 sge_execd
node004.cluster.private:        sge        518   0.0  0.1    28504     
948  ??  S     7:47PM   0:04.86 sge_execd
node005.cluster.private:        sge        517   0.0  0.1    28504     
948  ??  S     7:47PM   0:04.88 sge_execd
node006.cluster.private:        sge        517   0.0  0.1    28504     
948  ??  S     7:48PM   0:04.90 sge_execd
node007.cluster.private:        sge        509   0.0  0.1    28504     
948  ??  S     7:47PM   0:04.88 sge_execd
node008.cluster.private:        sge        518   0.0  0.1    28504     
948  ??  S     7:47PM   0:04.84 sge_execd
node009.cluster.private:        sge        520   0.0  0.1    28504     
948  ??  S     7:46PM   0:04.90 sge_execd
node010.cluster.private:        sge        517   0.0  0.1    28504     
948  ??  S     3:02AM   0:04.93 sge_execd
node011.cluster.private:        sge        520   0.0  0.1    28504     
948  ??  S    10:47PM   0:04.87 sge_execd
node013.cluster.private:        sge        520   0.0  0.1    28504     
948  ??  S    10:48PM   0:04.87 sge_execd
node014.cluster.private:        sge        520   0.0  0.1    28504     
948  ??  S     7:46PM   0:04.90 sge_execd
node015.cluster.private:        sge        520   0.0  0.1    28504     
948  ??  S    10:46PM   0:04.86 sge_execd

You would see a state of "au" if it was stopped.

To clear the error, I did this:

qmod -cq all.q@*

-Chris Dwan
  The Bioteam

On Jan 10, 2007, at 10:14 AM, stuart young via RT wrote:

>
> <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
>
> Hi Chris,
>
> After restarting yesterday, the queue is showing errors on all  
> active nodes,
> e.g.:
>
> all.q@node001.cluster.private  BIP   0/2       0.00      
> darwin        E
>
> When I try to restart SGE on the nodes I get (sudo SystemStarter -v  
> Start
> SGE):
>
> Checking disks
> ipfilter:state = "STOPPED"
> ipfilter:status = 0
>
> Cheers,
>
> Stuart.
>
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Jan  9 16:38:32 PST 2007
TRYING RUN OF IPRSCAN ARRAY JOB WITH orthologuesiprscan.pl

./orthologuesiprscan.pl -d orthologues -t 20 -p hmmpanther -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out





+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Jan  9 16:37:56 PST 2007
NO FREEZE BUT PROBLEM WITH JOB ON node006

gems:~ local$ qstat -f
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       0.21     darwin        d
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.10     darwin        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   1/2       0.02     darwin        
   5547 0.55500 iprscan-20 www          r     01/09/2007 12:56:25     1        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   0/2       -NA-     darwin        adu
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   0/2       0.03     darwin        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.00     darwin        d


ON node006:
node006:~ vanwye$ ps aux | grep ipr
www       3899   0.1  0.8    91488   8600  ??  S    10:13AM   2:03.51 /usr/bin/perl /common/iprscan/bin/iprscan_wrapper.pl
www       3855   0.0  0.1    89176    800  ??  Ss   10:13AM   0:00.07 -csh -c /common/iprscan/tmp/20070109/iprscan-20070109-12562155/iprscan-20070109-12562155.dcmd 
www       3898   0.0  0.0    84604    452  ??  S    10:13AM   0:00.01 /usr/bin/perl /common/iprscan/tmp/20070109/iprscan-20070109-12562155/iprscan-20070109-12562155.dcmd
vanwye   25377   0.0  0.0    27372    428  p0  S+    4:37PM   0:00.00 grep ipr
node006:~ vanwye$ 

em /common/iprscan/tmp/20070109/iprscan-20070109-12562155/*seq

>XM_347220|S16956187|unigene_rat
TTTCGGTGAGAACACCGAGTGACGATCTGTTGCTTCCCCTGAGGTGGCTACaaagaaagg
aagccgaggagggaggggagggggaaaaaaggaaagggaggggggttaaaaaaaaaaaGC
CGGGACTAGCTCGCGGCTTGTCAATTTCAACATCGGGTCACATGACCAGCACCTCCCTGC


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Jan  9 12:23:50 PST 2007
MSG: inquiry-support@bioteam.net

Hi Chris,

Thanks for doing all that work. That sounds like a good strategy.

I'll test the system now with a couple of runs of 'orthologuesengineiprscan2.pl' and then the array job version 'orthologuesiprscan.pl' and let you know how it goes.

Once everything's shipshape I'll clone over nodes 012 and 016 - thanks for the reminder.

Cheers,

Stuart.


>From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
>Reply-To: inquiry-support@bioteam.net
>To: youngstuart@hotmail.com
>Subject: Re: [bioteam.net #13817] problem with SGE array job
>Date: Tue, 9 Jan 2007 11:50:01 -0500
>
>
>Stuart,
>
>I've converted your cluster to "static" NFS mounts.  If the crashes
>were being caused by heavy load on the NFS server, this may help.  If
>they were caused by something else, then it won't.  In either event,
>it's a decent idea for a mid-sized cluster.
>
>First, I modified your NFS exports to export the root volume of your
>data disk, rather than three subdirectories.  I did this so that we
>could have *one* mount point rather than *three* on each client:
>
>        "name" = ( "/Volumes/gemshd4" );
>        "clients" = ( "129.171.101.233 node001.cluster.private
>node002.cluster.private  node00
>4.cluster.private  node005.cluster.private  node006.cluster.private
>node007.cluster.private
>    node008.cluster.private  node009.cluster.private
>node010.cluster.private  node011.cluster
>.private  node012.cluster.private  node013.cluster.private
>node014.cluster.private  node015
>.cluster.private  node016.cluster.private" );
>        "opts" = ( "maproot=root", "alldirs" );
>
>The "alldirs" option allows clients to mount subdirectories rather
>than the root level, so you shouldn't need to change anything on your
>workstation.
>
>Then I installed a new startup item on each of the nodes,
>"ClusterServices."  Instead of going through the automounter (/etc/
>bipod/auto.common) and all that, I directly mount the data volume
>under a new directory called /nfs.  /common and so on are links to that.
>
>On the portal, none of this matters.  On the nodes, they are doing a
>regular, standard NFS mount of a single point, rather than an autoFS
>mount of three.
>
>NOTE:  I did not make any changes to nodes 12 or 16.  My suggestion
>on those is to use disk utility or Carbon Copy Cloner to copy the
>entire boot disk of a working node (anything except 12 or 16) and re-
>image the disk of those two machines.  They are different in some
>way, and it's generally a waste of time to debug such things by hand
>when one can simply re-image and be done with it.
>
>-Chris Dwan
>   The BioTeam
>


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Jan  8 19:01:14 PST 2007
MSG: inquiry-support@bioteam.net

Hi Chris,

I tried adding the user with nicl because it didn't work when I used Workgroup Manager: I wasn't able to log on via ssh or locally as the user I created with Workgroup Manager. 

I guess nicl and Workgroup Manager don't play nice together? Do you know if there's a way to do it with Workgroup Manager or some other means?

I hope this is related to and has perhaps solved the freeze problem. The way I got it to freeze was to run one or two jobs using orthologuesengineiprscan2.pl:

su
su - www
uni    #cd to unigene directory
/Users/local/FUNNYBASE/bin/unigene/orthologuesengineiprscan2.pl 17

Btw, I got your three recent messages ok.

Cheers,

Stuart.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Jan  8 17:48:34 PST 2007
MSG: inquiry-support@bioteam.net

Hi there, 

I'm hoping for your timely help on this urgent problem that I've been having since Friday and which is holding back my main production activity. I waited patiently for a week for someone to respond to my message the last time I contacted bioteam. But this time I really cannot afford to wait that long.

Here's some additional information:

A couple of nodes (008 and 009) are showing this error when I login:

node008

Password:
Last login: Tue Jan  2 12:52:41 2007 from node002.cluster
Welcome to Darwin!
Could not chdir to home directory /Users/vanwye: Host is down
-bash: /common/sge/default/common/settings.sh: RPC prog. not avail
-bash: /Users/vanwye/.bash_profile: Host is down

node009

Last login: Mon Dec 18 14:24:58 2006 from node008.cluster
Welcome to Darwin!
Could not chdir to home directory /Users/vanwye: Host is down
-bash: /common/sge/default/common/settings.sh: RPC prog. not avail
-bash: /Users/vanwye/.bash_profile: Host is down
node009:/ vanwye$ showmount -e portal2net
Exports list on portal2net:
/opt                                Everyone
/Library/Perl                      192.168.2.0 
/Volumes/gemshd4/common            129.171.101.233 node001.cluster.private node002.cluster.private node004.cluster.private node005.cluster.private node006.cluster.private node007.cluster.private node008.cluster.private node009.cluster.private node010.cluster.private node011.cluster.private node012.cluster.private node013.cluster.private node014.cluster.private node015.cluster.private node016.cluster.private 
node009:/ vanwye$ 

It seems there's a problem with my NFS. Here's the output of nidump -r /exports:

{
  "name" = ( "exports" );
  CHILDREN = (
    {
      "name" = ( "/Volumes/gemshd4/Users" );
      "clients" = ( "node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.private  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private  node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root" );
    },
    {
      "name" = ( "/Volumes/gemshd4/Groups" );
      "clients" = ( "node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.private  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private  node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root" );
    },
    {
      "name" = ( "/Library/Perl" );
      "clients" = ( );
      "opts" = ( "ro", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Volumes/gemshd4/common" );
      "clients" = ( "129.171.101.233 node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.private  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private  node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root" );
    },
    {
      "name" = ( "/opt" );
      CHILDREN = (
        {
          "clients" = ( "129.171.101.233" );
          "name" = ( "/opt" );
          "opts" = ( "maproot=root" );
        }
      )
    }
  )
}

Chris Dwan has the access information. If he's busy with other customers, I'd appreciate it if someone else could take a look.

Cheers,

Stuart.

Mon Jan  8 17:45:32 PST 2007

++++++++++++++++++++++++++++++++

Hi there,

If Chris Dwan is busy, could someone else take a look at my problem? Chris has been helping me troubleshoot it successfully so far but there's a new issue: I keep having to manually reboot my head node after it freezes up and denies ssh access and local logon, and I'm not anywhere near full capacity on my cluster (please see my previous messages below).

Cheers,

Stuart.


++++++++++++++++++++++++++++


Hi Chris,

In addition to the problem below, I now have repeated freezes of the head node, which can only be fixed by restarting manually. It happens when I run large numbers of individual jobs concurrently.

It may be a problem related to nfs as I also cannot mount the data directory from gems on my local machine:

dlc-genomics:~ young$ sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/Volumes/gemshd4/common /common
mount_nfs: realpath /private/var/autofs/common: Host is down

When I run array jobs, the jobs appear to get 'stuck' (see below for previous problem).

I've left the user/pswds the same so you're free to access the system to take a look.

Cheers,

Stuart.


++++++++++++++++++++++++++++++++++++++++++++++


Hi Chris,

I have a problem with SGE array jobs though I've been able to run all of the iprscan apps using a simple non-array job submitter:

/Users/local/FUNNYBASE/bin/unigene/iprscan2.pl

E.G.:

su
su - www
uni
 ./iprscan2.pl -p hmmpfam -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1

The array job application I've been using is:

/Users/local/FUNNYBASE/bin/unigene/orthologuesiprscan.pl

Here's a summary of the errors:
========================================================

I DISABLED node012 AND node016 (WHICH WERE 'DIFFERENT' FROM THE OTHERS):

qmod -d all.q@node012
qmod -d all.q@node016

AND WAS ABLE TO RUN blastprodom WITH iprscan2.pl.

THEN TRIED RUNNING hmmpanther WITH orthologuesiprscan.pl AGAIN TO SEE IF DISABLING node012 AND node016 SOLVED THE PROBLEM WITH orthologuesiprscan.pl:

    ./orthologuesiprscan.pl -d orthologues -t 2 -p hmmpanther -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out

BUT IT GOT STUCK:

gems:~/FUNNYBASE/bin/unigene local$ em *2351.2

>>>...
No output file present... doing iprscan
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.2.hmmpanther.0 -o /Users/l\
ocal/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.2.hmmpanther.0 -iprlookup -goterms -appl hmmpanther
SUBMITTED iprscan-20070104-22410112
<<<

TRIED TO RUN superfamily WITH orthologuesiprscan.pl BUT IT GOT STUCK:

gems:/Users/local/FUNNYBASE/bin/unigene www$ em *2359.1
>>>...
/common/iprscan/bin/iprscan -cli -seqtype n -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1.superfamily.0 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1.superfamily.0 -iprlookup -goterms -appl superfamily
SUBMITTED iprscan-20070105-07091403
<<<

RAN superfamily WITH iprscan2.pl:

 ./iprscan2.pl -p superfamily -i /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.fasta.1 -o /Users/local/FUNNYBASE/pipeline/orthologues/fasta/orthologues.out.1


SO TRIED RUNNING funnybasearrayblast.pl, WHICH GAVE THIS ERROR:

gems:~/FUNNYBASE/pipeline/funnybase9/blast local$ em funnybase9-refseq-dog.sh.o2395.1

>>>
usage: join [-a fileno | -v fileno ] [-e string] [-1 field] [-2 field]
            [-o list] [-t char] file1 file2
[NULL_Caption] WARNING: 0.1: Unable to open refseq-dog.pin
[NULL_Caption] WARNING: 0.1: Unable to open refseq-dog.pin
[NULL_Caption] FATAL ERROR: 0.1: Database /common/data/refseq-dog was not found or does not exist
<<<

ALTHOUGH I CAN FIND /common/data/refseq ON node014, WHERE THE JOB WAS RUN... 
SO THE ERROR HAS TO DO WITH ARRAY JOBS NOT FINDING THE /common/data DIRECTORY?

========================================================

Any suggestions?

Thanks for all your help so far.

Cheers,

Stuart.







+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

CREATED AUTOMOUNT SCRIPT TO AUTOMATICALLY MOUNT /common AT STARTUP

FIREWALL SCRIPTS Firewall AND StartupParameters.plist:

em /Library/StartupItems/Firewall/Firewall
>>>
#!/bin/sh

##
# Mount /common directory of gems
##

. /etc/rc.common

StartService ()
{
    ConsoleMessage "Mounting /common"
    sh /etc/rc.mount > /dev/null
}

StopService ()
{
  ConsoleMessage "Stopping Firewall"
  /sbin/ipfw -f -q flush
}

RestartService ()
{
    ConsoleMessage "Restarting ipfw firewall"
    StopService
    StartService
}

RunService "$1"
<<<

em /Library/StartupItems/Firewall/StartupParameters.plist
>>>
{
  Description     = "Firewall";
  Provides        = ("Firewall");
  Requires        = ("NetworkExtensions","Resolver");
  OrderPreference = "Late";
  Messages =
  {
    start = "Starting firewall";
    stop  = "Stopping firewall";
  };
}
<<<










++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Dec 27 20:07:12 PST 2006
REMINDER OF ORIGINAL PROBLEM:

12/18/2006 11:21:12|qmaster|gems|W|job 947.1 failed on host node009.cluster.private general assumedly before job because: can't get password entry for user "local". Either the user does not exist or NIS error!
12/18/2006 11:21:12|qmaster|gems|W|rescheduling job 947.1
gems:/common/sge/default/spool local$ tail -100  qmaster/messages

MSG: support@bioteam.net

Hi Chris, 

I fixed the 'E' error state by simply rebooting the head node. Now I'm back to the original error when I try to run an iprscan job:

sudo qmod -cj 947
>>>
root@gems.rsmas.miami.edu cleared error state of job 947
<<<

gems:~ local$ qstat -j 947
>>>
job_number:                 947
exec_file:                  job_scripts/947
submission_time:            Mon Dec 18 11:21:10 2006
owner:                      local
uid:                        501
group:                      local
gid:                        501
sge_o_home:                 /Users/local
sge_o_log_name:             local
sge_o_path:                 /Users/local/apps/cytoscape/PLUGINS:/usr/lib/perl5/site_perl:/usr/share/apps/ActiveTcl/bin:/usr/local/mysql/bin:/usr/local/bin:/usr/share/apps/komodo:/sw/bin:/Users/local/FUNNYBASE/bin:/common/sge/bin/darwin:/bin:/sbin:/usr/bin:/usr/sbin:/common/mpich-1.2.7/ch_p4/bin:/common/bin:/common/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin:/usr/local/biotools/bin:/Applications/bioinf/phylip/exe:/Applications/bioinf/t_coffee:/usr/local/biotools/bin
sge_o_shell:                /bin/bash
sge_o_workdir:              /common/apps/iprscan
sge_o_host:                 gems
account:                    sge
stderr_path_list:           /dev/null
mail_list:                  local@gems.rsmas.miami.edu
notify:                     FALSE
job_name:                   iprscan-20061218-11211098
stdout_path_list:           /dev/null
jobshare:                   0
hard_queue_list:            all.q
env_list:                   
script_file:                /common/apps/iprscan/tmp/20061218/iprscan-20061218-11211098/iprscan-20061218-11211098.dcmd
error reason    1:          can't get password entry for user "local". Either the user does not exist or NIS error!
<<<

Do I still need to create the user 'local' in Workgroup Manager somehow (e.g., delete the 'local' user in NetInfo Manager and create a new 'local' user in Workgroup Manager )? 

One issue with removing the 'local' user is that I can't log in through the GUI using the other users - in fact, the head node freezes when I try and I have to manually restart it.

Is there another (better) way?

Cheers,

Stuart.

++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Dec 27 20:07:12 PST 2006
USED -explain E OPTION OF qstat TO SHOW REASON FOR 'E' ERROR STATE:


 qstat -explain E
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       0.19     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host gems.rsmas.miami.edu
        queue all.q marked QERROR as result of job 991's failure at host gems.rsmas.miami.edu
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.11     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node001.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node001.cluster.private
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.00     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node002.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node002.cluster.private
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   0/2       0.00     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node004.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node004.cluster.private
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.03     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node005.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node005.cluster.private
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   0/2       0.04     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node006.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node006.cluster.private
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.00     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node007.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node007.cluster.private
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.06     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node008.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node008.cluster.private
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.00     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node009.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node009.cluster.private
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   0/2       0.01     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node010.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node010.cluster.private
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.00     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node011.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node011.cluster.private
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   0/2       0.10     darwin        
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.00     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node013.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node013.cluster.private
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   0/2       0.00     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node014.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node014.cluster.private
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   0/2       0.01     darwin        E
        queue all.q marked QERROR as result of job 991's failure at host node015.cluster.private
        queue all.q marked QERROR as result of job 991's failure at host node015.cluster.private
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.00     darwin        

############################################################################
 - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS
############################################################################
    957 0.55500 iprscan-20 root         Eqw   12/20/2006 11:35:42     1        
    958 0.55500 iprscan-20 root         Eqw   12/20/2006 11:43:51     1        
    959 0.55500 iprscan-20 root         Eqw   12/20/2006 15:01:00     1        
    960 0.55500 iprscan-20 root         Eqw   12/21/2006 15:40:42     1        
    961 0.55500 iprscan-20 root         Eqw   12/21/2006 16:03:13     1        
    962 0.55500 iprscan-20 root         Eqw   12/21/2006 16:07:54     1        
    963 0.55500 iprscan-20 root         Eqw   12/21/2006 16:11:30     1        
    966 0.55500 iprscan-20 root         Eqw   12/21/2006 16:39:32     1        
    967 0.55500 iprscan-20 root         Eqw   12/21/2006 18:21:56     1        
    968 0.55500 iprscan-20 root         Eqw   12/21/2006 18:23:12     1        
    969 0.55500 iprscan-20 root         Eqw   12/21/2006 18:23:26     1        
    970 0.55500 iprscan-20 root         Eqw   12/21/2006 18:41:08     1        
    971 0.55500 iprscan-20 root         Eqw   12/21/2006 18:54:32     1        
    972 0.55500 iprscan-20 root         Eqw   12/21/2006 18:55:21     1        
    973 0.55500 iprscan-20 root         Eqw   12/21/2006 19:00:52     1        
    974 0.55500 iprscan-20 root         Eqw   12/21/2006 19:04:44     1        
    975 0.55500 iprscan-20 root         Eqw   12/21/2006 19:25:22     1        
    976 0.55500 iprscan-20 root         Eqw   12/21/2006 19:27:38     1        
    977 0.55500 iprscan-20 root         Eqw   12/21/2006 19:30:44     1        
    978 0.55500 iprscan-20 root         Eqw   12/21/2006 19:31:54     1        
    979 0.55500 iprscan-20 root         Eqw   12/21/2006 19:32:35     1        
    980 0.55500 iprscan-20 root         Eqw   12/22/2006 12:01:39     1        
    981 0.55500 iprscan-20 root         Eqw   12/22/2006 12:38:01     1        
    982 0.55500 iprscan-20 root         Eqw   12/22/2006 12:46:06     1        
    983 0.55500 iprscan-20 root         Eqw   12/22/2006 14:49:47     1        
    984 0.55500 iprscan-20 root         Eqw   12/22/2006 15:00:36     1        
    985 0.55500 iprscan-20 root         Eqw   12/22/2006 15:05:09     1        
    986 0.55500 iprscan-20 root         Eqw   12/26/2006 19:56:03     1        
    987 0.55500 iprscan-20 root         Eqw   12/26/2006 20:12:50     1        
    988 0.55500 iprscan-20 root         Eqw   12/26/2006 20:14:38     1        
    989 0.55500 iprscan-20 root         Eqw   12/26/2006 20:31:16     1        
    990 0.55500 iprscan-20 root         Eqw   12/26/2006 20:47:05     1        
    992 0.55500 iprscan-20 root         Eqw   12/26/2006 22:29:36     1        
    993 0.55500 iprscan-20 root         Eqw   12/26/2006 23:27:21     1        
    994 0.55500 iprscan-20 root         Eqw   12/26/2006 23:33:08     1        
    934 0.00000 iprscan-20 local        Eqw   12/03/2006 22:33:30     1        
    935 0.00000 iprscan-20 local        Eqw   12/08/2006 22:32:42     1        
    939 0.00000 iprscan-20 local        Eqw   12/13/2006 20:16:36     1        
    940 0.00000 iprscan-20 local        Eqw   12/13/2006 20:21:59     1        
    942 0.00000 iprscan-20 local        Eqw   12/18/2006 09:33:00     1        
    947 0.00000 iprscan-20 local        Eqw   12/18/2006 11:21:10     1        
    953 0.00000 iprscan-20 local2       Eqw   12/19/2006 11:33:55     1        
    954 0.00000 iprscan-20 root         Eqw   12/19/2006 11:34:20     1        
    955 0.00000 iprscan-20 local        Eqw   12/19/2006 11:35:52     1        
    956 0.00000 iprscan-20 root         Eqw   12/19/2006 11:36:09     1    

++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Dec 27 19:08:26 PST 2006

FIXED THIS ERROR: NetInfo write failed! You may not have permissions to make this change.

niutil -read . /users
>>...
34       securityagent
98       local
115      sge
130      local_2
<<<

niutil -read . /users/local_2
>>>hint: 
sharedDir: 
_writers_passwd: local
name: local_2
home: /Users/local
applemail: <?xml version="1.0" encoding="UTF-8"?>
<dict>
        <key>kAPOPRequired</key>
        <string>APOPNotRequired</string>
        <key>kAltMailStoreLoc</key>
        <string></string>
        <key>kAttributeVersion</key>
        <string>Apple Mail 1.0</string>
        <key>kAutoForwardValue</key>
        <string></string>
        <key>kIMAPLoginState</key>
        <string>IMAPAllowed</string>
        <key>kMailAccountLocation</key>
        <string>gems.rsmas.miami.edu</string>
        <key>kMailAccountState</key>
        <string>Enabled</string>
        <key>kPOP3LoginState</key>
        <string>POP3Allowed</string>
        <key>kUserDiskQuota</key>
        <string>0</string>
</dict>

authentication_authority: ;ShadowHash;
passwd: ********
_writers_hint: local
_writers_picture: local
_shadow_passwd: 
realname: local administrator
uid: 501
shell: /bin/bash
generateduid: 10A607E9-0DA9-4730-A800-916F1EE35B63
naprivs: -1073741569
gid: 501
_writers_tim_password: local
_writers_realname: 
picture: /Library/User Pictures/Animals/Butterfly.tif
<<<

niutil -read . /users/local  
>>>
hint: 
sharedDir: 
_writers_passwd: local
name: local
home: /Users/local
applemail: <?xml version="1.0" encoding="UTF-8"?>
<dict>
        <key>kAPOPRequired</key>
        <string>APOPNotRequired</string>
        <key>kAltMailStoreLoc</key>
        <string></string>
        <key>kAttributeVersion</key>
        <string>Apple Mail 1.0</string>
        <key>kAutoForwardValue</key>
        <string></string>
        <key>kIMAPLoginState</key>
        <string>IMAPAllowed</string>
        <key>kMailAccountLocation</key>
        <string>gems.rsmas.miami.edu</string>
        <key>kMailAccountState</key>
        <string>Enabled</string>
        <key>kPOP3LoginState</key>
        <string>POP3Allowed</string>
        <key>kUserDiskQuota</key>
        <string>0</string>
</dict>

authentication_authority: ;ShadowHash;
passwd: ********
_writers_hint: local
_writers_picture: local
_shadow_passwd: 
realname: local admin
uid: 501
shell: /bin/bash
generateduid: 10A607E9-0DA9-4730-A800-916F1EE35B63
naprivs: -1073741569
gid: 501
_writers_tim_password: local
_writers_realname: 
picture: /Library/User Pictures/Animals/Butterfly.tif
<<<

1. STRIP THE USER'S ADMIN STATUS IF EXISTS:
(DOES NOT EXIST IN THIS CASE - ONLY local, AND local2, BUT NO local_2 USER IN THE admin GROUP)
niutil -read . /groups/admin users local_2
>>>
name: admin
gid: 80
passwd: *
users: root local diradmin local2
generateduid: ABCDEFAB-CDEF-ABCD-EFAB-CDEF00000050
smb_sid: S-1-5-32-544
realname: Administrators
groupmembers: 7FB26970-BBFC-4AB4-8CD6-BB7E74D416BC 10A607E9-0DA9-4730-A800-916F1EE35B63 E52BAF50-08CB-4E86-A29D-579421A40AFD A6B2F8F8-3203-4C8C-98A3-A7B3E4703552 8CC70A3C-EF37-4446-9A9F-BA2E3F448CC9 78D5FCEE-AC3E-4AFA-BE92-A33443A39326
gems:~/Desktop local$ niutil -read . /groups/admin              
name: admin
gid: 80
passwd: *
users: root local diradmin local2
generateduid: ABCDEFAB-CDEF-ABCD-EFAB-CDEF00000050
smb_sid: S-1-5-32-544
realname: Administrators
groupmembers: 7FB26970-BBFC-4AB4-8CD6-BB7E74D416BC 10A607E9-0DA9-4730-A800-916F1EE35B63 E52BAF50-08CB-4E86-A29D-579421A40AFD A6B2F8F8-3203-4C8C-98A3-A7B3E4703552 8CC70A3C-EF37-4446-9A9F-BA2E3F448CC9 78D5FCEE-AC3E-4AFA-BE92-A33443A39326
<<<

sudo niutil -destroyval . /groups/admin users local_2

2. DELETE THE USER FROM NetInfo:
sudo niutil -destroy . /users/local_2
niutil -list . /users

3. REMOVE THE USER'S HOME DIRECTORY IF EXISTS:
(DOESN'T EXIST IN THIS CASE)
ls /Users
>>>Shared local  root   vanwye www    young

sudo rm -rf /Users/local_2



++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Dec 27 18:08:40 EST 2006

MSG: inquiry-support@bioteam.net

Hi Chris,

As you've probably noticed by now, most of the SGE execution nodes and the head node are showing the 'E' error status. It may be to do with changes I made in NetInfo Manager (I added the 'rw' value for the 'opts' option for the exports /Users, /Groups and /common). I tried to remove the changes but now I get this NetInfo Manager error:

NetInfo write failed! You may not have permissions to make this change.

so I can't reverse the changes.

SGE still seems to run okay on the two remaining nodes - I was able to run an array job just last night without hitch.

Regarding the issue of users on the host and on the execution hosts, the user 'young' is recognised on the execution hosts, as demonstrated by the fact that I can ssh into the nodes with this user and can su to this user whilst logged on as another user. However, the user 'local', which is not specified by WorkGroup Manager, cannot access the nodes by ssh and is not su'able on the nodes. I tried to create the user 'local' in Workgroup Manager but it won't allow me because the user 'local' already exists (it was created during the installation). However, I can't add/remove the user 'local' using System Preferences->Accounts because it's greyed out. I could do it on the command line but is that the best way?

Cheers,

Stuart.



Thanks,

Stuart.





>From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
>Reply-To: inquiry-support@bioteam.net
>To: youngstuart@hotmail.com
>Subject: Re: [bioteam.net #13817] install submit host + interproscan
>Date: Wed, 27 Dec 2006 17:40:37 -0500
>
>
>Message received, and I've succeeded in logging in.  Hopefully I'll
>have further updates for you in short order.
>
>-Chris Dwan
>   The BioTeam
>
>On Dec 27, 2006, at 12:21 PM, stuart young via RT wrote:
>
> >
> > <URL: https://rt.bioteam.net/Ticket/Display.html?id=13817 >
> >
> > Hi Chris,
> >
> > I just called and left the info on your voicemail.
> >
> > Cheers,
> >
> > Stuart.


++++++++++++++++++++++++++++++++++++++++++++++++++
Fri 22nd Dec, 2006
MSG: support@bioteam.net

Hi Chris,

Thanks for spotting that! The mount worked fine with:

sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/Volumes/gemshd4/common /common

You're set to login with the old passwords. Let me know if you have any troubles getting in.

Stuart.


>From: "Chris Dwan via RT" <inquiry-support@bioteam.net>
>Reply-To: inquiry-support@bioteam.net
>To: youngstuart@hotmail.com
>Subject: [bioteam.net #13817] Re: [bioteam.net #14465] NFS + interproscan  
>Date: Thu, 21 Dec 2006 14:15:07 -0500
>
>
> > Thanks for getting back to me and for your personal email - I'll
> > add it to
> > my future emails.
>
>Email to "support@bioteam.net" actually gets forwarded to every
>single member of the team.  Adding the personal email addresses
>simply puts additional copies in the same mailbox.
>
> > Here's the results of showmount -e gems.rsmas.miami.edu from my new
> > submit
> > host (identical results with 'showmount -e' on the head node):
> >
> > Exports list on gems.rsmas.miami.edu:
> > /opt                                Everyone
> > /Library/Perl                      192.168.2.0
> > /Volumes/gemshd4/common            129.171.101.233
> > node001.cluster.private
> > ...
> >
> > I was able to mount the three directories (/Users, /Groups and /
> > common) on
> > my submit host but now I get this error:
> > sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common /common
> >>>> mount_nfs: realpath /private/var/autofs/common: Permission denied
>
>The problem is that you're trying to mount "/common" from the portal,
>when you exported "/Volumes/gemshd4/common".  Try this command instead:
>
>sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/Volumes/gemshd4/common /
>common
>
> >>> 2) Interproscan
> >>> If get the following error when I submit a job to Interproscan:
> >>
> >> What do you get when you run "qstat -explain E
> >> <the_interpro_job_id>"?
> >> ...
> > error reason    1:          can't get password entry for user
> > "local2".
> > Either the user does not exist or NIS error!
>
> > I hope those answers help you in diagnosing the problem. Would it
> > be easier
> > if you could access the system? Let me know and I'll set up the
> > passwords as
> > before.
>
>I'm happy to log in and take a look.
>
>-Chris Dwan
>   The Bioteam
>
>
>
>


++++++++++++++++++++++++++++++++++++++++++++++++++
MSG: cdwan@bioteam.net,inquiry-support@bioteam.net
CC: dlopata@apple.net,bill@bioteam.net,chris@bioteam.net

Hi Chris,

Thanks for getting back to me and for your personal email - I'll add it to my future emails.

> > 1) NFS + Submit host
> > I just shifted over the shared directories (/common, /Users, /Groups,
> > /Library/perl) to our new RAID disc and am now getting 'permission
> >    denied'
> > from 'mount_nfs' when I try to mount them on my local machine (not the
> >    head
> > node). I'll be trying to fix that later today. Any suggestions
>
>My first suggestion is to *not* move /Library/Perl to the disk.  It's not much disk space, and the reason that we directly export
>that directory is to provide a way to update perl libraries cluster-wide, using apple's software update mechanism.  The new
>location may or may not work on the portal ... which could mess up a variety of things.
>
>What do you see when you run "showmount -e" on the portal?  That's the best place to start.
>

Sorry about the typo in my earlier message: I actually didn't move /Library/Perl (I just copied a chunk of text and didn't remove it). I only moved /Users, /Groups and /common to my RAID drive.

Here's the results of showmount -e gems.rsmas.miami.edu from my new submit host (identical results with 'showmount -e' on the head node):

Exports list on gems.rsmas.miami.edu:
/opt                                Everyone
/Library/Perl                      192.168.2.0 
/Volumes/gemshd4/common            129.171.101.233 node001.cluster.private node002.cluster.private node004.cluster.private node005.cluster.private node006.cluster.private node007.cluster.private node008.cluster.private node009.cluster.private node010.cluster.private node011.cluster.private node012.cluster.private node013.cluster.private node014.cluster.private node015.cluster.private node016.cluster.private 
/Volumes/gemshd4/Users             node001.cluster.private node002.cluster.private node004.cluster.private node005.cluster.private node006.cluster.private node007.cluster.private node008.cluster.private node009.cluster.private node010.cluster.private node011.cluster.private node012.cluster.private node013.cluster.private node014.cluster.private node015.cluster.private node016.cluster.private 
/Volumes/gemshd4/Groups            node001.cluster.private node002.cluster.private node004.cluster.private node005.cluster.private node006.cluster.private node007.cluster.private node008.cluster.private node009.cluster.private node010.cluster.private node011.cluster.private node012.cluster.private node013.cluster.private node014.cluster.private node015.cluster.private node016.cluster.private 

I was able to mount the three directories (/Users, /Groups and /common) on my submit host but now I get this error:
sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common /common
>>>mount_nfs: realpath /private/var/autofs/common: Permission denied

Please note:
1) I replaced the network/netmask options (192.168.0.2, etc.) with the client option (129.171.101.233, node001.cluster.private, etc) so that I could include the new submit host 129.171.101.233 (i.e., dlc-genomics.rsmas.miami.edu).
2) I added the /opt export to test the export settings

> > - Once I have it working, I'll try to set up my local machine as a
> >    submit
> > host by doing:
> > source /common/sge/default/common/settings.sh
>
>You'll also need to add your personal machine as a "submit host" on the portal:
>
>qconf -ah your_host

I also added my new submit host to the list of submit hosts on the head node by doing 'qconf -as <new_submit_host>' on the head node. Here are the results of qconf -ss on the head node:
>>>
dlc-genomics.rsmas.miami.edu
gems.rsmas.miami.edu
node001.cluster.private
node002.cluster.private
node003.cluster.private
node004.cluster.private
...<<<


>
> > - My scripts are in /Users/local on the head node on my local machine.
> >    If I
> > mount it as /Users2/local, will it suffice to specify a corresponding
> > mapping inside the 'sge_aliases' file to let the head node find the
> >    scripts
> > I'm referring to in '/Users/local' on the head node?
>
>The biggest stumbling block I've seen in this sort of setup is if the user ID's (the number, not the short-name) don't match
>between the submission host and the cluster.  That will result in jobs going into E state immediately after they wake up on a
>node.

I made sure that user 'local' has the same uid and gid on both the head node and my new submit host:

dlc-genomics:/Users/young local$ id
uid=501(local) gid=501(vanwye) groups=501(vanwye), 81(appserveradm), 79(appserverusr), 80(admin)
dlc-genomics:/Users/young local$ 

gems:~ local$ id
uid=501(local) gid=501(local) groups=501(local), 81(appserveradm), 500(com.apple.access_all_services), 79(appserverusr), 80(admin)
gems:~ local$ 


>
> > 2) Interproscan
> > If get the following error when I submit a job to Interproscan:
>
>What do you get when you run "qstat -explain E <the_interpro_job_id>"?
>


Here are the results of 'qstat -explain E <iprscan_job_id>':

qstat -explain E 953
>>>
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       0.13     darwin        
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.01     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.00     darwin        

############################################################################
 - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS
############################################################################
    957 0.55500 iprscan-20 root         Eqw   12/20/2006 11:35:42     1        
    958 0.55500 iprscan-20 root         Eqw   12/20/2006 11:43:51     1        
    959 0.55500 iprscan-20 root         Eqw   12/20/2006 15:01:00     1        
    934 0.00000 iprscan-20 local        Eqw   12/03/2006 22:33:30     1        
    935 0.00000 iprscan-20 local        Eqw   12/08/2006 22:32:42     1        
    939 0.00000 iprscan-20 local        Eqw   12/13/2006 20:16:36     1        
    940 0.00000 iprscan-20 local        Eqw   12/13/2006 20:21:59     1        
    942 0.00000 iprscan-20 local        Eqw   12/18/2006 09:33:00     1        
    947 0.00000 iprscan-20 local        Eqw   12/18/2006 11:21:10     1        
    953 0.00000 iprscan-20 local2       Eqw   12/19/2006 11:33:55     1        
    954 0.00000 iprscan-20 root         Eqw   12/19/2006 11:34:20     1        
    955 0.00000 iprscan-20 local        Eqw   12/19/2006 11:35:52     1        
    956 0.00000 iprscan-20 root         Eqw   12/19/2006 11:36:09     1        
<<<

And here's the output of qstat -j 953:
>>>
job_number:                 953
exec_file:                  job_scripts/953
submission_time:            Tue Dec 19 11:33:55 2006
owner:                      local2
uid:                        1025
group:                      staff
gid:                        20
sge_o_home:                 99
sge_o_log_name:             local
sge_o_path:                 /common/sge/bin/darwin:/bin:/sbin:/usr/bin:/usr/sbin:/common/mpich-1.2.7/ch_p4/bin:/common/bin:/common/sbin:/usr/local/bin:/usr/local/sbin:/usr/X11R6/bin
sge_o_shell:                /bin/bash
sge_o_workdir:              /common/apps/iprscan/bin
sge_o_host:                 gems
account:                    sge
stderr_path_list:           /dev/null
mail_list:                  local2@gems.rsmas.miami.edu
notify:                     FALSE
job_name:                   iprscan-20061219-11335477
stdout_path_list:           /dev/null
jobshare:                   0
hard_queue_list:            all.q
env_list:                   
script_file:                /common/apps/iprscan/tmp/20061219/iprscan-20061219-11335477/iprscan-20061219-11335477.dcmd
error reason    1:          can't get password entry for user "local2". Either the user does not exist or NIS error!
scheduling info:            job is in error state
<<

I hope those answers help you in diagnosing the problem. Would it be easier if you could access the system? Let me know and I'll set up the passwords as before.

Stuart.




+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


GEMS:
sudo ipfw add 40000 allow ip from 129.171.101.233 to 129.171.101.102
sudo ipfw add 40001 allow ip from 129.171.101.102 to 129.171.101.233
GENOMICS:
sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common /common
Password:
mount_nfs: can't access /common: Permission denied
(WITH BOTH young AND local USER)

CHECK IF portmap, nfsd, and mountd DAEMONS ARE RUNNING ON gems:

ps aux | grep portmap
>>>daemon     147   0.0  0.0    27252    384  ??  Ss   12:20PM   0:00.03 /usr/sbin/portmap
ps aux | grep mountd
>>>root       429   0.0  0.0    27720    292  ??  Ss   Tue12PM   0:00.02 mountd
ps aux | grep nfsd
>>local     5261   1.2  0.0    27376    424  p1  S+   12:33PM   0:00.00 grep nfsd
root       432   0.0  0.0    27244    160  ??  Ss   Tue12PM   0:00.00 nfsd-master    
root       434   0.0  0.0    27244    148  ??  S    Tue12PM   0:00.00 nfsd-server    
...<<<

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Dec 19 17:46:14 PST 2006
PROBLEM 'You are not allowed to use this computer' ON gems LOGIN:

DIAGNOSIS: MESSED UP Service ACLs IN Server Admin

SOLUTION: Open Server Admin, select the server, click Settings, and click Access.
Click on '+", then drag user 'young' over to the 'Allow only users and groups below' area.
Did you use this area to restrict the "Login Window" service?

You may have been able to accomplish a similar feat using Workgroup Manager and computer lists.

TRIED ABOVE AND ACTIVATING ALL USERS BUT NOW gems FREEZES AT LOGIN.


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
NFS DAEMONS (from Server Admin, NFS tab):

mountd
nfsd
portmap
rpc.lockd
rpc.statd

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Dec 13 21:28:28 EST 2006
USE genomics AS SUBMIT HOST (HAVING SET UP NFS MOUNT OF /common):

source /common/sge/default/common/settings.sh
qstat -f
>>>
queuename                      qtype used/tot. load_avg arch          states
----------------------------------------------------------------------------
all.q@gems.rsmas.miami.edu     BIP   0/2       0.15     darwin        
----------------------------------------------------------------------------
all.q@node001.cluster.private  BIP   0/2       0.03     darwin        
----------------------------------------------------------------------------
all.q@node002.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node004.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node005.cluster.private  BIP   0/2       0.04     darwin        
----------------------------------------------------------------------------
all.q@node006.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node007.cluster.private  BIP   0/2       0.19     darwin        
----------------------------------------------------------------------------
all.q@node008.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node009.cluster.private  BIP   0/2       0.03     darwin        
----------------------------------------------------------------------------
all.q@node010.cluster.private  BIP   0/2       0.03     darwin        
----------------------------------------------------------------------------
all.q@node011.cluster.private  BIP   0/2       0.09     darwin        
----------------------------------------------------------------------------
all.q@node012.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node013.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node014.cluster.private  BIP   0/2       0.02     darwin        
----------------------------------------------------------------------------
all.q@node015.cluster.private  BIP   0/2       0.00     darwin        
----------------------------------------------------------------------------
all.q@node016.cluster.private  BIP   0/2       0.00     darwin        

############################################################################
 - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS - PENDING JOBS
############################################################################
    934 0.00000 iprscan-20 local        Eqw   12/04/2006 01:33:30     1        
    935 0.00000 iprscan-20 local        Eqw   12/09/2006 01:32:42     1        
    939 0.00000 iprscan-20 local        Eqw   12/13/2006 23:16:36     1        
    940 0.00000 iprscan-20 local        Eqw   12/13/2006 23:21:59     1    
<<<



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Dec 13 21:22:52 EST 2006
SUMMARY OF NFS ENABLING PROCEDURE:

1. OPEN A CHANNEL THROUGH THE FIREWALL BY ADDING THESE ipfw RULES: 

sudo ipfw add 40000 allow ip from 129.171.101.233 to 129.171.101.102
sudo ipfw add 40001 allow ip from 129.171.101.102 to 129.171.101.233

2. IN NetInfo, THE /common EXPORT SHOULD HAVE THE FOLLOWING property/value PAIRS:
name            /common
clients         129.171.101.233 node001.cluster.private ... node016.cluster.private
opts            maproot=root

3. CHECK gems EXPORTS (ON genomics):
showmount -e gems.rsmas.miami.edu
>>>
Exports list on gems.rsmas.miami.edu:
/common                            129.171.101.233 node001.cluster.private node002.cluster.private node004.cluster.private node005.cluster.private node006.cluster.private node007.cluster.private node008.cluster.private node009.cluster.private node010.cluster.private node011.cluster.private node012.cluster.private node013.cluster.private node014.cluster.private node015.cluster.private node016.cluster.private 
/Users                             192.168.2.0 
/Library/Perl                      192.168.2.0 
/Groups                            192.168.2.0 
<<<


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Dec 13 18:12:29 EST 2006

1. CREATED USER local ON genomics USING 'System Prefs' (uid: 501, gid: 501 - SAME AS ON gems)

2. REDID ipfw ON gems TO ALLOW IP TO AND FROM genomics.

3. TRIED TO MOUNT /common USING local USER:

sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common /common2
>>>mount_nfs: can't access /common: Permission denied

man mount_nfs SAYS:

This message means that the remote host is either not exporting the file
     system you requested or is not exporting it to your host.  If you believe
     the remote host is indeed exporting a file system to you, make sure the
     exports(5) file is exporting the proper directories.  The program
     showmount(8) can be used to see a server's exports list.  The command
     ``showmount -e remotehostname'' will display what file systems the remote
     host is exporting.

SURE ENOUGH, showmount-e gems.rsmas.miami.edu GIVES:
Exports list on gems.rsmas.miami.edu:
/common                            192.168.2.0 
/Users                             192.168.2.0 
/Library/Perl                      192.168.2.0 
/Groups                            192.168.2.0 

SO gems IS ONLY EXPORTING TO THE 192.168.2.* SUBNET.     

4. CHECKED WHAT IS MOUNTED ALREADY:

mount
>>>
/dev/disk1s9 on / (local, journaled)
devfs on /dev (local)
fdesc on /dev (union)
<volfs> on /.vol
/dev/disk0s3 on /Volumes/Backup (local, journaled)
automount -nsl [127] on /Network (automounted)
automount -fstab [131] on /automount/Servers (automounted)
automount -static [131] on /automount/static (automounted)
automount /etc/bipod/auto.common [210] on private/var/autofs (automounted)
/dev/disk2s9 on /Volumes/PSP7000 (local, nodev, nosuid)
dlc-genomics:/Users/young local$ em /etc/bipod/auto.common
>>>
common -rw,rsize=8192,wsize=8192 gems.rsmas.miami.edu:/common
# Users  -rw,rsize=8192,wsize=8192 gems.rsmas.miami.edu:/Users
# Groups  -rw,rsize=8192,wsize=8192 gems.rsmas.miami.edu:/Groups
# RemotePerl  -ro,rsize=8192,wsize=8192 gems.rsmas.miami.edu:/Library/Perl
<<<
dlc-genomics:/Users/young local$ cd /var/autofs
dlc-genomics:/var/autofs local$ ls -al
total 2
dr-xr-xr-x    1 root  wheel  512 Dec 13 18:01 .
drwxr-xr-x   29 root  wheel  986 Dec 13 18:02 ..

ls: ./common: Host is down
lrwxr-xr-t    1 root  wheel  512 Dec 13 18:01 common
dlc-genomics:/var/autofs local$ ls common 
common
dlc-genomics:/var/autofs local$ cd common 
su: cd: common: Host is down

5. CHANGED NetInfo ENTRY FOR /common EXPORT BY ADDING clients 129.171.101.233 AND REBOOTED.
ON genomics:

showmount -e gems.rsmas.miami.edu
>>>Exports list on gems.rsmas.miami.edu:
/Users                             192.168.2.0 
/Library/Perl                      192.168.2.0 
/Groups                            192.168.2.0 
<<<

SEEMS LIKE SOME CLASH WHEN ASSIGNING clients AND network/netmask OPTIONS IN THE SAME EXPORT...


*****************************
SO REDID THE /common EXPORT ADDING:

clients         129.171.101.233 node001.cluster.private ... node016.cluster.private
opts            maproot=root
******************************
AND REMOVED THE network/netmask OPTIONS, THEN REBOOTED:

sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common /common2
>>>
Password:
mount_nfs: /common2: No such file or directory
<<<

BUT THIS WORKS!!!:

sudo mkdir /common
sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common /common            # OK!

dlc-genomics:/common young$ ls
Applications apps         data         etc          info         man          sbin         share        updatetool
Frameworks   bin          docs         examples     inquiry      mpich-1.2.7  scratch      shared       wisecfg
Preferences  build        dsh          include      lib          node         sge          tmp
dlc-genomics:/common young$ 


CHECKED LIST OF EXPORTS ON gems:

nidump -r /exports .
{
  "name" = ( "exports" );
  CHILDREN = (
    {
      "name" = ( "/Users" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Groups" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Library/Perl" );
      "clients" = ( );
      "opts" = ( "ro", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/common" );
      "clients" = ( "129.171.101.233 node001.cluster.private node002.cluster.private  node004.cluster.private  node005.cluster.private  node006.cluster.private  node007.cluster.private  node008.cluster.private  node009.cluster.private  node010.cluster.private  node011.cluster.private  node012.cluster.private  node013.cluster.private  node014.cluster.private  node015.cluster.private  node016.cluster.private" );
      "opts" = ( "maproot=root" );
    },
    {
      "clients" = ( "129.171.101.233" );
      "name" = ( "/Users/local/shared" );
    }
  )
}


JUST TO CHECK, DID MOUNT OF /opts

ON gems IN NetInfo:

name    /opt
clients 129.171.101.233
opts    maproot=root

*** REBOOTED gems ***

dlc-genomics:~ young$ showmount -e gems.rsmas.miami.edu
Exports list on gems.rsmas.miami.edu:
/opt                                Everyone
/common                            129.171.101.233 node001.cluster.private node002.cluster.private node004.cluster.private node005.cluster.private node006.cluster.private node007.cluster.private node008.cluster.private node009.cluster.private node010.cluster.private node011.cluster.private node012.cluster.private node013.cluster.private node014.cluster.private node015.cluster.private node016.cluster.private 
/Users                             192.168.2.0 
/Library/Perl                      192.168.2.0 
/Groups                            192.168.2.0 

sudo mkdir /opt2
sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/opt /opt2         # OK!!!

dlc-genomics:~ young$ ls /opt2/        
apache2
dlc-genomics:~ young$ ls /opt2/apache2/
bin     build   cgi-bin conf    error   htdocs  icons   include lib     logs    man     manual  modules

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Wed Dec 13 19:32:24 EST 2006
REWROTE ipfw TO PERMANENTLY ADD ACCESS TO AND FROM gems AND genomics:

FROM 'Exploring the Mac OSX Firewall part 2.html':
>>>
Our Own Startup Script

The statement "It's not possible to override the firewall built into Mac OS X" isn't entirely true. We could go in and hack around with the kext files, but this is more work than we really need to undertake. What we can do is create our own startup script that runs after the existing firewall to implement our rules. We need to create a directory called /Library/StartupItems/Firewall and include in it two files. The first is a generic startup script called Firewall.

sudo em /Library/StartupItems/Firewall/Firewall
>>>
#!/bin/sh

##
# Firewall
##

. /etc/rc.common

StartService ()
{
  if [ "${FIREWALL:=-NO-}" = "-YES-" ]
  then
    ConsoleMessage "Starting Firewall"
    sh /etc/rc.firewall > /dev/null
  fi
}

StopService ()
{
  ConsoleMessage "Stopping Firewall"
  /sbin/ipfw -f -q flush
}

RestartService ()
{
  StopService
  StartService
}

RunService "$1"
<<<

Additionally, we require a StartupParameters.plist file to tell the system when to start our script.

sudo em /Library/StartupItems/Firewall/StartupParameters.plist
>>>
{
  Description     = "Firewall";
  Provides        = ("Firewall");
  Requires        = ("NetworkExtensions","Resolver");
  OrderPreference = "Late";
  Messages =
  {
    start = "Starting firewall";
    stop  = "Stopping firewall";
  };
}
<<<

The real work is undertaken by the /etc/rc.firewall script where the actual calls to ipfw are made. For a moment, let's just look at the Firewall script. The service will only start if the environment variable FIREWALL is set to YES. The advantage is that if FIREWALL is undefined, it will default to NO. This allows us to try out new firewall rules by running the /etc/rc.firewall script by hand.

But if we have to reboot our computer, our rules will not be automatically loaded until we add the line FIREWALL=-YES- to the /etc/hostconfig file. This is a useful safety net when we are developing our own rules. Once we run our own rules, the firewall tab under services will not be usable until we run sudo ipfw flush to remove our rules.

sudo em /etc/hostconfig
>>>
FIREWALL=-YES-
AFPSERVER=-YES-
AUTHSERVER=-NO-
AUTOMOUNT=-YES-
CUPS=-AUTOMATIC-
NFSLOCKS=-AUTOMATIC-
NISDOMAIN=-NO-
TIMESYNC=-NO-
QTSSWEBADMIN=-NO-
WEBSERVER=-YES-
SMBSERVER=-NO-
SNMPSERVER=-NO-
SPOTLIGHT=-YES-
QTSSRUNSERVER=-NO-
TIMESERV=-YES-
WEBPERFCACHESERVER=-YES-
SOFTWAREUPDATESERVER=-NO-
HOSTNAME=gems.rsmas.miami.edu
LDAPSERVER=-YES-
IPFILTER=-YES-
CRASHREPORTER=-YES-
NATSERVER=-YES-
IPFORWARDING=-YES-
WATCHDOGTIMER=-YES-
MYSQL=-YES-
LDAPREPLICATOR=-NO-
OLDLDAPSERVER=-NO-

ARDAGENT=-YES-
MAILSERVER=-YES-
MAILMAN=-NO-
<<<

Finally, we need to fill in /etc/rc.firewall A copy of rules from earlier will do the job. With the changes made to the /etc/hostconfig file, our custom-made firewall rules will be loaded as part of the normal boot sequence. We now know nearly all we need know to play with our own rules.

sudo em /etc/rc.firewall
>>>
00001 allow udp from any 626 to any dst-port 626
00010 divert 8668 ip from any to any via en1
01000 allow ip from any to any via lo0
01010 deny ip from any to 127.0.0.0/8
01020 deny ip from 224.0.0.0/4 to any in
01030 deny tcp from any to 224.0.0.0/4 in
12300 allow tcp from any to any established
12301 allow tcp from any to any out
12302 allow tcp from any to any dst-port 22
12302 allow udp from any to any dst-port 22
12303 allow udp from any to any out keep-state
12304 allow udp from any to any in frag
12305 allow tcp from any to any dst-port 311
12306 allow tcp from any to any dst-port 625
12307 allow udp from any to any dst-port 626
12308 allow icmp from any to any icmptypes 8
12309 allow icmp from any to any icmptypes 0
12310 allow igmp from any to any
12311 allow tcp from any to any dst-port 3283,5900
12311 allow udp from any to any dst-port 3283,5900
12312 allow tcp from any to any dst-port 80
12313 allow udp from any to any dst-port 67
12314 allow udp from any to any dst-port 68
12315 allow ip from 192.168.0.0/16 to any
40000 allow ip from 129.171.101.233 to 129.171.101.102
40001 allow ip from 129.171.101.102 to 129.171.101.233
65534 deny ip from any to any
65535 allow ip from any to any
<<<


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Dec 12 18:12:58 EST 2006

rpcinfo IS NOW WORKING ON genomics TO LOOKUP gems (SEE ENTRY BELOW).

AND SHOWMOUNT ON genomics IS STILL WORKING:

dlc-genomics:~ young$ showmount -e gems.rsmas.miami.edu
Exports list on gems.rsmas.miami.edu:
/common/sge                        192.168.2.0 
/Users                             192.168.2.0 
/Library/Perl                      192.168.2.0 
/Groups                            192.168.2.0 
dlc-genomics:~ young$ 


TRIED TO MOUNT /common/sge:

sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common/sge /common2/sge
>>>mount_nfs: can't access /common/sge: Permission denied


TRIED AGAIN, THIS TIME MOUNT /common:

sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common /common
>>>mount_nfs: can't access /common: Permission denied

CHECKED man mount_nfs:
7

           mount_nfs: can't access /foo: Permission denied

     This message means that the remote host is either not exporting the file
     system you requested or is not exporting it to your host.  If you believe
     the remote host is indeed exporting a file system to you, make sure the
     exports(5) file is exporting the proper directories.  The program
     showmount(8) can be used to see a server's exports list.  The command
     ``showmount -e remotehostname'' will display what file systems the remote
     host is exporting.

CHECKED man exports:
>>>
The exports file specifies remote mount points for the NFS mount protocol
     per the NFS server specification ...
<<<
     
E.G.: CONTENTS OF auto.common FILE IN COMMAND:

/usr/sbin/automount -f -m /var/autofs /etc/bipod/auto.common

em /etc/bipod/auto.common
>>>
common -rw,rsize=8192,wsize=8192 gems.rsmas.miami.edu:/common
# Users  -rw,rsize=8192,wsize=8192 gems.rsmas.miami.edu:/Users
# Groups  -rw,rsize=8192,wsize=8192 gems.rsmas.miami.edu:/Groups
# RemotePerl  -ro,rsize=8192,wsize=8192 gems.rsmas.miami.edu:/Library/Perl
<<<

man exports (CONT.):
>>>
     Export options are specified as follows:

     -maproot=user The credential of the specified user is used for remote
     access by root.  The credential includes all the groups to which the user
     is a member on the local machine ( see id(1) ). The user may be specified
     by name or number.

     -maproot=user:group1:group2:... The colon separated list is used to spec-
     ify the precise credential to be used for remote access by root.  The
     elements of the list may be either names or numbers.  Note that ``user:''
     should be used to distinguish a credential containing no groups from a
     complete credential for that user.

     -mapall=user or -mapall=user:group1:group2:... specifies a mapping for
     all client uids (including root) using the same semantics as -maproot.

     The option -r is a synonym for -maproot in an effort to be backward com-
     patible with older export file formats.

     In the absence of -maproot and -mapall options, remote accesses by root
     will result in using a credential of -2:-2.  All other users will be
     mapped to their remote credential.  If a -maproot option is given, remote
     access by root will be mapped to that credential instead of -2:-2.  If a
     -mapall option is given, all users (including root) will be mapped to
     that credential in place of their own.

The -alldirs flag allows the host(s) to mount at any point within the
     file system, including regular files if the -r option is used on mountd.

     The -kerb option specifies that the Kerberos authentication server should
     be used to authenticate and map client credentials.  This option requires
     that the kernel be built with the NFSKERB option.

     The -ro option specifies that the file system should be exported read-
     only (default read/write).  The option -o is a synonym for -ro in an
     effort to be backward compatible with older export file formats.

     The -32bitclients option causes the NFS server to guarantee that direc-
     tory cookies will fit within 32 bits even though directory cookies are 64
     bits in NFSv3.  This option may be required with NFS clients that do not
     properly support 64 bit directory cookies.  Use of this option may result
     in sub-optimal performance of the exported file system.

****
The third component of a line specifies the host set to which the line
     applies.  The set may be specified in three ways.  The first way is to
     list the host name(s) separated by white space.  (Standard internet
     ``dot'' addresses may be used in place of names.)  The second way is to
     specify a ``netgroup'' as defined in the netgroup file (see netgroup(5)
     ). The third way is to specify an internet sub-network using a network
     and network mask that is defined as the set of all hosts with addresses
     within the sub-network.  This latter approach requires less overhead
     within the kernel and is recommended for cases where the export line
     refers to a large number of clients within an administrative sub-net.

     The first two cases are specified by simply listing the name(s) separated
     by whitespace.  All names are checked to see if they are ``netgroup''
     names first and are assumed to be hostnames otherwise.  Using the full
     domain specification for a hostname can normally circumvent the problem
     of a host that has the same name as a netgroup.  The third case is speci-
     fied by the flag -network=netname and optionally -mask=netmask.  If the
     mask is not specified, it will default to the mask for that network class
     (A, B or C; see inet(5) ).
***
...<<<

TRIED MOUNTING JUST /common:

sudo /usr/sbin/automount -f -m /var/autofs /etc/bipod/auto.common

BUT HUNG WITH NO RESULT.

TRIED MOUNTING AS young:

mount_nfs -T -s -i gems.rsmas.miami.edu:/common/sge /common2/sge
>>>mount_nfs: can't access /common/sge: Permission denied

CHANGED PERMISSIONS FROM ORIGINAL VALUES:

root    wheel   /common
sge    admin   /common/sge

TO:

CHANGED PERMISSIONS FROM ORIGINAL VALUES:

young    wheel   /common
young    wheel /common/sge

BUT SAME RESULT:
mount_nfs -T -s -i gems.rsmas.miami.edu:/common/sge /common2/sge
>>>mount_nfs: can't access /common/sge: Permission denied



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Dec 12 18:12:00 EST 2006

EDIT ipfw RULES ON gems:

NB: CAN TURN ON LOGGING:
sudo sysctl -w net.inet.ip.fw.verbose=1

DUMMY SHELL SCRIPT rules.sh:
-------------------------------------------------------------
#!/bin/sh

IPFW='/sbin/ipfw -q'


$IPFW -f flush
$IPFW add 2000 allow ip from any to any via lo*
$IPFW add 2010 deny log ip from 127.0.0.0/8 to any in
$IPFW add 2020 deny log ip from any to 127.0.0.0/8 in
$IPFW add 2030 deny log ip from 224.0.0.0/3 to any in
$IPFW add 2040 deny log tcp from any to 224.0.0.0/3 in
$IPFW add 2050 allow log tcp from any to any out
$IPFW add 2060 allow log tcp from any to any established
$IPFW add 2070 allow log tcp from any to any 22 in
$IPFW add 2080 allow log tcp from any to any 80 in
$IPFW add 2090 allow log tcp from any to any 427 in
$IPFW add 12190 deny log tcp from any to any
-------------------------------------------------------------

These rules are a copy of the rules that Mac OS X generated for Personal Web Sharing and Remote Login with logging turned on.
You do not need to enter rule 65535 as this is hard-coded into the firewall and cannot be changed.


ACTUAL SHELL SCRIPT, CONTAINING sudo ipfw list OUTPUT FOR gems, /Users/young/FUNNYBASE/dev/osx/nfs/gems.ipfw.rules.sh:
_____________________________________________________________
#!/bin/sh

IPFW='/sbin/ipfw -q'

$IPFW -f flush
$IPFW add 00001 allow udp from any 626 to any dst-port 626
$IPFW add 00010 divert 8668 ip from any to any via en1
$IPFW add 01000 allow ip from any to any via lo0
$IPFW add 01010 deny ip from any to 127.0.0.0/8
$IPFW add 01020 deny ip from 224.0.0.0/4 to any in
$IPFW add 01030 deny tcp from any to 224.0.0.0/4 in
$IPFW add 12300 allow tcp from any to any established
$IPFW add 12301 allow tcp from any to any out
$IPFW add 12302 allow tcp from any to any dst-port 22
$IPFW add 12302 allow udp from any to any dst-port 22
$IPFW add 12303 allow udp from any to any out keep-state
$IPFW add 12304 allow udp from any to any in frag
$IPFW add 12305 allow tcp from any to any dst-port 311
$IPFW add 12306 allow tcp from any to any dst-port 625
$IPFW add 12307 allow udp from any to any dst-port 626
$IPFW add 12308 allow icmp from any to any icmptypes 8
$IPFW add 12309 allow icmp from any to any icmptypes 0
$IPFW add 12310 allow igmp from any to any
$IPFW add 12311 allow tcp from any to any dst-port 3283,5900
$IPFW add 12311 allow udp from any to any dst-port 3283,5900
$IPFW add 12312 allow tcp from any to any dst-port 80
$IPFW add 12313 allow udp from any to any dst-port 67
$IPFW add 12314 allow udp from any to any dst-port 68
$IPFW add 12315 allow ip from 192.168.0.0/16 to any
$IPFW add 40000 allow ip from 129.171.101.233 to 129.171.101.102
$IPFW add 40001 allow ip from 129.171.101.102 to 129.171.101.233
$IPFW add 65534 deny ip from any to any
-------------------------------------------------------------

COPY TO gems:

RUN THE rules IN THE SHELL SCRIPT:

sudo sh gems.ipfw.rules.sh

OR ALTERNATELY (AND ACTUALLY):

sudo ipfw add 40000 allow ip from 129.171.101.233 to 129.171.101.102
sudo ipfw add 40001 allow ip from 129.171.101.102 to 129.171.101.233

CHECK THAT they have loaded correctly:

sudo ipfw list

>>>
sudo ipfw list
00001 allow udp from any 626 to any dst-port 626
00010 divert 8668 ip from any to any via en1
01000 allow ip from any to any via lo0
01010 deny ip from any to 127.0.0.0/8
01020 deny ip from 224.0.0.0/4 to any in
01030 deny tcp from any to 224.0.0.0/4 in
12300 allow tcp from any to any established
12301 allow tcp from any to any out
12302 allow tcp from any to any dst-port 22
12302 allow udp from any to any dst-port 22
12303 allow udp from any to any out keep-state
12304 allow udp from any to any in frag
12305 allow tcp from any to any dst-port 311
12306 allow tcp from any to any dst-port 625
12307 allow udp from any to any dst-port 626
12308 allow icmp from any to any icmptypes 8
12309 allow icmp from any to any icmptypes 0
12310 allow igmp from any to any
12311 allow tcp from any to any dst-port 3283,5900
12311 allow udp from any to any dst-port 3283,5900
12312 allow tcp from any to any dst-port 80
12313 allow udp from any to any dst-port 67
12314 allow udp from any to any dst-port 68
12315 allow ip from 192.168.0.0/16 to any
40000 allow ip from 129.171.101.233 to 129.171.101.102
40001 allow ip from 129.171.101.102 to 129.171.101.233
65534 deny ip from any to any
65535 allow ip from any to any
<<<


FINALLY!!
ON genomics:

rpcinfo -p gems.rsmas.miami.edu
   program vers proto   port
    100000    2   tcp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp   1020  status
    100024    1   tcp   1015  status
    100021    0   udp   1008  nlockmgr
    100021    1   udp   1008  nlockmgr
    100021    3   udp   1008  nlockmgr
    100021    4   udp   1008  nlockmgr
    100021    0   tcp   1014  nlockmgr
    100021    1   tcp   1014  nlockmgr
    100021    3   tcp   1014  nlockmgr
    100021    4   tcp   1014  nlockmgr
    100005    1   udp    989  mountd
    100005    3   udp    989  mountd
    100003    2   udp   2049  nfs
    100005    1   tcp   1010  mountd
    100003    3   udp   2049  nfs
    100005    3   tcp   1010  mountd
    100003    2   tcp   2049  nfs
    100003    3   tcp   2049  nfs

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

CHANGED CONFIGURATION WITH NFS_Manager:

gems.rsmas.miami.edu:/common/sge /common2/sge nfs -s,-i,net 0 0

CHANGE APPEARS IN /etc/fstab:

emacs -nw /etc/fstab

# fs_spec                                    fs_file  fs_vfstype  fs_mntops
gems.rsmas.miami.edu:/common/sge /common2/sge nfs       resvport,net    0 0



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tue Dec 12 16:30:33 EST 2006

MODIFIED FIREWALL SETTINGS IN ServerAdmin ON gems:

ADDED GROUP genomics:

129.171.101.233

TRIED TO GET rpcinfo FROM gems ON genomics:

rpcinfo -p gems.rsmas.miami.edu
rpcinfo: can't contact portmapper: RPC: Remote system error - Operation timed out
dlc-genomics:~ young$ portmap
dlc-genomics:~ young$ rpcinfo -p gems.rsmas.miami.edu
rpcinfo: can't contact portmapper: RPC: Remote system error - Operation timed out
ps aux | grep portm
daemon    2638   0.0 -0.0    27248    384  ??  Ss    8:48PM   0:00.21 /usr/sbin/portmap
young     3527   0.0 -0.0    27372    428  p5  S+    4:29PM   0:00.01 grep portm
dlc-genomics:~ young$ 


MOUNTS ON NODE 001 SEEM OKAY, PLUS WORKGROUP MANAGER USERS PRESENT:

'gems:~ local$ node001
Password:
Last login: Tue Dec 12 14:06:25 2006 from portal2net.clus
Welcome to Darwin!
-bash: /common/sge/default/common/settings.sh: Stale NFS file handle
-bash: /common/sge/default/common/settings.sh: Stale NFS file handle
node001:~ vanwye$ cd /common 
-bash: cd: /common: Stale NFS file handle
node001:~ vanwye$ ls   
Desktop      Library      Music        Public       plmods       test.DBI.pl~
Documents    Movies       Pictures     Sites        test.DBI.pl
node001:~ vanwye$ 


SEARCHED 'Stale NFS file handle':

Q. Why do I get the following error message sometimes?
	Stale NFS file handle

A. This type of error message is seen when a file or directory that was opened by an NFS client is removed, renamed, or replaced. 
   To fix this problem, the NFS file handles must be renegotiated. Try one of these on the client machine:
   
	a) Unmount and remount the file system, may need to use the -O (overlay option) of mount.
	
	   From the man pages:
	           -O    Overlay mount.  Allow  the  file  system  to  be
	                 mounted  over  an  existing mount  point, making
	                 the underlying file system inaccessible.   If  a
	                 mount is attempted on a pre-existing mount point
	                 without setting this flag, the mount will  fail,
              		 producing the error "device busy".
              		 
	b) Kill or restart the process trying to use the nonexistent files.
	
	c) Create another mount point and access the files from the new mount point.
	
	d) Run: /etc/init.d/nfs.client stop; /etc/init.d/nfs.client start
	
	e) Reboot the client having problems.

ANOTHER DEFINITION:

Stale NFS file handle. An attempt was made to access an open
             file (on an NFS filesystem) which is now unavailable as refer-
             enced by the file descriptor.  This may indicate the file was
             deleted on the NFS server or some other catastrophic event oc-
             curred.


NB: THIS ONLY OCCURS with /common. THE OTHER SHARES (/Users, /Groups, /RemotePerl) ARE GOOD, CAN cd INSIDE THEM, ls, ETC.

THIS IS BECAUSE node001 IS EXPECTING /common EXPORTED FROM gems, BUT gems CURRENTLY ONLY EXPORTS:

gems:~ local$ showmount -e
Exports list on localhost:
/common/sge                        192.168.2.0 
/Users                             192.168.2.0 
/Library/Perl                      192.168.2.0 
/Groups                            192.168.2.0 






++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Dec 11 22:09:01 EST 2006

# opts
The other thing to note is, if your NFS server requires a client to be coming from a privileged network port (less than 1024), you will need to add -P to the opts property, instead of the empty string. You can also modify the server to allow 'insecure' ports, but using -P doesn't require root access to the server.
This will be the case with certain BSD-based servers and some Linux ones as well. If the local mount point becomes a symlink (as discussed above), but doesn't have any of the files expected from the server, try adding the -P option, then tell automount. If the mount still doesn't work, there are other issues to deal with (a full NFS troubleshooting discussion is beyond the scope of this document).


TRY A mount_nfs:

sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common/sge /sge
>>>NFS Portmap: RPC: Port mapper failure - RPC: Timed out

( FROM:
sudo mount_nfs -2 -T -s -i localhost:/path/to/share /path/to/mountpoint
(-2 means: use NFSv2, -T means: use tcp, -s means: soft, -i means interruptable)    )
    

sudo mount_nfs -T -s -i gems.rsmas.miami.edu:/common /common



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Dec 11 20:10:02 EST 2006

IS portmap RUNNING?

ps aux | grep portmap

genomics:
root      1812   0.0 -0.0    27452    636  p1- S     5:15PM   0:00.01 sudo portmap -v
young     2586   0.0 -0.0    27372    428  p1  S+    8:35PM   0:00.01 grep portmap

gems:
daemon     144   0.0  0.0    27252    384  ??  Ss   11:44AM   0:00.01 /usr/sbin/portmap
local     2116   0.0  0.0    18068    304  p1  R+    8:38PM   0:00.00 grep portmap

node001:
vanwye   27436   1.5  0.0    27384    436  p0  S+    5:57PM   0:00.01 grep portmap
daemon     215   0.0  0.0    27260     48  ??  Ss   30Oct06   0:00.02 /usr/sbin/portmap

SO THE portmap DAEMON ISN'T RUNNING ON genomics!

CHECK THE STATUS OF THE portmap DAEMON:

rpcinfo -p
>>>
rpcinfo: can't contact portmapper: RPC: Remote system error - Connection refused
<<<

START THE portmap DAEMON:

# start mountd, this will start portmap too
sudo /usr/sbin/mountd
# start nfsd in (-t) TCP mode and (-r) only register and exit.
sudo /sbin/nfsd -t -r
# check status of portmap
rpcinfo -p
>>>
program vers proto   port
    100000    2   tcp    111  portmapper
    100000    2   udp    111  portmapper
    100005    1   udp    876  mountd
    100005    3   udp    876  mountd
    100005    1   tcp   1009  mountd
    100005    3   tcp   1009  mountd
<<<



CHECK SYSTEM LOG:
em /var/log/system.log
>>> ...
Dec 11 21:00:48 GEMS-G5-Genomics automount[2765]: Can't mount gems.rsmas.miami.edu:/gems on /private/Network/Servers/gems.rsmas.miami.edu/gems: No route to host (65)
Dec 11 21:00:48 GEMS-G5-Genomics automount[2765]: Attempt to mount /automount/Servers/gems.rsmas.miami.edu/gems returned 65 (No route to host)
Dec 11 21:00:48 GEMS-G5-Genomics automount[131]: Can't mount gems.rsmas.miami.edu:/gems on /private/Network/Servers/gems.rsmas.miami.edu/gems: No route to host (65)
Dec 11 21:00:56 GEMS-G5-Genomics kernel[0]: nfs server automount -fstab [131]: not responding
Dec 11 21:01:27 GEMS-G5-Genomics kernel[0]: nfs server automount -fstab [131]: not responding
Dec 11 21:01:44 GEMS-G5-Genomics ARDAgent [1999]: UDPWritePacket error 65 No route to host for  -127.171.101.226
Dec 11 21:01:44 GEMS-G5-Genomics ARDAgent [1999]: UDPWritePacket error 65 No route to host for  -127.171.101.228
Dec 11 21:01:58 GEMS-G5-Genomics kernel[0]: nfs server automount -fstab [131]: not responding
Dec 11 21:02:04 GEMS-G5-Genomics automount[2768]: Can't mount gems.rsmas.miami.edu:/gems on /private/Network/Servers/gems.rsmas.miami.edu/gems: No route to host (65)
Dec 11 21:02:04 GEMS-G5-Genomics automount[2768]: Attempt to mount /automount/Servers/gems.rsmas.miami.edu/gems returned 65 (No route to host)
Dec 11 21:02:04 GEMS-G5-Genomics automount[131]: Can't mount gems.rsmas.miami.edu:/gems on /private/Network/Servers/gems.rsmas.miami.edu/gems: No route to host (65)
Dec 11 21:02:29 GEMS-G5-Genomics kernel[0]: nfs server automount -fstab [131]: not responding
Dec 11 21:03:00 GEMS-G5-Genomics kernel[0]: nfs server automount -fstab [131]: not responding
Dec 11 21:03:20 GEMS-G5-Genomics automount[2775]: Can't mount gems.rsmas.miami.edu:/gems on /private/Network/Servers/gems.rsmas.miami.edu/gems: No route to host (65)
Dec 11 21:03:20 GEMS-G5-Genomics automount[2775]: Attempt to mount /automount/Servers/gems.rsmas.miami.edu/gems returned 65 (No route to host)
Dec 11 21:03:20 GEMS-G5-Genomics automount[131]: Can't mount gems.rsmas.miami.edu:/gems on /private/Network/Servers/gems.rsmas.miami.edu/gems: No route to host (65)
Dec 11 21:03:31 GEMS-G5-Genomics kernel[0]: nfs server automount -fstab [131]: not responding
<<<


CHECK FIREWALL ON gems:

System Preferences --> Sharing --> Firewall ...? NOT PRESENT BECAUSE IT WASN'T INSTALLED ON gems...

CHECK ON COMMAND LINE:
gems:~ local$ sudo ipfw list
Password:
00001 allow udp from any 626 to any dst-port 626
00010 divert 8668 ip from any to any via en1
01000 allow ip from any to any via lo0
01010 deny ip from any to 127.0.0.0/8
01020 deny ip from 224.0.0.0/4 to any in
01030 deny tcp from any to 224.0.0.0/4 in
12300 allow tcp from any to any established
12301 allow tcp from any to any out
12302 allow tcp from any to any dst-port 22
12302 allow udp from any to any dst-port 22
12303 allow udp from any to any out keep-state
12304 allow udp from any to any in frag
12305 allow tcp from any to any dst-port 311
12306 allow tcp from any to any dst-port 625
12307 allow udp from any to any dst-port 626
12308 allow icmp from any to any icmptypes 8
12309 allow icmp from any to any icmptypes 0
12310 allow igmp from any to any
12311 allow tcp from any to any dst-port 3283,5900
12311 allow udp from any to any dst-port 3283,5900
12312 allow tcp from any to any dst-port 80
12313 allow udp from any to any dst-port 67
12314 allow udp from any to any dst-port 68
12315 allow ip from 192.168.0.0/16 to any
65534 deny ip from any to any
65535 allow ip from any to any


sudo ipfw show
>>>
Password:
00001       0         0 allow udp from any 626 to any dst-port 626
00010   97422  22330088 divert 8668 ip from any to any via en1
01000 5013664 707399944 allow ip from any to any via lo0
01010       0         0 deny ip from any to 127.0.0.0/8
01020       0         0 deny ip from 224.0.0.0/4 to any in
01030       0         0 deny tcp from any to 224.0.0.0/4 in
12300  192806  55357382 allow tcp from any to any established
12301       8       480 allow tcp from any to any out
12302       8       508 allow tcp from any to any dst-port 22
12302       0         0 allow udp from any to any dst-port 22
12303  119012  16130615 allow udp from any to any out keep-state
12304       0         0 allow udp from any to any in frag
12305       0         0 allow tcp from any to any dst-port 311
12306       0         0 allow tcp from any to any dst-port 625
12307       0         0 allow udp from any to any dst-port 626
12308       1        28 allow icmp from any to any icmptypes 8
12309       1        28 allow icmp from any to any icmptypes 0
12310    3743    119644 allow igmp from any to any
12311       8       496 allow tcp from any to any dst-port 3283,5900
12311       5       163 allow udp from any to any dst-port 3283,5900
12312       0         0 allow tcp from any to any dst-port 80
12313     533    176124 allow udp from any to any dst-port 67
12314       0         0 allow udp from any to any dst-port 68
12315  231875  11406971 allow ip from 192.168.0.0/16 to any
65534   48083   5026623 deny ip from any to any
65535    4418    447694 allow ip from any to any
<<<

FIREWALL ON genomics:
sudo ipfw list
>>>65535 allow ip from any to any



CHECK syslog CONFIG FILE FOR gems, FOUND HERE:

em /etc/syslog.conf
>>>
*.err;kern.*;auth.notice;authpriv,remoteauth,install.none;mail.crit             /dev/console
*.notice;authpriv,remoteauth,ftp,install.none;kern.debug;mail.crit      /var/log/system.log

# Send messages normally sent to the console also to the serial port.
# To stop messages from being sent out the serial port, comment out this line.
#*.err;kern.*;auth.notice;authpriv,remoteauth.none;mail.crit            /dev/tty.serial

# The authpriv log file should be restricted access; these
# messages shouldn't go to terminals or publically-readable
# files.
authpriv.*;remoteauth.crit                              /var/log/secure.log

lpr.info                                                /var/log/lpr.log
mail.*                                                  /var/log/mail.log
ftp.*                                                   /var/log/ftp.log
netinfo.err                                             /var/log/netinfo.log
install.*                                               /var/log/install.log
install.*                                               @127.0.0.1:32376
local0.*                                                /var/log/ipfw.log
<<<



CHECK portmap OF genomics FROM gems:

rpcinfo -p 129.171.101.233
>>>
program vers proto   port
    100000    2   tcp    111  portmapper
    100000    2   udp    111  portmapper
    100005    1   udp    876  mountd
    100005    3   udp    876  mountd
    100005    1   tcp   1009  mountd
    100005    3   tcp   1009  mountd
<<<

CHECK portmap OF gems FROM genomics :
rpcinfo -p  gems.rsmas.miami.edu
>>> rpcinfo: can't contact portmapper: RPC: Remote system error - Operation timed out


CHECK FOR gems EXPORTS ON genomics:
showmount -e gems.rsmas.miami.edu
>>>



man mount_nfs
mount -t nfs remotehost:/filesystem /localmountpoint
mount -t nfs gems.rsmas.miami.edu:/common/sge /common/sge


cd /automount
ls Servers/
dlc-genomics.rsmas.miami.edu gems.rsmas.miami.edu
dlc-genomics:/automount young$ ls Servers
nfs server automount -fstab [131]: not responding
nfs server automount -fstab [131]: not responding

DUMP EXPORTS FROM NetInfo:

ON genomics:
nidump -r /exports .


ON gems:

nidump -r /exports .
>>>
{
  "name" = ( "exports" );
  CHILDREN = (
    {
      "name" = ( "/Users" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Groups" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/common/sge" );
      "clients" = ( );
      "opts" = ( "maproot=root", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/Library/Perl" );
      "clients" = ( );
      "opts" = ( "ro", "network=192.168.2.0", "mask=255.255.255.0" );
    },
    {
      "name" = ( "/common" );
      "clients" = ( "129.171.101.233" );
      "opts" = ( "", "", "ro" );
    },
    {
      "name" = ( "/common" );
      "clients" = ( "129.171.101.233" );
      "opts" = ( "", "", "ro" );
    }
  )
}
<<<

ON node001
nidump -r /exports .
>>>
{
  "name" = ( "exports" );
  CHILDREN = (
    {
      "name" = ( "/Library/NetBoot/NetBootSP0" );
      "opts" = ( "ro" );
    }
  )
}<<<

showmount -e
>>>
Exports list on localhost:
/Library/NetBoot/NetBootSP0         Everyone
<<<






++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

RECENT CREATED LINKS BUT NO SUCCESSFUL MOUNTS:

lrwxr-xr-x    1 root    admin         18B Dec  3 21:41 Groups -> /var/autofs/Groups
lrwxr-xr-x    1 root    admin         22B Dec  3 21:41 RemotePerl -> /var/autofs/RemotePerl
lrwxr-xr-x    1 root    admin         17B Dec  3 21:41 Users.wrong -> /var/autofs/Users

lrwxr-xr-x    1 root    admin         18B Dec  6 20:19 common -> /var/autofs/common
lrwxr-xr-x    1 root    admin         18B Dec  5 10:16 common.bkp -> /var/autofs/common
drwxr-xr-x    3 root    admin        102B Dec  5 10:16 common_CONFLICT




FROM nfs_exam.pdf:

Table 9.5. Overview of NFS-related programs and files 

program or file             description 
The kernel                  provides NFS support 
The portmapper              handles RPC requests 
rpc.nfsd                    NFS server control (kernel space) or software (user space) 
rpc.mountd                  handles incoming (un)mount requests 
The file /etc/exports       defines which filesystems are exported 
The exportfs command        (un)exports filesystems 
showmount --exports         shows current exports 
The rpcinfo command         reports RPC information 
The nfsstat command         reports NFS statistics 
showmount --all             shows active mounts to me (this host) 
mount -t nfs remote:/there/here     mounts a remote filesystem 
umount -t nfs -a            unmounts all remote filesystems 


STARTED portmap:

sudo portmap -v &


THEN CHECKED FOR showmount:

showmount -e gems.rsmas.miami.edu

>>> RPC: Port mapper failure: Can't do Exports rpc

TIP:
Make sure portmap (or rpcbind if the server is 5.x) is allowed to talk to
the client in /etc/hosts.allow.



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Dec 11 16:53:26 PST 2006
ON genomics:

showmount -e
RPC: Program not registered: Can't do Exports rpc
dlc-genomics:~ young$ showmount -e 

CHANGED USERID AND GROUPID OF young ON gems USING WORKGROUP MANAGER.

NB: COULD HAVE USED COMMAND LINE TO GET USERID AND GROUP ID:
First find out what the currently assigned user id of the user is. Log in as root (sudo sh) on the client and then:

ON gems:
niutil -read . /users/local | grep '^uid'
>>> uid: 501

GID ON LOCAL MACHINE:
niutil -read . /users/local | grep '^gid'
>>> gid: 501

CURRENT GID:
niutil -read . /groups/local | grep '^gid'
>>> gid: 501





EDIT /etc/fstab TO CONFIGURE NFS MOUNTS:

em /etc/fstab
>>>
# fs_spec                                    fs_file  fs_vfstype  fs_mntops
#
# UUID=DF000C7E-AE0C-3B15-B730-DFD2EF15CB91  /export  ufs         ro
# UUID=FAB060E9-79F7-33FF-BE85-E1D3ABD3EDEA  none     hfs         rw,noauto
# LABEL=This\040Is\040The\040Volume\040Name  none     msdos        ro
<<<

nidump fstab .

gems.rsmas.miami.edu:/common/sge /common2/sge nfs  0 0
gems.rsmas.miami.edu:/gems /Network/Servers url net,url==afp://;AUTH=NO%20USER%20AUTHENT@129.171.101.102/gems 0 0

SO WENT INTO NetInfo AND DELETED THE ENTRY FOR LINE 2 AND REDID nidump:

nidump fstab .

export                        mount_point vfs_type options         dumpFreq passno
gems.rsmas.miami.edu:/common/sge /common2/sge nfs                   0 0

SAW IN NFSAutomountOSX THAT ARGUMENTS resvport,net ARE NEEDED, I.E.:


asha:/pub                /Network/Servers   nfs   resvport,net

SO ADDED resvport,net:
em /etc/fstab
gems.rsmas.miami.edu:/common/sge /common2/sge nfs  0 0
niload -m fstab . < /etc/fstab

AND NetInfo WAS CHANGED, NOW INCLUDES AMONGST opts resvport,net:

mounts --> gems.rsmas.miami.edu -->
    passno  0
    dir /common2/sge
    opts    (resvport,net,)
    dump_freq   0
    name    gems.rsmas.miami.edu:/common/sge
    type    nfs
    vfstype nfs


sudo nidump fstab .
>>> gems.rsmas.miami.edu:/common/sge /common2/sge nfs resvport,net, 0 0

em /etc/fstab

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mon Dec 11 10:32:38 EST 2006

Mac OS X as an NFS Client

Mounting NFS filesystems on OS X can be done simply by running:

sudo mount nfsserver:/exported/path /private/mnt

sudo mount gems.rsmas.miami.edu:/common/sge /private/mnt

 RPC: Timed out
NFS Portmap: RPC: Port mapper failure - RPC: Timed out

DOESN'T WORK:
check if nfsd and portmap really started on server:
sockstat | grep portmap
sockstat | grep nfsd



ON gems:

rpcinfo -p gems.rsmas.miami.edu

>>>
program vers proto   port
    100000    2   tcp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp   1020  status
    100024    1   tcp   1015  status
    100021    0   udp   1008  nlockmgr
    100021    1   udp   1008  nlockmgr
    100021    3   udp   1008  nlockmgr
    100021    4   udp   1008  nlockmgr
    100021    0   tcp   1014  nlockmgr
    100021    1   tcp   1014  nlockmgr
    100021    3   tcp   1014  nlockmgr
    100021    4   tcp   1014  nlockmgr
    100005    1   udp    989  mountd
    100005    3   udp    989  mountd
    100005    1   tcp   1010  mountd
    100005    3   tcp   1010  mountd
    100003    2   udp   2049  nfs
    100003    3   udp   2049  nfs
    100003    2   tcp   2049  nfs
    100003    3   tcp   2049  nfs
<<<

ON node001:
rpcinfo -p gems.rsmas.miami.edu
>>>   program vers proto   port
    100000    2   tcp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp   1020  status
    100024    1   tcp   1015  status
    100021    0   udp   1008  nlockmgr
    100021    1   udp   1008  nlockmgr
    100021    3   udp   1008  nlockmgr
    100021    4   udp   1008  nlockmgr
    100021    0   tcp   1014  nlockmgr
    100021    1   tcp   1014  nlockmgr
    100021    3   tcp   1014  nlockmgr
    100021    4   tcp   1014  nlockmgr
    100005    1   udp    989  mountd
    100005    3   udp    989  mountd
    100005    1   tcp   1010  mountd
    100005    3   tcp   1010  mountd
    100003    2   udp   2049  nfs
    100003    3   udp   2049  nfs
    100003    2   tcp   2049  nfs
    100003    3   tcp   2049  nfs
<<<

BUT ON genomics:

rpcinfo -p gems.rsmas.miami.edu
>>> rpcinfo: can't contact portmapper: RPC: Remote system error - Operation timed out


SEEMS THAT portmapper IS NOT RUNNING ON genomics:

rpcinfo -p 129.171.101.233
rpcinfo: can't contact portmapper: RPC: Remote system error - Connection refused

rpcinfo -p genomics.rsmas.miami.edu
rpcinfo: can't contact portmapper: RPC: Remote system error - Connection refused


START portmapper:

sudo portmapper



man mountd
>>>
 -d      The -d option will cause mountd to not daemonize itself and it
             will log lots of information about its inner-workings to stderr
             instead of syslog(3).
<<<

/usr/sbin/mountd



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ON genomics:

showmount -e gems.rsmas.miami.edu

ERROR:

RPC: Port mapper failure: Can't do Exports rpc




The following error:

NFS Portmap: RPC: Program not registered

means that the remote host is not running mountd(8).  The program
rpcinfo(8) can be used to determine if the remote host is running nfsd,
and mountd by issuing the command:

    rpcinfo -p remotehostname

If the remote host is running nfsd, mountd, rpc.statd, and rpc.lockd it
would display:

DID

rpcinfo -p gems.rsmas.miami.edu


CHECKED FOR RUNNING AUTOMOUNT:

ps aux | grep automount

FOUND THAT automount WAS RUNNING 

/usr/sbin/automount -f -m /var/autofs /etc/bipod/auto.common

SO VIEWED auto.common:


sudo em /etc/bipod/auto.common

/usr/sbin/automount -f -m /var/autofs /etc/bipod/auto.common


