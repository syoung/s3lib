<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en">
<head profile="http://gmpg.org/xfn/11">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>cloud-computing &laquo;  Blue Collar Bioinformatics</title>

<link rel="stylesheet" href="http://s1.wp.com/wp-content/themes/pub/journalist/style.css?m=1308063518g" type="text/css" media="screen" />
<!--[if IE 6]>
<link rel="stylesheet" href="http://s1.wp.com/wp-content/themes/pub/journalist/ie6.css?m=1306999063g" type="text/css" media="screen" />
<![endif]-->
<link rel="pingback" href="http://bcbio.wordpress.com/xmlrpc.php" />
<link rel="alternate" type="application/rss+xml" title="Blue Collar Bioinformatics &raquo; Feed" href="http://bcbio.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Blue Collar Bioinformatics &raquo; Comments Feed" href="http://bcbio.wordpress.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Blue Collar Bioinformatics &raquo; cloud-computing Tag Feed" href="http://bcbio.wordpress.com/tag/cloud-computing/feed/" />
<script type="text/javascript">
/* <![CDATA[ */
function addLoadEvent(func){var oldonload=window.onload;if(typeof window.onload!='function'){window.onload=func;}else{window.onload=function(){oldonload();func();}}}
/* ]]> */
</script>
<link rel="stylesheet" href="http://s0.wp.com/wp-content/themes/h4/global.css?m=1306998998g" type="text/css" />
<script type='text/javascript' src='http://s1.wp.com/wp-includes/js/l10n.js?m=1306998980g&amp;ver=20101110'></script>
<script type='text/javascript' src='http://s0.wp.com/wp-includes/js/jquery/jquery.js?m=1306998979g&amp;ver=1.6.1'></script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://bcbio.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://bcbio.wordpress.com/wp-includes/wlwmanifest.xml" /> 
<link rel='index' title='Blue Collar Bioinformatics' href='http://bcbio.wordpress.com/' />
<meta name="generator" content="WordPress.com" />
<link rel="shortcut icon" type="image/x-icon" href="http://s1.wp.com/i/favicon-stacked.ico?m=1306998978g" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="http://s1.wp.com/i/favicon-stacked.ico?m=1306998978g" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon" href="http://s0.wp.com/wp-content/themes/h4/i/webclip.png?m=1306998996g" />
<link rel='openid.server' href='http://bcbio.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='http://bcbio.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="http://bcbio.wordpress.com/osd.xml" title="Blue Collar Bioinformatics" />
<link rel="search" type="application/opensearchdescription+xml" href="http://wordpress.com/opensearch.xml" title="WordPress.com" />
	<style type="text/css">
	/* <![CDATA[ */
				div#likes { margin-top: 15px; }
		.like-button { border: 1px solid #eee; padding: 2px 6px; font-size: 13px; font-family: arial, tahoma, sans-serif; }
		#wpl-likebox { clear: left; font-size: 11px; font-family: arial, tahoma, verdana, sans-serif !important; min-height: 30px; margin: 10px 0 !important; padding: 5px 0 10px 0 !important; }
		#wpl-button { float: left; background: url( http://s0.wp.com/i/buttonbg.png ) top left repeat-x; margin-right: 7px; border: 1px solid #d4d4d4; -moz-border-radius: 3px; -webkit-border-radius: 3px; border-radius: 3px; }
		#wpl-button a { border-bottom: none !important; color: #666 !important; line-height: 130% !important; text-decoration: none !important; outline: none; float: left; padding: 3px 6px 2px 24px !important; font-size: 11px !important; background: url( http://s1.wp.com/i/likestar.png ) 6px 49.8% no-repeat; }
		#wpl-button a:hover { border-bottom: none !important; }
		#wpl-button.liked { background: #feffce; border: 1px solid #f3e389; }
		#wpl-button.liked a { color: #ba871b !important; }
		#wpl-likebox #wpl-count { min-height: 25px; line-height: 130% !important; float: left; padding-top: 4px; }
		#wpl-likebox #wpl-avatars { clear: left; max-height: 98px; overflow: hidden; margin-top: 15px; line-height: 130% !important; }
		#wpl-likebox #wpl-avatars img { border: none !important; }
		#wpl-likebox #wpl-mustlogin { line-height: 14px !important; font-size: 11px; clear: left; margin-top: 5px; background: #f0f0f0; padding: 10px; width: 65%; -moz-border-radius: 3px; -webkit-border-radius: 3px; border-radius: 3px; }
		#wpl-likebox #wpl-mustlogin a { color: #888; text-decoration: underline; }
		#wpl-likebox #wpl-mustlogin p { margin: 5px 0; padding: 0 }
		#wpl-likebox #wpl-mustlogin input.input { padding: 2px; background: #fff; font-size: 11px; font-family: inherit; border: 1px solid #ccc; -moz-box-shadow: 1px 1px 1px rgba(0, 0, 0, 0.1) inset; -webkit-box-shadow: 1px 1px 1px rgba(0, 0, 0, 0.1) inset; line-height: 12px; }
		#wpl-likebox #wpl-mustlogin input#wp-submit { border: 1px solid #ccc; font-size: 11px; background: #fafafa; repeat-x; -moz-border-radius: 3px; -webkit-border-radius: 3px; border-radius: 3px; padding: 2px 4px !important; line-height: 12px; }
		#wpl-likebox #wpl-mustlogin label { position: relative; cursor: text; }
		#wpl-likebox #wpl-mustlogin label span { position: absolute; top: 0px; left: 5px; padding: 0 !important; }
		#wpl-likebox #wpl-mustlogin label span { top /*\**/: -10px\9; }
	/* ]]> */
	</style>
	<meta name="application-name" content="Blue Collar Bioinformatics" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=http://bcbio.wordpress.com/feed/;icon-uri=http://s1.wp.com/i/favicon-stacked.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=http://s2.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=http://s2.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=http://s2.wp.com/i/favicon.ico" /></head>

<body class="archive tag tag-cloud-computing tag-1715866 typekit-enabled highlander-enabled highlander-light">
<div id="container" class="group">

<h1><a href="http://bcbio.wordpress.com/">Blue Collar Bioinformatics</a></h1>

<div id="content">

<h2 class="archive">Posts Tagged &#8216;<strong>cloud-computing</strong>&#8217;</h2>


<h2 id="post-187"><a href="http://bcbio.wordpress.com/2011/04/10/parallel-upload-to-amazon-s3-with-python-boto-and-multiprocessing/" rel="bookmark">Parallel upload to Amazon S3 with python, boto and&nbsp;multiprocessing</a></h2>
	<p class="comments"><a href="http://bcbio.wordpress.com/2011/04/10/parallel-upload-to-amazon-s3-with-python-boto-and-multiprocessing/#comments">with 13 comments</a></p>

<div class="main">
	<p>One challenge with moving analysis pipelines to cloud resources like <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/ec2/">Amazon EC2</a> is figuring out the logistics of transferring files. Biological data is big; with the rapid adoption of new machines like the <a href="http://www.illumina.com/systems/hiseq_2000.ilmn">HiSeq</a> and decreasing <a href="http://www.genome.gov/sequencingcosts/">sequencing costs</a>, the data transfer question isn&#8217;t going away soon. The use of Amazon in bioinformatics was brought up during a recent <a href="http://biostar.stackexchange.com/questions/7143/is-amazons-ec2-commonly-used-for-bioinformatics">discussion on the BioStar question answer site</a>. <a href="http://mndoci.github.com/">Deepak&#8217;s</a> answer highlighted the role of parallelizing uploads and downloads to ease this transfer burden. Here I describe a method to improve upload speed by splitting over multiple processing cores.</p>
<p><a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/s3/faqs/#What_is_Amazon_S3">Amazon Simple Storage System (S3)</a> provides relatively inexpensive cloud storage with their <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/s3/faqs/#What_is_RRS">reduced redundancy storage</a> option. S3, and all of Amazon&#8217;s cloud services, are accessible directly from Python using <a href="http://code.google.com/p/boto/">boto</a>. By using <a href="http://www.elastician.com/2010/12/s3-multipart-upload-in-boto.html">boto&#8217;s multipart upload support</a>, coupled with Python&#8217;s built in <a href="http://docs.python.org/library/multiprocessing.html">multiprocessing</a> module, I&#8217;ll demonstrate maximizing transfer speeds to make uploading data less painful. The <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/utils/s3_multipart_upload.py">script is available from GitHub</a> and requires <a href="https://github.com/boto/boto">the latest boto from GitHub (2.0b5 or better)</a>.</p>
<div id="parallel-upload-with-multiprocessing">
<h2>Parallel upload with multiprocessing</h2>
<p>The overall process uses boto to connect to an S3 upload bucket, initialize a multipart transfer, split the file into multiple pieces, and then upload these pieces in parallel over multiple cores. Each processing core is passed a set of credentials to identify the transfer: the multipart upload identifier (<code>mp.id</code>), the S3 file key name (<code>mp.key_name</code>) and the S3 bucket name (<code>mp.bucket_name</code>).</p>
<pre class="brush: python;">
import boto

conn = boto.connect_s3()
bucket = conn.lookup(bucket_name)
mp = bucket.initiate_multipart_upload(s3_key_name, reduced_redundancy=use_rr)
with multimap(cores) as pmap:
    for _ in pmap(transfer_part, ((mp.id, mp.key_name, mp.bucket_name, i, part)
                                  for (i, part) in
                                  enumerate(split_file(tarball, mb_size, cores)))):
        pass
mp.complete_upload()
</pre>
<p>The <code>split_file</code> function uses the unix split command to divide the file into sections, each of which will be uploaded separately.</p>
<pre class="brush: python;">
def split_file(in_file, mb_size, split_num=5):
    prefix = os.path.join(os.path.dirname(in_file),
                          &quot;%sS3PART&quot; % (os.path.basename(s3_key_name)))
    split_size = int(min(mb_size / (split_num * 2.0), 250))
    if not os.path.exists(&quot;%saa&quot; % prefix):
        cl = [&quot;split&quot;, &quot;-b%sm&quot; % split_size, in_file, prefix]
        subprocess.check_call(cl)
    return sorted(glob.glob(&quot;%s*&quot; % prefix))
</pre>
<p>The multiprocessing aspect is managed using a <a href="http://docs.python.org/library/contextlib.html">contextmanager</a>. The initial multiprocessing pool is setup, using a specified number of cores, and configured to allow keyboard interrupts. We then return a lazy map function (<a href="http://docs.python.org/library/itertools.html#itertools.imap">imap</a>) which can be used just like Python&#8217;s standard <code>map</code>. This transparently divides the function calls for each file part over all available cores. Finally, the pool is cleaned up when the map is finished running.</p>
<pre class="brush: python;">
@contextlib.contextmanager
def multimap(cores=None):
    if cores is None:
        cores = max(multiprocessing.cpu_count() - 1, 1)
    def wrapper(func):
        def wrap(self, timeout=None):
            return func(self, timeout=timeout if timeout is not None else 1e100)
        return wrap
    IMapIterator.next = wrapper(IMapIterator.next)
    pool = multiprocessing.Pool(cores)
    yield pool.imap
    pool.terminate()
</pre>
<p>The actual work of transferring each portion of the file is done using two functions. The helper function, <code>mp_from_ids</code>, uses the id information about the bucket, file key and multipart upload id to reconstitute a multipart upload object:</p>
<pre class="brush: python;">
def mp_from_ids(mp_id, mp_keyname, mp_bucketname):
    conn = boto.connect_s3()
    bucket = conn.lookup(mp_bucketname)
    mp = boto.s3.multipart.MultiPartUpload(bucket)
    mp.key_name = mp_keyname
    mp.id = mp_id
    return mp
</pre>
<p>This object, together with the number of the file part and the file itself, are used to transfer that section of the file. The file part is removed after successful upload.</p>
<pre class="brush: python;">
@map_wrap
def transfer_part(mp_id, mp_keyname, mp_bucketname, i, part):
    mp = mp_from_ids(mp_id, mp_keyname, mp_bucketname)
    print &quot; Transferring&quot;, i, part
    with open(part) as t_handle:
        mp.upload_part_from_file(t_handle, i+1)
    os.remove(part)
</pre>
<p>When all sections, distributed over all processors, are finished, the multipart upload is signaled complete and Amazon finishes the process. Your file is now available on S3.</p>
</div>
<div id="parallel-download">
<h2>Parallel download</h2>
<p>Download speeds can be maximized by utilizing several existing parallelized accelerators:</p>
<ul>
<li><a href="http://axel.alioth.debian.org/">axel</a></li>
<li><a href="http://aria2.sourceforge.net/">aria2</a></li>
<li><a href="http://lftp.yar.ru/">lftp</a></li>
</ul>
<p>Combine these with the uploader to build up a cloud analysis workflow: move your data to S3, run a complex analysis pipeline on EC2, push the results back to S3, and then download them to local machines. Please share other tips and tricks you use to deal with Amazon file transfer in the comments.</p>
</div>
</div>

<div class="meta group">
<div class="signature">
    <p>Written by Brad Chapman <span class="edit"></span></p>
    <p>April 10, 2011 at 1:27 pm</p>
</div>	
<div class="tags">
    <p>Posted in <a href="http://en.wordpress.com/tag/analysis/" title="View all posts in analysis" rel="category tag">analysis</a></p>
    <p>Tagged with <a href="http://en.wordpress.com/tag/bioinformatics/" rel="tag">bioinformatics</a>, <a href="http://en.wordpress.com/tag/cloud-computing/" rel="tag">cloud-computing</a>, <a href="http://en.wordpress.com/tag/ec2/" rel="tag">ec2</a>, <a href="http://en.wordpress.com/tag/python/" rel="tag">python</a></p></div>
</div>


<h2 id="post-164"><a href="http://bcbio.wordpress.com/2010/10/13/cloudbiolinux-progress-on-bioinformatics-cloud-images-and-data/" rel="bookmark">CloudBioLinux: progress on bioinformatics cloud images and&nbsp;data</a></h2>
	<p class="comments"><a href="http://bcbio.wordpress.com/2010/10/13/cloudbiolinux-progress-on-bioinformatics-cloud-images-and-data/#comments">with 3 comments</a></p>

<div class="main">
	<p>My last post introduced a <a href="http://bcbio.wordpress.com/2010/05/08/automated-build-environment-for-bioinformatics-cloud-images/">framework for building bioinformatics cloud images</a>, which makes it easy to do biological computing work using <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/ec2/">Amazon EC2</a> and other on-demand computing providers. Since that initial announcement we&#8217;ve had amazing interest from the community and made great progress with:</p>
<ul>
<li>A permanent web site at <a href="http://cloudbiolinux.org/">cloudbiolinux.org</a></li>
<li>Additional software and genomic data</li>
<li>New user documentation</li>
<li>A community coding session: <a href="http://www.open-bio.org/wiki/Codefest_2010">Codefest 2010</a></li>
</ul>
<div id="new-software-and-data">
<h2>New software and data</h2>
<p>The most exciting changes have been the rapid expansion of installed software and libraries. The goal is to provide an image that experienced developers will find as useful as their custom configured servers. A <a href="http://github.com/chapmanb/cloudbiolinux/tree/master/contributors.mkd">great group of contributors</a> have put together a large set of <a href="http://github.com/chapmanb/cloudbiolinux/tree/master/config/#readme">programs and libraries</a>; the configuration files have all the details on <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/packages.yaml">installed programs</a> as well as libraries for <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/python-libs.yaml">Python</a>, <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/perl-libs.yaml">Perl</a>, <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/ruby-libs.yaml">Ruby</a>, and <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/r-libs.yaml">R</a>. Another addition is support for <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/custom.yaml">non-packaged programs</a> which provides software not yet neatly wrapped in a package manger or library-specific install system: next-gen software packages like <a href="http://picard.sourceforge.net/">Picard</a>, <a href="http://www.broadinstitute.org/gsa/wiki/index.php/The_Genome_Analysis_Toolkit">GATK</a> and <a href="http://bowtie-bio.sourceforge.net/">Bowtie</a> are installed through custom scripts.</p>
<p>To improve accessibility for developers who prefer a desktop experience, a <a href="http://freenx.berlios.de/">FreeNX server</a> was integrated with the provided images. Tim Booth from the <a href="http://nebc.nerc.ac.uk/tools/bio-linux">NEBC Bio-Linux</a> team headed up the integration of FreeNX, and the user experience looks very similar to a locally installed Bio-Linux desktop.</p>
<p>In addition to the software image, a publicly available data volume is now available that contains:</p>
<ul>
<li>Genome sequences pre-indexed for search with next-gen aligners like <a href="http://bowtie-bio.sourceforge.net/">Bowtie</a>, <a href="http://www.novocraft.com/main/page.php?s=novoalign">Novoalign</a>, and <a href="http://bio-bwa.sourceforge.net/">BWA</a>.</li>
<li><a href="http://genome.ucsc.edu/cgi-bin/hgLiftOver">LiftOver</a> files for mapping between sequence coordinates.</li>
<li><a href="http://www.ebi.ac.uk/uniref/">UniRef</a> protein databases, indexed for searching with <a href="http://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=Download">BLAST+</a>.</li>
</ul>
<p>Coupled with the software images, this volume makes it easy to do next-gen analyses. Start up an Amazon AMI, attach the genome data volume, transfer your fastq file to the instance, and kick off the analysis. The overhead of software installation and genome indexing is completely removed. Thanks to the work of Enis Afgan and James Taylor of <a href="http://galaxyproject.org">Galaxy</a>, the data volume plugs directly into <a href="http://bitbucket.org/galaxy/galaxy-central/wiki/cloud">Galaxy&#8217;s ready to use cloud image</a>. Coupling the data and software with Galaxy provides a familiar web interface for running tools and developing biological workflows.</p>
<p>The data volume preparation is fully automated via a <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/data_fabfile.py">fabric install script</a>, similar to the <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/fabfile.py">software install script</a>. Additional data sources are easily integrated, and we hope to expand the available datasets based on feedback from the community.</p>
</div>
<div id="documentation-and-presentations">
<h2>Documentation and presentations</h2>
<p>The software and data volumes are only as good as the documentation which helps people use them:</p>
<ul>
<li>Bela Tiwari of the NEBC Bio-Linux team has written an <a href="http://bazaar.launchpad.net/%7Ebt27uk/cloud-bl-tutorials/trunk/download/head%3A/gettingstarted_cloud-20100913135735-ilr1gse4kxo9ylx6-1/gettingStarted_CloudBioLinux.pdf">excellent introduction to Amazon EC2 and CloudBioLinux</a>. This breaks down the process of signing up for an account, creating a software image, associating data volumes and setting up a graphical server. It&#8217;s a great place to get started with CloudBioLinux.</li>
<li>Ntino Krampis, from the <a href="http://www.jcvi.org/cms/research/projects/jcvi-cloud-biolinux/overview/">JCVI Cloud Bio-Linux</a> project, gave a <a href="http://www.slideshare.net/agbiotec/chi-next-genntinokrampis">presentation on CloudBioLinux</a> explaining the motivation behind the project and providing usage examples.</li>
<li>My presentation on <a href="http://www.slideshare.net/chapmanb/chapman-opensource-cloud">the open source community behind CloudBioLinux</a> from <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/genomics_workshop/">Amazon&#8217;s Genomic Data workshop</a>. This details the project goals and automated code organization.</li>
</ul>
</div>
<div id="community:-codefest-2010">
<h2>Community: Codefest 2010</h2>
<p>The CloudBioLinux community had a chance to work together for two days in July at <a href="http://www.open-bio.org/wiki/Codefest_2010">Codefest 2010</a>. In conjunction with the <a href="http://www.open-bio.org/wiki/BOSC_2010">Bioinformatics Open Source Conference (BOSC)</a> in Boston, this was a free to attend coding session hosted at <a href="http://www.hsph.harvard.edu/research/bioinfocore/">Harvard School of Public Health</a> and <a href="http://www.mgh.harvard.edu/">Massachusetts General Hospital</a>. Over 30 developers donated two days of their time to working on CloudBioLinux and other bioinformatics open source projects.</p>
<p>Many of the advances in CloudBioLinux detailed above were made possible through this session: the FreeNX graphical client integration, documentation, Galaxy interoperability, and many library and data improvements were started during the two days of coding and discussions. Additionally, the relationships developed are the foundation for better communication amongst open source projects, which is something we need to be continually striving for in the scientific computing world.</p>
<p>It was amazing and inspiring to get such positive feedback from so many members of the bioinformatics community. We&#8217;re planning another session next year in Vienna, again just before <a href="http://www.iscb.org/ismbeccb2011">BOSC and ISMB 2011</a>; and again, everyone is welcome.</p>
</div>
<div id="summary">
<h2>Summary</h2>
<p>Go to the <a href="http://cloudbiolinux.org/">CloudBioLinux website</a> for the latest publicly available images and data volumes, which are ready to use on Amazon EC2. With <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/about-aws/whats-new/2010/09/09/announcing-micro-instances-for-amazon-ec2/">Amazon&#8217;s new micro-images</a> you can start analyzing data for only a few cents an hour. It&#8217;s an easy way to explore if cloud resources will help with computational demands in your work. We&#8217;re very interested in feedback and happy to have other developers helping out; please get in touch on the <a href="http://groups.google.com/group/cloudbiolinux">CloudBioLinux mailing list</a>.</p>
</div>
</div>

<div class="meta group">
<div class="signature">
    <p>Written by Brad Chapman <span class="edit"></span></p>
    <p>October 13, 2010 at 6:19 pm</p>
</div>	
<div class="tags">
    <p>Posted in <a href="http://en.wordpress.com/tag/openbio/" title="View all posts in OpenBio" rel="category tag">OpenBio</a></p>
    <p>Tagged with <a href="http://en.wordpress.com/tag/bioinformatics/" rel="tag">bioinformatics</a>, <a href="http://en.wordpress.com/tag/cloud-computing/" rel="tag">cloud-computing</a>, <a href="http://en.wordpress.com/tag/ec2/" rel="tag">ec2</a>, <a href="http://en.wordpress.com/tag/ngs/" rel="tag">ngs</a></p></div>
</div>


<h2 id="post-150"><a href="http://bcbio.wordpress.com/2010/05/08/automated-build-environment-for-bioinformatics-cloud-images/" rel="bookmark">Automated build environment for Bioinformatics cloud&nbsp;images</a></h2>
	<p class="comments"><a href="http://bcbio.wordpress.com/2010/05/08/automated-build-environment-for-bioinformatics-cloud-images/#comments">with 15 comments</a></p>

<div class="main">
	<p><a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com">Amazon web services</a> provide scalable, on demand computational resources through their <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/ec2/">elastic compute cloud (EC2)</a>. Previously, I <a href="http://bcbio.wordpress.com/2009/09/07/usage-plans-for-amazon-web-services-research-grant/">described the goal</a> of providing publicly available machine images loaded with bioinformatics tools. I&#8217;m happy to describe an initial step in that direction: an automated build system, using easily editable configuration files, that generates a bioinformatics-focused <a href="http://en.wikipedia.org/wiki/Amazon_Machine_Image">Amazon Machine Image (AMI)</a> containing packages integrated from several existing efforts. The hope is to consolidate the community&#8217;s open source work around a single, continuously improving, machine image.</p>
<p>This image incorporates software from several existing AMIs:</p>
<ul>
<li><a href="http://www.jcvi.org/cms/research/projects/jcvi-cloud-biolinux/overview/">JCVI Cloud BioLinux</a> &#8212; JCVI&#8217;s work porting Bio-Linux to the cloud.</li>
<li><a href="http://fortinbras.us/bioperl-max/">bioperl-max</a> &#8212; Fortinbras&#8217; package of BioPerl and associated informatics tools.</li>
<li><a href="http://blog.infochimps.org/2009/02/06/start-hacking-machetec2-released/">MachetEC2</a> &#8212; An InfoChimps image loaded with data mining software.</li>
</ul>
<p>Each of these libraries inspired different aspects of developing this image and associated infrastructure, and I&#8217;m extremely grateful to the authors for their code, documentation and discussions.</p>
<p>The current AMI is available for loading on EC2 &#8212; search for &#8216;CloudBioLinux&#8217; in the  <a href="https://console.aws.amazon.com/">AWS console</a> or go to <a href="http://cloudbiolinux.org">the CloudBioLinux project page</a> for the latest AMIs. Automated scripts and configuration files with contained packages are available as a  <a href="https://github.com/chapmanb/cloudbiolinux">GitHub repository</a>.</p>
<h2>Contributions encouraged</h2>
<p>This image is intended as a starting point for developing a community resource that provides biology and data-mining oriented software. Experienced developers should be able to fire up this image and expect to find the same up to date libraries and programs they have installed  on their work machines. If their favorite package is missing it should be quick  and easy to add, making the improvement available to future developers.</p>
<p>Achieving these goals requires help and contributions from other programmers utilizing the cloud &#8212; everyone reading this. The current image is ready to be used, but is more complete in areas where I normally work.  For instance, the Python and R libraries are off to a good start. I&#8217;d like to extend an invitation to folks with expertise in other areas to  help improve the coverage of this AMI:</p>
<ul>
<li>Programmers: help expand the configuration files for your areas of interest:
<ul>
<li>Perl CPAN support and libraries</li>
<li>Ruby gems</li>
<li>Java libraries</li>
<li>Haskell hackage support and libraries</li>
<li>Erlang libraries</li>
<li>Bioinformatics areas of specialization:
<ul>
<li>Next-gen sequencing</li>
<li>Structural biology</li>
<li>Parallelized algorithms</li>
</ul>
</li>
<li>Much more&#8230; Let us know what you are interested in.</li>
</ul>
</li>
<li>Documentation experts: provide cookbook style instructions to help others get started.</li>
<li>Porting specialists: The automation infrastructure is dependent on having  good ports for libraries and programs. Many widely used biological programs are not  yet ported. Establishing a Debian or Ubuntu port for a missing program will not only  help this effort, but make the programs more widely available.</li>
<li>Systems administrators: The ultimate goal is to have the AMI be automatically updated on a regular basis with the latest changes. We&#8217;d like to set up an Amazon instance that pulls down the latest configuration, populates an image, builds the AMI, and then updates a central web page and REST API for getting the latest and greatest.</li>
<li>Testers: Check that this runs on <a href="http://www.eucalyptus.com/">open source Eucalyptus</a> clouds, additional linux distributions, and other cloud deployments.</li>
</ul>
<p>If any of this sounds interesting, please get in contact. The <a href="http://groups.google.com/group/cloudbiolinux">Cloud BioLinux mailing list</a> is a good central point for discussion.</p>
<h2>Infrastructure overview</h2>
<p>In addition to supplying an image for downstream use, this implementation was designed to be easily extendible. Inspired by the <a href="http://github.com/infochimps/machetec2">MachetEC2</a> project, packages to be installed are entered into a set of easy to edit configuration files in <a href="http://www.yaml.org/">YAML</a> syntax. There are  three different configuration file types:</p>
<ul>
<li><a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/main.yaml">main.yaml</a> &#8212; The high level configuration file defining which groups of packages to install. This allows a user to build a custom image simply by commenting out those groups which are not of interest.</li>
<li><a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/packages.yaml">packages.yaml</a> &#8212; Defines debian/ubuntu packages to be installed. This leans heavily on the work of <a href="http://www.debian.org/devel/debian-med/">DebianMed</a> and <a href="http://nebc.nox.ac.uk/tools/bio-linux/">Bio-Linux</a> communities, as well as all of the hard working package maintainers for the distributions. If it exists in package form, you can list it here.</li>
<li><a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/python-libs.yaml">python-libs.yaml</a>, <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/config/r-libs.yaml">r-libs.yaml</a> &#8212; These take advantage of language specific ways of installing libraries. Currently implemented is support for Python library installation  from the <a href="http://pypi.python.org/">Python package index</a>, and R library installation from <a href="http://cran.r-project.org/">CRAN</a> and <a href="http://www.bioconductor.org/docs/install/">Bioconductor</a>. This will be expanded to include support for other languages.</li>
</ul>
<p>The <a href="http://docs.fabfile.org/">Fabric remote automated deployment tool</a> is used to build AMIs from  these configuration files. Written in Python, the <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/fabfile.py">fabfile</a> automates the process of installing packages on the cloud machine.</p>
<p>We hope that the straightforward architecture of the build system will encourage  other developers to dig in and provide additional coverage of program and libraries  through the configuration files. For those comfortable with Python, the fabfile is  very accessible for adding in new functionality.</p>
<p>If you are interested in face-to-face collaboration and will be in the Boston area on July 7th and 8th, check out <a href="http://www.open-bio.org/wiki/Codefest_2010">Codefest 2010</a>; it&#8217;ll be two enjoyable days of cloud informatics development. I&#8217;m looking forward to hearing from other developers who are interested in building and maintaining an easy to use, up to date, machine image that can help make biological computation more accessible to the community.</p>
</div>

<div class="meta group">
<div class="signature">
    <p>Written by Brad Chapman <span class="edit"></span></p>
    <p>May 8, 2010 at 9:35 am</p>
</div>	
<div class="tags">
    <p>Posted in <a href="http://en.wordpress.com/tag/openbio/" title="View all posts in OpenBio" rel="category tag">OpenBio</a></p>
    <p>Tagged with <a href="http://en.wordpress.com/tag/bioinformatics/" rel="tag">bioinformatics</a>, <a href="http://en.wordpress.com/tag/cloud-computing/" rel="tag">cloud-computing</a>, <a href="http://en.wordpress.com/tag/ec2/" rel="tag">ec2</a>, <a href="http://en.wordpress.com/tag/openbio/" rel="tag">OpenBio</a></p></div>
</div>


<h2 id="post-125"><a href="http://bcbio.wordpress.com/2009/09/07/usage-plans-for-amazon-web-services-research-grant/" rel="bookmark">Usage plans for Amazon Web Services research&nbsp;grant</a></h2>
	<p class="comments"><a href="http://bcbio.wordpress.com/2009/09/07/usage-plans-for-amazon-web-services-research-grant/#comments">with 7 comments</a></p>

<div class="main">
	<p>
<a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/">Amazon Web Services</a> provide an excellent distributed computing infrastructure through their <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/ec2/">Elastic Compute Cloud (EC2)</a>, <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/ebs/">Elastic Block Storage (EBS)</a> and associated resources. Essentially, they make available on demand compute power and storage at prices that scale with usage. In the past I&#8217;ve written about <a href="http://bcbio.wordpress.com/2009/03/29/python-gff-parser-update-parallel-parsing-and-gff2/">using EC2 for parallel parsing of large files</a>. Generally, I am a big proponent of distributed computing as a solution to dealing with problems ranging from job scaling to improving code availability.
</p>
<p>
One of the challenges in advocating for using EC2 at my day to day work is the presence of existing computing resources. We have servers and clusters, but how will we scale for future work? Thankfully, we are able to assess the utility of Amazon services for future scaling through their <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/education/#researchers">education and research grants</a>. Our group applied and was accepted for a research grant which we plan to use to develop and distribute next generation sequencing analyses both within our group at <a href="http://genetics.mgh.harvard.edu/PublicWeb/">Mass General Hospital</a> and in the larger community.
</p>
<p>
Amazon Machine Images (AMIs) provide an opportunity for the open source bioinformatics community to increase code availability. AMIs are essentially pre-built operating systems with installed programs. By <a href="http://www.philchen.com/2009/02/14/how-to-create-an-amazon-elastic-compute-cloud-ec2-machine-image-ami">creating AMIs</a> and making them available, a programmer can make their code readily accessible to users and avoid any of the intricacies of installation and configuration. Add this to available data in the form of <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/publicdatasets/">public data sets</a> and you have a ready to go analysis platform with very little overhead. There is already a large set of <a href="http://developer.amazonwebservices.com/connect/kbcategory.jspa?categoryID=171">available AMIs</a> from which to build.
</p>
<p>
This idea and our thoughts on moving portions of our next generation sequencing analysis to EC2 are fleshed out further in our research grant application, portions of which are included below. We&#8217;d love to collaborate with others moving their bioinformatics work to Amazon resources.
</p>
<h3><b>Research Background</b></h3>
<p>
One broad area of rapid growth in biology research is deep sequencing (or short read) technology. A single lab investigator can produce hundreds of millions of DNA sequences, equivalent in scale to the entire human genome, in a period of days. This DNA sequencing technology is widely available through both on-site facilities as well as through commercial services. Creating scalable analysis methods is a high priority for the entire bioinformatics community; see <a href="http://selab.janelia.org/people/eddys/blog/?p=123">http://selab.janelia.org/people/eddys/blog/?p=123</a> for a presentation nicely summarizing the issues. We propose to address the computational bottlenecks resulting from this huge data volume using distributed AWS resources.
</p>
<p>
An additional aim of our work is to provide tools to biologists looking to solve their data analysis challenges. When the computational portion of a project becomes a time limiting step, we can often speed up the cycling between experiment and analysis by providing researchers with ready to run scripts or web interfaces. However, this is complicated by high usage on shared computational resources and heterogeneous platforms requiring time consuming configuration. Both problems could be ameliorated by scalable EC2 instances with custom configured machine images.
</p>
<p>
The goals of this grant application are to develop our analysis platform on Amazon&#8217;s compute cloud and assess transfer, storage and utilization costs. We currently have internal computational resources ranging from high performance clusters to large memory machines. We believe Amazon&#8217;s compute cloud to be an ideal solution as our analysis needs outgrow our current hardware.
</p>
<h3><b>Benefits to Amazon and the community</b></h3>
<p>
Developing software on AWS architecture presents a move towards a standard platform for bioinformatics research. Our group is invested in the open source community and shares both code and analysis tools. One common hindrance to sharing is the heterogeneity of platforms; code is developed on a local cluster and not readily generalizable, hence it is not shared.
</p>
<p>
By building public machine images along with reusable source code, a diverse variety of users can readily use our code and tools. As short read sequencing continues to increase in utility and popularity, a practical ready-to-go platform for analyses will encourage many users to adopt parallelization on cloud resources as a research approach. We have begun initial work with this paradigm by developing parsers for large annotation files using MapReduce on EC2.
</p>
<p>
Having the ability to utilize AWS with your support will help us further develop and disseminate analysis templates for the larger biology community, enabling science both at MGH and elsewhere.</p>
</div>

<div class="meta group">
<div class="signature">
    <p>Written by Brad Chapman <span class="edit"></span></p>
    <p>September 7, 2009 at 7:42 pm</p>
</div>	
<div class="tags">
    <p>Posted in <a href="http://bcbio.wordpress.com/category/uncategorized/" title="View all posts in Uncategorized" rel="category tag">Uncategorized</a></p>
    <p>Tagged with <a href="http://en.wordpress.com/tag/bioinformatics/" rel="tag">bioinformatics</a>, <a href="http://en.wordpress.com/tag/cloud-computing/" rel="tag">cloud-computing</a>, <a href="http://en.wordpress.com/tag/distributed-computing/" rel="tag">distributed-computing</a>, <a href="http://en.wordpress.com/tag/ec2/" rel="tag">ec2</a>, <a href="http://en.wordpress.com/tag/ngs/" rel="tag">ngs</a></p></div>
</div>


<h2 id="post-111"><a href="http://bcbio.wordpress.com/2009/06/11/talking-at-bosc-2009-about-publishing-biological-data-on-the-web/" rel="bookmark">Talking at BOSC 2009 about publishing biological data on the&nbsp;web</a></h2>
	<p class="comments"><a href="http://bcbio.wordpress.com/2009/06/11/talking-at-bosc-2009-about-publishing-biological-data-on-the-web/#comments">with 3 comments</a></p>

<div class="main">
	<p>
The <a href="http://open-bio.org/wiki/BOSC_2009">Bioinformatics Open Source Conference (BOSC)</a> is taking place later this month in Stockholm, Sweden. I will be attending for the first time in a few years, and giving a short and sweet 10 minute talk about ideas for publishing biological data on the web. BOSC provides a chance to meet and talk with many of the great people involved in open source bioinformatics; <a href="http://open-bio.org/wiki/BOSC_2009_Schedule">the schedule</a> this year looks fantastic. The talk will be held in conjunction with <a href="http://dam-sig.org/">The Data and Analysis Management special interest group</a>, also full of interesting talks.
</p>
<p>
The talk will promote development of reusable web based interface libraries layered on top of existing open source projects. The <a href="http://open-bio.org/w/images/2/2b/Bosc2009_submission_14.pdf">PDF abstract</a> provides the full description and motivation; below is a more detailed outline based on some brainstorming and organization:
</p>
<ul>
<li>Motivation: rapidly organize and display biological data in a web accessible format.
</ul>
<ul>
<li>Current state: reusable bioinformatics libraries targeted at programmers &#8212; <a href="http://biopython.org/">Biopython</a>, <a href="http://bitbucket.org/james_taylor/bx-python/wiki/Home">bx-python</a>, <a href="http://code.google.com/p/pygr/"> pygr</a>, <a href="http://sourceforge.net/projects/pycogent">PyCogent</a>
<li>Current state: back end databases for storing biological data &#8212; <a href="http://www.biosql.org/wiki/Main_Page">BioSQL</a>, <a href="http://gmod.org/wiki/GMOD">GMOD</a>
<li>Current state: full featured web applications targeted at users &#8212; <a href="http://galaxy.psu.edu/">Galaxy</a>, <a href="http://gmod.org/wiki/Ggb/">GBrowse</a>
</ul>
<ul>
<li>My situation: biologist and developer with organized data that needs analysis and presentation, internally with collaborators and externally with larger community.
</ul>
<ul>
<li>Proposal: integrate bioinformatics libraries, database schemas, and open source web development frameworks to provide re-usable components that can serve as a base for custom data presentation.
</ul>
<ul>
<li>Framework: utilize cloud infrastructure for reliable deployment &#8212; <a href="http://code.google.com/appengine/">Google App Engine</a>, <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/ec2/">Amazon EC2</a>
<li>Framework: make use of front end javascript frameworks &#8212; <a href="http://jquery.com/">jQuery</a>, <a href="http://extjs.com/">ExtJS</a>.
<li>Framework: make use of back end web frameworks &#8212; <a href="http://pylonshq.com/">Pylons</a>
</ul>
<ul>
<li>Implementation: <a href="http://biosqlweb.appspot.com">Demo server</a> for displaying sequences plus annotations
<li>Implementation: Utilizes BioSQL schema, ported to object oriented data store; Google App engine backend or <a href="http://www.10gen.com/blog/2009/3/use-mongodb-with-googles-appengine-sdk">MongoDB backend</a>
<li>Implementation: Data import/export with Biopython libraries &#8212; GenBank in and GFF out
<li>Implementation: Additional screenshots from internal web displays.
</ul>
<ul>
<li>Challenges: Generalizing and organizing display and retrieval code without having to buy into a large framework.
<li>Challenges: Re-usable components for cross-language functionality; javascript front end displays for multi-language back ends.
<li>Challenges: Build a community that thinks of reusing and sharing display code as much as parsing and pipeline development code.
</ul>
<p>
I would be happy to hear comments or suggestions about the talk. If you&#8217;re going to BOSC and want to meet up, definitely drop me a line.</p>
</div>

<div class="meta group">
<div class="signature">
    <p>Written by Brad Chapman <span class="edit"></span></p>
    <p>June 11, 2009 at 7:41 am</p>
</div>	
<div class="tags">
    <p>Posted in <a href="http://en.wordpress.com/tag/openbio/" title="View all posts in OpenBio" rel="category tag">OpenBio</a></p>
    <p>Tagged with <a href="http://en.wordpress.com/tag/bioinformatics/" rel="tag">bioinformatics</a>, <a href="http://en.wordpress.com/tag/cloud-computing/" rel="tag">cloud-computing</a>, <a href="http://en.wordpress.com/tag/ec2/" rel="tag">ec2</a>, <a href="http://en.wordpress.com/tag/gae/" rel="tag">gae</a>, <a href="http://en.wordpress.com/tag/visualization/" rel="tag">visualization</a></p></div>
</div>


<h2 id="post-86"><a href="http://bcbio.wordpress.com/2009/03/29/python-gff-parser-update-parallel-parsing-and-gff2/" rel="bookmark">Python GFF parser update &#8212; parallel parsing and&nbsp;GFF2</a></h2>
	<p class="comments"><a href="http://bcbio.wordpress.com/2009/03/29/python-gff-parser-update-parallel-parsing-and-gff2/#comments">with 9 comments</a></p>

<div class="main">
	<h3><b>Parallel parsing</b></h3>
<p>
Last week we discussed <a href="http://bcbio.wordpress.com/2009/03/22/mapreduce-implementation-of-gff-parsing-for-biopython/">refactoring the Python GFF parser to use a MapReduce framework</a>. This was designed with the idea of being able to scale GFF parsing as file size increases. In addition to large files describing genome annotations, GFF is spreading to next-generation sequencing; SOLiD provides <a href="http://solidsoftwaretools.com/gf/project/matogff/">a tool to convert their mapping files to GFF</a>.
</p>
<p>
Parallel processing introduces overhead due to software intermediates and networking costs. For the <a href="http://discoproject.org/">Disco</a> implementation of GFF parsing, parsed lines run through Erlang and are translated to and from JSON strings. Invoking this overhead is worthwhile only if enough processors are utilized to overcome the slowdown. To estimate when we should start to parallelize, I looked at parsing a 1.5GB GFF file on a small multi-core machine and a remote cluster. Based on rough testing and non-scientific linear extrapolation of the results, I estimate 8 processors are needed to start to see a speed-up over local processing.</p>
<p>
The starting baseline for parsing our 1.5GB file is one and half minutes using a single processor on my commodity Dell desktop. This desktop has 4 cores, and running Disco utilizing all 4 CPUs, the time increases to 3 minutes. Once Disco itself has been set up, switching between the two is seamless since the file is parsed in shared memory.
</p>
<p>
The advantage of utilizing Disco is that it can scale from this local implementation to very large clusters. Amazon&#8217;s <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/ec2/">Elastic Computing Cloud (EC2)</a> is an amazing resource where you can quickly set up and run jobs on <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/ec2/instance-types/">powerful hardware</a>. It is essentially an instant on-demand cluster for running applications. Using the <a href="http://ec2-downloads.s3.amazonaws.com/elasticfox-owners-manual.pdf">ElasticFox Firefox plugin</a> and the <a href="http://discoproject.org/doc/start/ec2setup.html#ec2setup">setup directions for Disco on EC2</a>, I was able to quickly test GFF parsing on a test cluster of three small (AMI <code>ami-cfbc58a6</code>, a Debian 5.0 Lenny instance) instances. For distributed jobs, the main challenges are setting up each of the cluster nodes with the software, and distributing the files across the nodes. Disco provides scripts to install itself across the cluster and to <a href="http://discoproject.org/doc/start/tutorial.html#amazon-ec2">distribute the file being parsed</a>. When you are attacking a GFF parsing job that is prohibitively slow or memory intensive on your local hardware, a small cluster of a few extra-large of extra-large high CPU instances on EC2 will help you overcome these limitations. Hopefully in the future Disco will become available on some standard Amazon machine images, lowering the threshold to getting a job running.
</p>
<p>
In practical terms, local GFF parsing will be fine for most standard files. When you are limited by parsing time with large files, attack the problem using either a local cluster or EC2 with 8 or more processors. To better utilize a small number of local CPUs, it makes sense to explore a light weight solution such as the new python <a href="http://docs.python.org/dev/library/multiprocessing.html#module-multiprocessing">multiprocessing</a> module.
</p>
<h3><b>GFF2 support</b></h3>
<p>
The initial target for GFF parsing was the <a href="http://www.sequenceontology.org/gff3.shtml">GFF3</a> standard. However, many genome centers still use the older <a href="http://www.sanger.ac.uk/Software/formats/GFF/GFF_Spec.shtml">GFF2</a> or <a href="http://mblab.wustl.edu/GTF22.html">GTF</a> formats. The main parsing difference between these formats are the attributes. In GFF3, they look like:
</p>
<pre>
  ID=CDS:B0019.1;Parent=Transcript:B0019.1;locus=amx-2
</pre>
<p></p>
<p>
while in GFF2 they are less standardized, and look like:
</p>
<pre>
  Transcript "B0019.1" ; WormPep "WP:CE40797" ; Note "amx-2"
</pre>
<p></p>
<p>
The parser has been updated to handle GFF2 attributes correctly, with test cases from several genome centers. In practice, there are several tricky implementations of the GFF2 specifications; if you find examples of incorrectly parsed attributes by the current parser, please pass them along.
</p>
<p>
GFF2 and GFF3 also differ in how nested features are handled. A standard example of nesting is specifying the coding regions of a transcript. Since GFF2 didn&#8217;t provide a default way to do this, there are several different methods used in practice. Currently, the parser leaves these GFF2 features as flat and you would need to write custom code on top of the parser to nest them if desired.
</p>
<p>
The latest version of the GFF parsing code is available from <a href="http://github.com/chapmanb/bcbb/tree/master/gff">GitHub</a>. To install it, click the download link on that page and you will get the whole directory along with a setup.py file to install it. It installs outside of Biopython since it is still under development. As always, I am happy to accept any contributions or suggestions.</p>
</div>

<div class="meta group">
<div class="signature">
    <p>Written by Brad Chapman <span class="edit"></span></p>
    <p>March 29, 2009 at 10:49 am</p>
</div>	
<div class="tags">
    <p>Posted in <a href="http://en.wordpress.com/tag/openbio/" title="View all posts in OpenBio" rel="category tag">OpenBio</a></p>
    <p>Tagged with <a href="http://en.wordpress.com/tag/bioinformatics/" rel="tag">bioinformatics</a>, <a href="http://en.wordpress.com/tag/biopython/" rel="tag">biopython</a>, <a href="http://en.wordpress.com/tag/cloud-computing/" rel="tag">cloud-computing</a>, <a href="http://en.wordpress.com/tag/ec2/" rel="tag">ec2</a>, <a href="http://en.wordpress.com/tag/gff/" rel="tag">gff</a></p></div>
</div>


<h2 id="post-80"><a href="http://bcbio.wordpress.com/2009/03/15/biosql-on-google-app-engine/" rel="bookmark">BioSQL on Google App&nbsp;Engine</a></h2>
	<p class="comments"><a href="http://bcbio.wordpress.com/2009/03/15/biosql-on-google-app-engine/#comments">with 2 comments</a></p>

<div class="main">
	<p>
The <a href="http://www.biosql.org/wiki/Main_Page">BioSQL</a> project provides a well thought out relational database schema for storing biological sequences and annotations. For those developers who are responsible for setting up local stores of biological data, BioSQL provides a huge advantage via reusability. Some of the best features of BioSQL from my experience are:</p>
<ul>
<li>Available interfaces for several languages (via Biopython, BioPerl, BioJava and BioRuby).
<li>Flexible storage of data via a key/value pair model. This models information in an extensible manner, and helps with understanding distributed key/value stores like <a onclick="return mugicPopWin(this,event);" oncontextmenu="mugicRightClick(this);" href="http://aws.amazon.com/simpledb/">SimpleDB</a> and <a href="http://couchdb.apache.org/">CouchDB</a>.
<li>Overall data model based on GenBank flat files. This makes teaching the model to biology oriented users much easier; you can pull up a text file from NCBI with a sequence and directly show how items map to the database.
</ul>
</p>
<p>
Given the usefulness of BioSQL for local relational data storage, I would like to see it move into the rapidly expanding <a href="http://en.wikipedia.org/wiki/Cloud_computing">cloud development community</a>. Tying BioSQL data storage in with <a href="http://en.wikipedia.org/wiki/Web_application_framework">Web frameworks</a> will help researchers make their data publicly available earlier and in standard formats. As a nice recent example, George has a series of posts on using <a href="http://biorelated.wordpress.com/2009/01/07/bio-graphics-biosql-and-rails-part-1/">BioSQL with Ruby on Rails</a>. There have also been several discussions of the BioSQL mailing list around standard web tools and APIs to sit on top of the database; see <a href="http://lists.open-bio.org/pipermail/biosql-l/2009-January/001464.html">this thread</a> for a recent example.
</p>
<p>
Towards these goals, I have been working on a BioSQL backed interface for <a href="http://code.google.com/appengine/">Google App Engine</a>. Google App Engine is a Python based framework to quickly develop and deploy web applications. For data storage, Google&#8217;s <a href="http://code.google.com/appengine/docs/python/datastore/">Datastore</a> provides an object interface to a distributed scalable storage backend. Practically, App Engine has free hosting quotas which can scale to larger instances as demand for the data in the application increases; this will appeal to cost-conscious researchers by avoiding an initial barrier to making their data available.
</p>
<p>
My work on this was accelerated by the Open Bioinformatics Foundation&#8217;s move to apply for participation in <a href="http://open-bio.org/wiki/Google_Summer_of_Code_2009">Google&#8217;s Summer of Code</a>. OpenBio is a great community that helps organize projects like BioPerl, Biopython, BioJava and BioRuby. After writing up a <a href="http://open-bio.org/wiki/Google_Summer_Code_2009#BioSQL_web_interface_and_API_on_Google_App_Engine">project idea</a> for BioSQL on Google App Engine in our application, I was inspired to finish a demonstration of the idea.
</p>
<p>
I am happy to announce a simple demonstration server running a BioSQL based backend: <a href="http://biosqlweb.appspot.com/">BioSQL Web</a>. The source code is available from <a href="http://github.com/chapmanb/biosqlweb/tree/master">my git repository</a>. Currently the server allows uploads of GenBank formatted files and provides a simple view of the records, annotations and sequences. The data is stored in the Google Datastore with an <a href="http://github.com/chapmanb/biosqlweb/blob/183ea5fad91437a2d9a4524db5708f13865c3e9a/BioSQL/GAE/BioSQLModels.py">object interface</a> that mimics the BioSQL relational model.
</p>
<p>
Future posts will provide more details on the internals of the server end and client interface as they develop. As always, feedback, thoughts and code contributions are very welcome.</p>
</div>

<div class="meta group">
<div class="signature">
    <p>Written by Brad Chapman <span class="edit"></span></p>
    <p>March 15, 2009 at 8:45 am</p>
</div>	
<div class="tags">
    <p>Posted in <a href="http://en.wordpress.com/tag/openbio/" title="View all posts in OpenBio" rel="category tag">OpenBio</a></p>
    <p>Tagged with <a href="http://en.wordpress.com/tag/bioinformatics/" rel="tag">bioinformatics</a>, <a href="http://en.wordpress.com/tag/biopython/" rel="tag">biopython</a>, <a href="http://en.wordpress.com/tag/biosql/" rel="tag">biosql</a>, <a href="http://en.wordpress.com/tag/cloud-computing/" rel="tag">cloud-computing</a>, <a href="http://en.wordpress.com/tag/gae/" rel="tag">gae</a>, <a href="http://en.wordpress.com/tag/gsoc/" rel="tag">gsoc</a>, <a href="http://en.wordpress.com/tag/openbio/" rel="tag">OpenBio</a></p></div>
</div>


<div class="navigation group">
	<div class="alignleft"></div>
	<div class="alignright"></div>
</div>

</div> 

<div id="sidebar">
			<div class="textwidget"><a href="http://feeds2.feedburner.com/bcbio" rel="alternate"><img src="http://www.feedburner.com/fb/images/pub/feed-icon16x16.png" style="border-width:0;" alt=""></a>&nbsp;<a href="http://feeds2.feedburner.com/bcbio" rel="alternate">Subscribe</a></div>
		<h3> </h3>		<ul>
			<li class="page_item page-item-2"><a href="http://bcbio.wordpress.com/about/" title="About">About</a></li>
		</ul>
		<form role="search" method="get" id="searchform" action="http://bcbio.wordpress.com/" >
	<div><label class="screen-reader-text" for="s">Search for:</label>
	<input type="text" value="" name="s" id="s" />
	<input type="submit" id="searchsubmit" value="Search" />
	</div>
	</form>				<h3>Recent Posts</h3>		<ul>
				<li><a href="https://bcbio.wordpress.com/2011/07/04/summarizing-next-gen-sequencing-variation-statistics-with-hadoop-using-cascalog/" title="Summarizing next-gen sequencing variation statistics with Hadoop using&nbsp;Cascalog">Summarizing next-gen sequencing variation statistics with Hadoop using&nbsp;Cascalog</a></li>
				<li><a href="https://bcbio.wordpress.com/2011/04/10/bioinformatics-jobs-at-harvard-school-of-public-health/" title="Bioinformatics jobs at Harvard School of Public&nbsp;Health">Bioinformatics jobs at Harvard School of Public&nbsp;Health</a></li>
				<li><a href="https://bcbio.wordpress.com/2011/04/10/parallel-upload-to-amazon-s3-with-python-boto-and-multiprocessing/" title="Parallel upload to Amazon S3 with python, boto and&nbsp;multiprocessing">Parallel upload to Amazon S3 with python, boto and&nbsp;multiprocessing</a></li>
				<li><a href="https://bcbio.wordpress.com/2011/01/11/next-generation-sequencing-information-management-and-analysis-system-for-galaxy/" title="Next generation sequencing information management and analysis system for&nbsp;Galaxy">Next generation sequencing information management and analysis system for&nbsp;Galaxy</a></li>
				<li><a href="https://bcbio.wordpress.com/2010/10/13/cloudbiolinux-progress-on-bioinformatics-cloud-images-and-data/" title="CloudBioLinux: progress on bioinformatics cloud images and&nbsp;data">CloudBioLinux: progress on bioinformatics cloud images and&nbsp;data</a></li>
				<li><a href="https://bcbio.wordpress.com/2010/05/08/automated-build-environment-for-bioinformatics-cloud-images/" title="Automated build environment for Bioinformatics cloud&nbsp;images">Automated build environment for Bioinformatics cloud&nbsp;images</a></li>
				<li><a href="https://bcbio.wordpress.com/2010/03/26/biopython-projects-for-google-summer-of-code-2010/" title="Biopython projects for Google Summer of Code&nbsp;2010">Biopython projects for Google Summer of Code&nbsp;2010</a></li>
				</ul>
		<h3> </h3>			<div class="textwidget"><a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/"><img alt="Creative Commons License" style="border-width:0;" src="http://i.creativecommons.org/l/by/3.0/us/80x15.png" /></a><br />This <span>work</span> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/">Creative Commons Attribution 3.0 United States License</a>.</div>
		

</div>

</div>

<div id="footer">
	<p><a href="http://wordpress.com/?ref=footer" rel="generator">Blog at WordPress.com</a>. Theme: <a href="http://theme.wordpress.com/themes/journalist/">The Journalist v1.9</a> by <a href="http://lucianmarin.com/" rel="designer">Lucian E. Marin</a>. </p>
</div>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script type="text/javascript">_qoptions={qacct:'p-18-mFEk4J448M',labels:'language.en,type.wpcom'};</script>
<script type="text/javascript" src="http://edge.quantserve.com/quant.js"></script>
<noscript><p><img class="robots-nocontent" src="http://pixel.quantserve.com/pixel/p-18-mFEk4J448M.gif?labels=language.en%2Ctype.wpcom" style="display:none" height="1" width="1" alt="" /></p></noscript>
<script type='text/javascript' src='http://s.gravatar.com/js/gprofiles.js?w&#038;ver=MU'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var WPGroHo = {
	my_hash: ""
};
/* ]]> */
</script>
<script type='text/javascript' src='http://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1306998986g&amp;ver=MU'></script>
	<div style="display:none">
	</div>

	<script type="text/javascript" src="http://s.skimresources.com/js/wordpress.js"></script> 
	<script type="text/javascript">
		var skimlinks_pub_id = "725X584219"
		var skimlinks_sitename = 'bcbio.wordpress.com';
		skimlinks();
	</script> 		
	<script type="text/javascript" src="http://b.scorecardresearch.com/beacon.js"></script><script type="text/javascript">try{COMSCORE.beacon({c1:2,c2:7518284});}catch(e){}</script><noscript><p class="robots-nocontent"><img src="http://b.scorecardresearch.com/p?cj=1c1=2&#038;c2=7518284" alt="" style="display:none" width="1" height="1" /></p></noscript><script type='text/javascript' src='http://s1.wp.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter/scripts/shCore.js?m=1306998989g&#038;ver=2.1.364b'></script>
<script type='text/javascript' src='http://s1.wp.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter/scripts/shBrushPython.js?m=1306998989g&#038;ver=2.1.364b'></script>
<script type='text/javascript'>
	(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var themecssurl = "http://s0.wp.com/wp-content/plugins/syntaxhighlighter-wpcom/shThemeDefault.css?m=1306998987g&amp;ver=2.1.364b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		document.getElementsByTagName("head")[0].appendChild(themecss);
	})();
	SyntaxHighlighter.config.clipboardSwf = 'http://s2.wp.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter/scripts/clipboard.swf?m=1306998989g';
	SyntaxHighlighter.config.strings.expandSource = 'show source';
	SyntaxHighlighter.config.strings.viewSource = 'view source';
	SyntaxHighlighter.config.strings.copyToClipboard = 'copy to clipboard';
	SyntaxHighlighter.config.strings.copyToClipboardConfirmation = 'The code is in your clipboard now';
	SyntaxHighlighter.config.strings.print = 'print';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
	SyntaxHighlighter.all();
</script>
<script src="http://s.stats.wordpress.com/w.js?19" type="text/javascript"></script>
<script type="text/javascript">
st_go({'blog':'5850073','v':'wpcom','user_id':'0','post':'0','subd':'bcbio'});
ex_go({'crypt':'UE40eW5QN0p8M2Y/RE1LVmwrVi5vQS5fVFtfdHBbPyw1VXIrU3hWLHhzVndTdktBX0ddJnpXX3daZ01pUE5OSC15OXUseD0uJVRRNixpMUJrbyVWblR2TCV6Zll8OEQyWz9MPyV5SHJOTjZ8M1dXK1d+LTR8NWlwX0o2RVN1WUgzZFoyTV96NF96VkNOQVYuXy96RVY5NStRaT9+WG9HNEpDWjlhQnJMRk95SWdXaCxPbSZ0am8vXVp+TjYraDcvOVBadjFPVkN2RnVaWA=='});
addLoadEvent(function(){linktracker_init('5850073',0);});
	</script>

</body>
</html>